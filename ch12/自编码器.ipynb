{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的两种基本范式: 监督学习(Supervised Learning)和无监督学习(Unsupervised Learning).\n",
    "\n",
    "两者最主要的区别是在于模型在训练时是否需要**人工标注**的**标签信息**。\n",
    "\n",
    "自监督学习(Self-Supervised Learning): 算法把数据**$x$本身**作为监督信号来学习. 利用辅助任务（pretext）从大规模的无监督数据中挖掘自身的监督信息，通过这种构造的监督信息对网络进行训练，从而可以学习到对下游任务有价值的表征.\n",
    "\n",
    "\n",
    "[自监督学习](https://blog.csdn.net/sdu_hao/article/details/104515917) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器原理\n",
    "\n",
    "有监督学习中神经网络的功能可以看做是特种降维(Dimensionality Reduction)的过程: $高维输入特征x \\rightarrow 低维变量o$.\n",
    "\n",
    "自监督学习利用数据$x$本身作为监督信号来指导网络的训练，即希望神经网络能够学习到映射$f_{\\theta}: x \\rightarrow \\bar x$. 将网络分成两部分:\n",
    "- $g_{\\theta_1}:x \\rightarrow z$, Encoder网络: 输入$x$数据编码成低维隐变量(Latent Variable).\n",
    "- $h_{\\theta_2}:z \\rightarrow \\bar x$, Decoder网络: 编码过后的输入$z$解码为高维度的$\\bar x$\n",
    "把整个模型$f_{\\theta}$称为自动编码器(Auto-Encoder)\n",
    "\n",
    "![自编码器模型](自编码器模型.png)\n",
    "\n",
    "我们希望解码器的输出能够完美地或者近似重建(Reconstruct, 或恢复)出原来的输入, 即$\\bar x \\approx x$. 优化目标可以写成:\n",
    "$$\n",
    "Minimize L = dist(x, \\bar x) \\\\\n",
    "\\bar x = h_{\\theta_2}(g_{\\theta_1}(x))\n",
    "$$\n",
    "\n",
    "$dist$距离度量, 常用欧氏距离."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "        tf.config.experimential.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32) / 255. \n",
    "X_test = X_test.astype(np.float32)/ 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只需要图片原始数据 不需要标签y\n",
    "train_db = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_db.take(1):\n",
    "    print(x.shape)\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "train_db = train_db.shuffle(10000).batch(BATCH_SIZE)\n",
    "test_db = test_db.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # 784 -> 20\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(hid_dim)\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "    def call(self, inputs, training=None):\n",
    "        hidden = self.encoder(inputs)\n",
    "        x = self.decoder(hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.sigmoid_cross_entropy_with_logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_rec_logist = model(inp)\n",
    "        # 或者直接使用MSE损失\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=inp, logits=x_rec_logist)\n",
    "        loss =tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(images, name):\n",
    "    new_im  = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    # 10行 10列  100张 = 50 + 50 \n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = images[index] \n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            # 将小图片写入对应位置  列方向排布\n",
    "            new_im.paste(im, (i, j))  # (x轴, y轴) 一列\n",
    "            index += 1\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        loss = train_step(x)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {step}, Loss {float(loss)}\")\n",
    "    \n",
    "    # 每个epoch 进行一次重建\n",
    "    x = next(iter(test_db))\n",
    "    logits = model(tf.reshape(x, [-1, 784]))\n",
    "    x_hat = tf.sigmoid(logits)  # 将输出转换为0至1的像素值，使用sigmoid 函数\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28])  # 恢复原来形状\n",
    "    \n",
    "    # 原始图片 + 重建图片 对比\n",
    "    x_concat = tf.concat([x[:50], x_hat[:50]], axis=0)\n",
    "    x_concat = x_concat.numpy() * 255  # 像素值恢复\n",
    "    x_concat = x_concat.astype(np.uint8)\n",
    "    \n",
    "    save_image(x_concat, f\"ae_images/rec_epoch_{epoch}.png\")\n",
    "print('Time taken for {EPOCHS} epochs {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('ae_images/rec_epoch_99.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "b = tf.constant([[1, 1, 1], [0, 0, 0]])\n",
    "tf.concat([a, b], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器变种\n",
    "### Denising Auto-Encoder\n",
    "防止神经网络记忆住输入数据的底层特征, 给输入数据添加随机的噪声扰动:\n",
    "$$\n",
    "\\tilde x = x + \\epsilon, \\epsilon \\sim \\mathcal N(0, var)\n",
    "$$\n",
    "\n",
    "### Dropout Auto-Encoder\n",
    "在网络层之间插入Dropout 层实现网络连接的随即断开, 防止过拟合.\n",
    "\n",
    "### Adversarial Auto-Encoder\n",
    "对抗自编码器利用额外的判别器网络来判断降维的隐藏变量$z$是否采样自先验分布$P(z)$, 方便利用$P(z)$来重建输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Auto-Encoder\n",
    "基本的自编码器是一个判别模型, 而不是生成模型. \n",
    "\n",
    "变分自编码器(VAE)可以实现给定隐藏变量的分布$P(z)$, 通过学习条件概率分布$P(x|z)$, 对联合概率分布$P(x, z) = P(x|z)P(z)$进行采样, 生成不同的样本.\n",
    "\n",
    "![VAE](VAE.png)\n",
    "\n",
    "对比自编码器, VAE模型对隐藏变量$z$的分布有显示的约束, 希望其符合预设的先验分布$P(z)$. 因此，在损失函数的设计上，除了原有的重建误差项外，还添加了隐变量𝒛分布的约束项。\n",
    "\n",
    "最大化目标  \n",
    "$$L(\\phi, \\theta) = -D_{KL}(q_{\\phi}(z|x)||p(z)) + E_{z \\sim q}[log p_{\\theta}(x|z)]$$\n",
    "\n",
    "用编码器网络参数化$q_{\\phi}(z|x)$函数, 解码器网络参数化$p_{\\theta}(x|z)$.\n",
    "\n",
    "特别地, 当$q_{\\phi}(z|x)$和$p(z)$都为**正态分布**时, 第一项散度的计算可以简化为:\n",
    "$$\n",
    "D_{KL}(q_{\\phi}(z|x)||p(z)) = log\\frac {\\sigma_2}{\\sigma_1} + \\frac {\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{x\\sigma_2^2} - \\frac 1 2\n",
    "$$\n",
    "\n",
    "详细推导过程: [KL散度推导](https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
