{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。\n",
    "\n",
    "循环神经网络除了训练困难，还有一个更严重的问题，那就是短时记忆(Short-term memory).它在处理较长的句子时，往往只能够理解有限长度内的信息，而对于位于较长范围类的有用信息往往不能够很好的利用起来。\n",
    "\n",
    "![](./images/LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "\n",
    "RNN的核心思想是上一个时间戳的状态向量$h_{t-1}$与当前时间戳的输入$x_t$经过线性变换后, 通过激活函数得到新的状态向量$h_{t}$.\n",
    "LSTM 新增了一个状态向量$𝑪$, 同时引入了门控(Gate)机制，通过门控单元来控制信息的遗忘和刷新，他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作.\n",
    "\n",
    "### 遗忘层门\n",
    "\n",
    "作用: 将细胞状态中的信息选择性的遗忘, 作用于 LSTM 状态向量𝒄上面，.\n",
    "\n",
    "操作步骤：该门会读取$h_{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C_{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。\n",
    "\n",
    "公式: $$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入层门\n",
    "\n",
    "作用: 将新的信息选择性的记录到细胞状态中, 控制LSTM对输入的接收程度\n",
    "\n",
    "操作步骤: \n",
    "\n",
    "1. sigmoid层决定什么值我们将要更新($i_t$);   \n",
    "2. tanh 层创建一个新的候选值向量$\\tilde{C}_t​$加入到状态中;  \n",
    "3. 将$c_{t-1}$更新为$c_{t}$, 丢弃需要丢弃的旧状态, 获得需要获得的新状态\n",
    "\n",
    "公式:\n",
    "$$\n",
    "i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i) \\\\\n",
    "\\tilde{C}_t = tanh(W_C[h_{t-1}, x_t] + b_C) \\\\\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde C_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出门层\n",
    "\n",
    "作用: 确定输出什么值, 内部状态$C_t$不会直接用于输出\n",
    "\n",
    "操作步骤: \n",
    "\n",
    "1. 通过sigmoid 层来确定细胞状态的哪个部分将输出\n",
    "2. 把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘, 输出确定输出的部分\n",
    "\n",
    "公式:\n",
    "$$\n",
    "o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \\\\\n",
    "h_t = o_t * tanh(C_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|输入门控 |遗忘门控 |LSTM行为|\n",
    "|---|---|---|\n",
    "|0|1|只是用记忆|\n",
    "|1|1| 综合输入和记忆|\n",
    "|0|0|清零记忆|\n",
    "|1|0|输入覆盖记忆|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model, Sequential, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 80, 100])  # \n",
    "xt = x[:, 0, :]  # 第一个单词, 第一个时间戳的输入\n",
    "\n",
    "cell = layers.LSTMCell(64)  # 与SimpleRNNCell 类似\n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]  # 初始状态h0,C0\n",
    "out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xh,w_hh, b =  cell.trainable_variables\n",
    "w_xh.shape, w_hh.shape, b.shape  # 4 个部分堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM层\n",
    "lstm_layer = layers.LSTM(64)\n",
    "\n",
    "out = lstm_layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单堆叠LSTM层\n",
    "lstm_net = Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True),\n",
    "    layers.LSTM(units=64),\n",
    "])\n",
    "lstm_net = lstm_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  使用LSTM进行情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # 词汇表大小\n",
    "MAX_REVIEW_LEN = 80  # 句子长度\n",
    "EMBEDDING_LEN = 100  # 词向量长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # 单词-数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加标志位\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # 表示填充\n",
    "word_index[\"<START>\"] = 1  # 表示起始\n",
    "word_index[\"<UNK>\"] = 2  # 表示未知单词\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# 翻转\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # 数字序列 -> 文本\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断填充(前部分) 成等长的序列\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # 舍弃最后一组 \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # 建立映射关系: 单词: 词向量(长度50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练\n",
    "# 将 单词序号-> 单词向量\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # 根据glove.6B.100d 将单词转为词向量\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # 初始状态向量\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # 词嵌入层\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # 分类层\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # 最末层 最后一个时间戳的输出\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTMRNN(64)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "LSTM 不容易出现梯度弥散现象。但是LSTM 结构相对较复杂，计算代价较高，模型参数量较大。\n",
    "门控循环网络(Gated Recurrent Unit，GRU), 是应用最广泛的LSTM简化版本. 将忘记门和输入门合成了一个单一的更新门, 内部状态向量和输出向量合并，统一为状态向量h.\n",
    "![](./images/LSTM12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 复位门(Reset Gate): 控制上一个时间戳的状态$h_{t-1}$进入GRU 的量;\n",
    "- 更新门(Update Gate): 控制上一时间戳的状态$h_{t-1}$和新输入$\\tilde h_t$对新状态向量$h_t$的影响程度."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [tf.zeros([2, 64])]\n",
    "cell = layers.GRUCell(64)\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "    \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRURNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # 初始状态向量\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # 词嵌入层\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.GRU(units, dropout=0.5, return_sequences=True),\n",
    "            layers.GRU(units, dropout=0.5)\n",
    "        ])\n",
    "        # 分类层\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # 最末层 最后一个时间戳的输出\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGRURNN(64)\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
