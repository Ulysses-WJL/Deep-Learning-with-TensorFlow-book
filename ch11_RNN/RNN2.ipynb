{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittf2conda2a75a45106264ceab7472c43279a5d24",
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，会造成梯度消失或者梯度膨胀的现象。\n",
    "\n",
    "循环神经网络除了训练困难，还有一个更严重的问题，那就是短时记忆(Short-term memory).它在处理较长的句子时，往往只能够理解有限长度内的信息，而对于位于较长范围类的有用信息往往不能够很好的利用起来。\n",
    "\n",
    "![](./images/LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "\n",
    "RNN的核心思想是上一个时间戳的状态向量$h_{t-1}$与当前时间戳的输入$x_t$经过线性变换后, 通过激活函数得到新的状态向量$h_{t}$.\n",
    "LSTM 新增了一个状态向量$𝑪$, 同时引入了门控(Gate)机制，通过门控单元来控制信息的遗忘和刷新，他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作.\n",
    "\n",
    "### 遗忘层门\n",
    "\n",
    "作用: 将细胞状态中的信息选择性的遗忘, 作用于 LSTM 状态向量𝒄上面，.\n",
    "\n",
    "操作步骤：该门会读取$h_{t-1}$和$x_t$，输出一个在 0 到 1 之间的数值给每个在细胞状态$C_{t-1}$中的数字。1 表示“完全保留”，0 表示“完全舍弃”。\n",
    "\n",
    "公式: $$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入层门\n",
    "\n",
    "作用: 将新的信息选择性的记录到细胞状态中, 控制LSTM对输入的接收程度\n",
    "\n",
    "操作步骤: \n",
    "\n",
    "1. sigmoid层决定什么值我们将要更新($i_t$);   \n",
    "2. tanh 层创建一个新的候选值向量$\\tilde{C}_t​$加入到状态中;  \n",
    "3. 将$c_{t-1}$更新为$c_{t}$, 丢弃需要丢弃的旧状态, 获得需要获得的新状态\n",
    "\n",
    "公式:\n",
    "$$\n",
    "i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i) \\\\\n",
    "\\tilde{C}_t = tanh(W_C[h_{t-1}, x_t] + b_C) \\\\\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde C_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出门层\n",
    "\n",
    "作用: 确定输出什么值, 内部状态$C_t$不会直接用于输出\n",
    "\n",
    "操作步骤: \n",
    "\n",
    "1. 通过sigmoid 层来确定细胞状态的哪个部分将输出\n",
    "2. 把细胞状态通过 tanh 进行处理，并将它和 sigmoid 门的输出相乘, 输出确定输出的部分\n",
    "\n",
    "公式:\n",
    "$$\n",
    "o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \\\\\n",
    "h_t = o_t * tanh(C_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|输入门控 |遗忘门控 |LSTM行为|\n",
    "|---|---|---|\n",
    "|0|1|只是用记忆|\n",
    "|1|1| 综合输入和记忆|\n",
    "|0|0|清零记忆|\n",
    "|1|0|输入覆盖记忆|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 80, 100])  # \n",
    "xt = x[:, 0, :]  # 第一个单词, 第一个时间戳的输入\n",
    "\n",
    "cell = layers.LSTMCell(64)  # 与SimpleRNNCell 类似\n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]  # 初始状态h0\n",
    "out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'name': 'lstm_cell',\n 'trainable': True,\n 'dtype': 'float32',\n 'units': 64,\n 'activation': 'tanh',\n 'recurrent_activation': 'sigmoid',\n 'use_bias': True,\n 'kernel_initializer': {'class_name': 'GlorotUniform',\n  'config': {'seed': None}},\n 'recurrent_initializer': {'class_name': 'Orthogonal',\n  'config': {'gain': 1.0, 'seed': None}},\n 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n 'unit_forget_bias': True,\n 'kernel_regularizer': None,\n 'recurrent_regularizer': None,\n 'bias_regularizer': None,\n 'kernel_constraint': None,\n 'recurrent_constraint': None,\n 'bias_constraint': None,\n 'dropout': 0.0,\n 'recurrent_dropout': 0.0,\n 'implementation': 2}"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "cell.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}