{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "RNNåœ¨å¤„ç†é•¿æœŸä¾èµ–ï¼ˆæ—¶é—´åºåˆ—ä¸Šè·ç¦»è¾ƒè¿œçš„èŠ‚ç‚¹ï¼‰æ—¶ä¼šé‡åˆ°å·¨å¤§çš„å›°éš¾ï¼Œå› ä¸ºè®¡ç®—è·ç¦»è¾ƒè¿œçš„èŠ‚ç‚¹ä¹‹é—´çš„è”ç³»æ—¶ä¼šæ¶‰åŠé›…å¯æ¯”çŸ©é˜µçš„å¤šæ¬¡ç›¸ä¹˜ï¼Œä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦è†¨èƒ€çš„ç°è±¡ã€‚\n",
    "\n",
    "å¾ªç¯ç¥ç»ç½‘ç»œé™¤äº†è®­ç»ƒå›°éš¾ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´ä¸¥é‡çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯çŸ­æ—¶è®°å¿†(Short-term memory).å®ƒåœ¨å¤„ç†è¾ƒé•¿çš„å¥å­æ—¶ï¼Œå¾€å¾€åªèƒ½å¤Ÿç†è§£æœ‰é™é•¿åº¦å†…çš„ä¿¡æ¯ï¼Œè€Œå¯¹äºä½äºè¾ƒé•¿èŒƒå›´ç±»çš„æœ‰ç”¨ä¿¡æ¯å¾€å¾€ä¸èƒ½å¤Ÿå¾ˆå¥½çš„åˆ©ç”¨èµ·æ¥ã€‚\n",
    "\n",
    "![](./images/LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸç†\n",
    "\n",
    "RNNçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€å‘é‡$h_{t-1}$ä¸å½“å‰æ—¶é—´æˆ³çš„è¾“å…¥$x_t$ç»è¿‡çº¿æ€§å˜æ¢å, é€šè¿‡æ¿€æ´»å‡½æ•°å¾—åˆ°æ–°çš„çŠ¶æ€å‘é‡$h_{t}$.\n",
    "LSTM æ–°å¢äº†ä¸€ä¸ªçŠ¶æ€å‘é‡$ğ‘ª$, åŒæ—¶å¼•å…¥äº†é—¨æ§(Gate)æœºåˆ¶ï¼Œé€šè¿‡é—¨æ§å•å…ƒæ¥æ§åˆ¶ä¿¡æ¯çš„é—å¿˜å’Œåˆ·æ–°ï¼Œä»–ä»¬åŒ…å«ä¸€ä¸ª sigmoid ç¥ç»ç½‘ç»œå±‚å’Œä¸€ä¸ª pointwise ä¹˜æ³•æ“ä½œ.\n",
    "\n",
    "### é—å¿˜å±‚é—¨\n",
    "\n",
    "ä½œç”¨: å°†ç»†èƒçŠ¶æ€ä¸­çš„ä¿¡æ¯é€‰æ‹©æ€§çš„é—å¿˜, ä½œç”¨äº LSTM çŠ¶æ€å‘é‡ğ’„ä¸Šé¢ï¼Œ.\n",
    "\n",
    "æ“ä½œæ­¥éª¤ï¼šè¯¥é—¨ä¼šè¯»å–$h_{t-1}$å’Œ$x_t$ï¼Œè¾“å‡ºä¸€ä¸ªåœ¨ 0 åˆ° 1 ä¹‹é—´çš„æ•°å€¼ç»™æ¯ä¸ªåœ¨ç»†èƒçŠ¶æ€$C_{t-1}$ä¸­çš„æ•°å­—ã€‚1 è¡¨ç¤ºâ€œå®Œå…¨ä¿ç•™â€ï¼Œ0 è¡¨ç¤ºâ€œå®Œå…¨èˆå¼ƒâ€ã€‚\n",
    "\n",
    "å…¬å¼: $$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥å±‚é—¨\n",
    "\n",
    "ä½œç”¨: å°†æ–°çš„ä¿¡æ¯é€‰æ‹©æ€§çš„è®°å½•åˆ°ç»†èƒçŠ¶æ€ä¸­, æ§åˆ¶LSTMå¯¹è¾“å…¥çš„æ¥æ”¶ç¨‹åº¦\n",
    "\n",
    "æ“ä½œæ­¥éª¤: \n",
    "\n",
    "1. sigmoidå±‚å†³å®šä»€ä¹ˆå€¼æˆ‘ä»¬å°†è¦æ›´æ–°($i_t$);   \n",
    "2. tanh å±‚åˆ›å»ºä¸€ä¸ªæ–°çš„å€™é€‰å€¼å‘é‡$\\tilde{C}_tâ€‹$åŠ å…¥åˆ°çŠ¶æ€ä¸­;  \n",
    "3. å°†$c_{t-1}$æ›´æ–°ä¸º$c_{t}$, ä¸¢å¼ƒéœ€è¦ä¸¢å¼ƒçš„æ—§çŠ¶æ€, è·å¾—éœ€è¦è·å¾—çš„æ–°çŠ¶æ€\n",
    "\n",
    "å…¬å¼:\n",
    "$$\n",
    "i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i) \\\\\n",
    "\\tilde{C}_t = tanh(W_C[h_{t-1}, x_t] + b_C) \\\\\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde C_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å‡ºé—¨å±‚\n",
    "\n",
    "ä½œç”¨: ç¡®å®šè¾“å‡ºä»€ä¹ˆå€¼, å†…éƒ¨çŠ¶æ€$C_t$ä¸ä¼šç›´æ¥ç”¨äºè¾“å‡º\n",
    "\n",
    "æ“ä½œæ­¥éª¤: \n",
    "\n",
    "1. é€šè¿‡sigmoid å±‚æ¥ç¡®å®šç»†èƒçŠ¶æ€çš„å“ªä¸ªéƒ¨åˆ†å°†è¾“å‡º\n",
    "2. æŠŠç»†èƒçŠ¶æ€é€šè¿‡ tanh è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å®ƒå’Œ sigmoid é—¨çš„è¾“å‡ºç›¸ä¹˜, è¾“å‡ºç¡®å®šè¾“å‡ºçš„éƒ¨åˆ†\n",
    "\n",
    "å…¬å¼:\n",
    "$$\n",
    "o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \\\\\n",
    "h_t = o_t * tanh(C_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|è¾“å…¥é—¨æ§ |é—å¿˜é—¨æ§ |LSTMè¡Œä¸º|\n",
    "|---|---|---|\n",
    "|0|1|åªæ˜¯ç”¨è®°å¿†|\n",
    "|1|1| ç»¼åˆè¾“å…¥å’Œè®°å¿†|\n",
    "|0|0|æ¸…é›¶è®°å¿†|\n",
    "|1|0|è¾“å…¥è¦†ç›–è®°å¿†|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model, Sequential, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 80, 100])  # \n",
    "xt = x[:, 0, :]  # ç¬¬ä¸€ä¸ªå•è¯, ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å…¥\n",
    "\n",
    "cell = layers.LSTMCell(64)  # ä¸SimpleRNNCell ç±»ä¼¼\n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]  # åˆå§‹çŠ¶æ€h0,C0\n",
    "out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_xh,w_hh, b =  cell.trainable_variables\n",
    "w_xh.shape, w_hh.shape, b.shape  # 4 ä¸ªéƒ¨åˆ†å †å "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMå±‚\n",
    "lstm_layer = layers.LSTM(64)\n",
    "\n",
    "out = lstm_layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•å †å LSTMå±‚\n",
    "lstm_net = Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True),\n",
    "    layers.LSTM(units=64),\n",
    "])\n",
    "lstm_net = lstm_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ä½¿ç”¨LSTMè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°\n",
    "MAX_REVIEW_LEN = 80  # å¥å­é•¿åº¦\n",
    "EMBEDDING_LEN = 100  # è¯å‘é‡é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # å•è¯-æ•°å­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ æ ‡å¿—ä½\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # è¡¨ç¤ºå¡«å……\n",
    "word_index[\"<START>\"] = 1  # è¡¨ç¤ºèµ·å§‹\n",
    "word_index[\"<UNK>\"] = 2  # è¡¨ç¤ºæœªçŸ¥å•è¯\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# ç¿»è½¬\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # æ•°å­—åºåˆ— -> æ–‡æœ¬\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆªæ–­å¡«å……(å‰éƒ¨åˆ†) æˆç­‰é•¿çš„åºåˆ—\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # èˆå¼ƒæœ€åä¸€ç»„ \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢„è®­ç»ƒ\n",
    "# å°† å•è¯åºå·-> å•è¯å‘é‡\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.100d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTMRNN(64)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "LSTM ä¸å®¹æ˜“å‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚ä½†æ˜¯LSTM ç»“æ„ç›¸å¯¹è¾ƒå¤æ‚ï¼Œè®¡ç®—ä»£ä»·è¾ƒé«˜ï¼Œæ¨¡å‹å‚æ•°é‡è¾ƒå¤§ã€‚\n",
    "é—¨æ§å¾ªç¯ç½‘ç»œ(Gated Recurrent Unitï¼ŒGRU), æ˜¯åº”ç”¨æœ€å¹¿æ³›çš„LSTMç®€åŒ–ç‰ˆæœ¬. å°†å¿˜è®°é—¨å’Œè¾“å…¥é—¨åˆæˆäº†ä¸€ä¸ªå•ä¸€çš„æ›´æ–°é—¨, å†…éƒ¨çŠ¶æ€å‘é‡å’Œè¾“å‡ºå‘é‡åˆå¹¶ï¼Œç»Ÿä¸€ä¸ºçŠ¶æ€å‘é‡h.\n",
    "![](./images/LSTM12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å¤ä½é—¨(Reset Gate): æ§åˆ¶ä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€$h_{t-1}$è¿›å…¥GRU çš„é‡;\n",
    "- æ›´æ–°é—¨(Update Gate): æ§åˆ¶ä¸Šä¸€æ—¶é—´æˆ³çš„çŠ¶æ€$h_{t-1}$å’Œæ–°è¾“å…¥$\\tilde h_t$å¯¹æ–°çŠ¶æ€å‘é‡$h_t$çš„å½±å“ç¨‹åº¦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [tf.zeros([2, 64])]\n",
    "cell = layers.GRUCell(64)\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "    \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRURNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.GRU(units, dropout=0.5, return_sequences=True),\n",
    "            layers.GRU(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGRURNN(64)\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
