{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "RNNåœ¨å¤„ç†é•¿æœŸä¾èµ–ï¼ˆæ—¶é—´åºåˆ—ä¸Šè·ç¦»è¾ƒè¿œçš„èŠ‚ç‚¹ï¼‰æ—¶ä¼šé‡åˆ°å·¨å¤§çš„å›°éš¾ï¼Œå› ä¸ºè®¡ç®—è·ç¦»è¾ƒè¿œçš„èŠ‚ç‚¹ä¹‹é—´çš„è”ç³»æ—¶ä¼šæ¶‰åŠé›…å¯æ¯”çŸ©é˜µçš„å¤šæ¬¡ç›¸ä¹˜ï¼Œä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦è†¨èƒ€çš„ç°è±¡ã€‚\n",
    "\n",
    "å¾ªç¯ç¥ç»ç½‘ç»œé™¤äº†è®­ç»ƒå›°éš¾ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´ä¸¥é‡çš„é—®é¢˜ï¼Œé‚£å°±æ˜¯çŸ­æ—¶è®°å¿†(Short-term memory).å®ƒåœ¨å¤„ç†è¾ƒé•¿çš„å¥å­æ—¶ï¼Œå¾€å¾€åªèƒ½å¤Ÿç†è§£æœ‰é™é•¿åº¦å†…çš„ä¿¡æ¯ï¼Œè€Œå¯¹äºä½äºè¾ƒé•¿èŒƒå›´ç±»çš„æœ‰ç”¨ä¿¡æ¯å¾€å¾€ä¸èƒ½å¤Ÿå¾ˆå¥½çš„åˆ©ç”¨èµ·æ¥ã€‚\n",
    "\n",
    "![](./images/LSTM2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸç†\n",
    "\n",
    "RNNçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€å‘é‡$h_{t-1}$ä¸å½“å‰æ—¶é—´æˆ³çš„è¾“å…¥$x_t$ç»è¿‡çº¿æ€§å˜æ¢å, é€šè¿‡æ¿€æ´»å‡½æ•°å¾—åˆ°æ–°çš„çŠ¶æ€å‘é‡$h_{t}$.\n",
    "LSTM æ–°å¢äº†ä¸€ä¸ªçŠ¶æ€å‘é‡$ğ‘ª$, åŒæ—¶å¼•å…¥äº†é—¨æ§(Gate)æœºåˆ¶ï¼Œé€šè¿‡é—¨æ§å•å…ƒæ¥æ§åˆ¶ä¿¡æ¯çš„é—å¿˜å’Œåˆ·æ–°ï¼Œä»–ä»¬åŒ…å«ä¸€ä¸ª sigmoid ç¥ç»ç½‘ç»œå±‚å’Œä¸€ä¸ª pointwise ä¹˜æ³•æ“ä½œ.\n",
    "\n",
    "### é—å¿˜å±‚é—¨\n",
    "\n",
    "ä½œç”¨: å°†ç»†èƒçŠ¶æ€ä¸­çš„ä¿¡æ¯é€‰æ‹©æ€§çš„é—å¿˜, ä½œç”¨äº LSTM çŠ¶æ€å‘é‡ğ’„ä¸Šé¢ï¼Œ.\n",
    "\n",
    "æ“ä½œæ­¥éª¤ï¼šè¯¥é—¨ä¼šè¯»å–$h_{t-1}$å’Œ$x_t$ï¼Œè¾“å‡ºä¸€ä¸ªåœ¨ 0 åˆ° 1 ä¹‹é—´çš„æ•°å€¼ç»™æ¯ä¸ªåœ¨ç»†èƒçŠ¶æ€$C_{t-1}$ä¸­çš„æ•°å­—ã€‚1 è¡¨ç¤ºâ€œå®Œå…¨ä¿ç•™â€ï¼Œ0 è¡¨ç¤ºâ€œå®Œå…¨èˆå¼ƒâ€ã€‚\n",
    "\n",
    "å…¬å¼: $$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥å±‚é—¨\n",
    "\n",
    "ä½œç”¨: å°†æ–°çš„ä¿¡æ¯é€‰æ‹©æ€§çš„è®°å½•åˆ°ç»†èƒçŠ¶æ€ä¸­, æ§åˆ¶LSTMå¯¹è¾“å…¥çš„æ¥æ”¶ç¨‹åº¦\n",
    "\n",
    "æ“ä½œæ­¥éª¤: \n",
    "\n",
    "1. sigmoidå±‚å†³å®šä»€ä¹ˆå€¼æˆ‘ä»¬å°†è¦æ›´æ–°($i_t$);   \n",
    "2. tanh å±‚åˆ›å»ºä¸€ä¸ªæ–°çš„å€™é€‰å€¼å‘é‡$\\tilde{C}_tâ€‹$åŠ å…¥åˆ°çŠ¶æ€ä¸­;  \n",
    "3. å°†$c_{t-1}$æ›´æ–°ä¸º$c_{t}$, ä¸¢å¼ƒéœ€è¦ä¸¢å¼ƒçš„æ—§çŠ¶æ€, è·å¾—éœ€è¦è·å¾—çš„æ–°çŠ¶æ€\n",
    "\n",
    "å…¬å¼:\n",
    "$$\n",
    "i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i) \\\\\n",
    "\\tilde{C}_t = tanh(W_C[h_{t-1}, x_t] + b_C) \\\\\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde C_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å‡ºé—¨å±‚\n",
    "\n",
    "ä½œç”¨: ç¡®å®šè¾“å‡ºä»€ä¹ˆå€¼, å†…éƒ¨çŠ¶æ€$C_t$ä¸ä¼šç›´æ¥ç”¨äºè¾“å‡º\n",
    "\n",
    "æ“ä½œæ­¥éª¤: \n",
    "\n",
    "1. é€šè¿‡sigmoid å±‚æ¥ç¡®å®šç»†èƒçŠ¶æ€çš„å“ªä¸ªéƒ¨åˆ†å°†è¾“å‡º\n",
    "2. æŠŠç»†èƒçŠ¶æ€é€šè¿‡ tanh è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å®ƒå’Œ sigmoid é—¨çš„è¾“å‡ºç›¸ä¹˜, è¾“å‡ºç¡®å®šè¾“å‡ºçš„éƒ¨åˆ†\n",
    "\n",
    "å…¬å¼:\n",
    "$$\n",
    "o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o) \\\\\n",
    "h_t = o_t * tanh(C_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|è¾“å…¥é—¨æ§ |é—å¿˜é—¨æ§ |LSTMè¡Œä¸º|\n",
    "|---|---|---|\n",
    "|0|1|åªæ˜¯ç”¨è®°å¿†|\n",
    "|1|1| ç»¼åˆè¾“å…¥å’Œè®°å¿†|\n",
    "|0|0|æ¸…é›¶è®°å¿†|\n",
    "|1|0|è¾“å…¥è¦†ç›–è®°å¿†|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model, Sequential, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 80, 100])  # \n",
    "xt = x[:, 0, :]  # ç¬¬ä¸€ä¸ªå•è¯, ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å…¥\n",
    "\n",
    "cell = layers.LSTMCell(64)  # ä¸SimpleRNNCell ç±»ä¼¼\n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]  # åˆå§‹çŠ¶æ€h0,C0\n",
    "out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'lstm_cell_1',\n",
       " 'trainable': True,\n",
       " 'dtype': 'float32',\n",
       " 'units': 64,\n",
       " 'activation': 'tanh',\n",
       " 'recurrent_activation': 'sigmoid',\n",
       " 'use_bias': True,\n",
       " 'kernel_initializer': {'class_name': 'GlorotUniform',\n",
       "  'config': {'seed': None}},\n",
       " 'recurrent_initializer': {'class_name': 'Orthogonal',\n",
       "  'config': {'gain': 1.0, 'seed': None}},\n",
       " 'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       " 'unit_forget_bias': True,\n",
       " 'kernel_regularizer': None,\n",
       " 'recurrent_regularizer': None,\n",
       " 'bias_regularizer': None,\n",
       " 'kernel_constraint': None,\n",
       " 'recurrent_constraint': None,\n",
       " 'bias_constraint': None,\n",
       " 'dropout': 0.0,\n",
       " 'recurrent_dropout': 0.0,\n",
       " 'implementation': 2}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([100, 256]), TensorShape([64, 256]), TensorShape([256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_xh,w_hh, b =  cell.trainable_variables\n",
    "w_xh.shape, w_hh.shape, b.shape  # 4 ä¸ªéƒ¨åˆ†å †å "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, state = cell(xt, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTMå±‚\n",
    "lstm_layer = layers.LSTM(64)\n",
    "\n",
    "out = lstm_layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 80, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_layer1 = layers.LSTM(64, return_sequences=True)\n",
    "\n",
    "out = lstm_layer1(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 1, 100])  # \n",
    "xt = x[:, 0, :]  # ç¬¬ä¸€ä¸ªå•è¯, ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å…¥\n",
    "\n",
    "cell = tf.keras.layers.LSTM(64, return_state=True)  \n",
    "state = [tf.zeros([2, 64]), tf.zeros([2, 64])]  # åˆå§‹çŠ¶æ€h0,C0\n",
    "out, state0, state1 = cell(x, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=12711, shape=(80, 64), dtype=float32, numpy=\n",
       " array([[ 0.02453458,  0.00537144,  0.16522579, ..., -0.14636117,\n",
       "          0.12870291, -0.19787529],\n",
       "        [-0.14883734, -0.06304549,  0.0218755 , ..., -0.07071137,\n",
       "          0.00717948, -0.03601092],\n",
       "        [-0.09798   , -0.17283341,  0.12789893, ..., -0.20422818,\n",
       "          0.19600664, -0.10933438],\n",
       "        ...,\n",
       "        [-0.23750345, -0.21065132, -0.22160155, ...,  0.02451491,\n",
       "         -0.22423549, -0.08433109],\n",
       "        [-0.24251884, -0.0784284 , -0.01251083, ...,  0.22695696,\n",
       "         -0.28828096, -0.11059391],\n",
       "        [-0.22755438, -0.1300424 , -0.26687545, ...,  0.03499105,\n",
       "         -0.18308741, -0.06962312]], dtype=float32)>,\n",
       " <tf.Tensor: id=12715, shape=(80, 64), dtype=float32, numpy=\n",
       " array([[ 0.12468018, -0.16513684,  0.13801843, ...,  0.20605457,\n",
       "         -0.09332453, -0.02846486],\n",
       "        [ 0.12871115, -0.09259171,  0.15879299, ...,  0.27870795,\n",
       "          0.02170868,  0.11957248],\n",
       "        [ 0.0965796 , -0.43249977,  0.22204703, ...,  0.4578754 ,\n",
       "         -0.08077777,  0.04101942],\n",
       "        ...,\n",
       "        [ 0.0379016 , -0.00698382,  0.01020187, ..., -0.122311  ,\n",
       "          0.02981921,  0.03197779],\n",
       "        [ 0.26127556, -0.03656434,  0.01060249, ...,  0.199124  ,\n",
       "         -0.19505078, -0.05562445],\n",
       "        [ 0.3235069 , -0.31286842,  0.06937418, ...,  0.345107  ,\n",
       "         -0.01897224, -0.20586495]], dtype=float32)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0], out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•å †å LSTMå±‚\n",
    "lstm_net = Sequential([\n",
    "    layers.LSTM(units=64, return_sequences=True),\n",
    "    layers.LSTM(units=64),\n",
    "])\n",
    "lstm_net = lstm_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ä½¿ç”¨LSTMè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°\n",
    "MAX_REVIEW_LEN = 80  # å¥å­é•¿åº¦\n",
    "EMBEDDING_LEN = 100  # è¯å‘é‡é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fawn', 34701)\n",
      "('tsukino', 52006)\n",
      "('nunnery', 52007)\n",
      "('sonja', 16816)\n",
      "('vani', 63951)\n",
      "('woods', 1408)\n",
      "('spiders', 16115)\n",
      "('hanging', 2345)\n",
      "('woody', 2289)\n",
      "('trawling', 52008)\n"
     ]
    }
   ],
   "source": [
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # å•è¯-æ•°å­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ æ ‡å¿—ä½\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # è¡¨ç¤ºå¡«å……\n",
    "word_index[\"<START>\"] = 1  # è¡¨ç¤ºèµ·å§‹\n",
    "word_index[\"<UNK>\"] = 2  # è¡¨ç¤ºæœªçŸ¥å•è¯\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# ç¿»è½¬\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # æ•°å­—åºåˆ— -> æ–‡æœ¬\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆªæ–­å¡«å……(å‰éƒ¨åˆ†) æˆç­‰é•¿çš„åºåˆ—\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # èˆå¼ƒæœ€åä¸€ç»„ \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9793 (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "# é¢„è®­ç»ƒ\n",
    "# å°† å•è¯åºå·-> å•è¯å‘é‡\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.100d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.LSTM(units, dropout=0.5, return_sequences=True),\n",
    "            layers.LSTM(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_lstmrnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1000000   \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  75264     \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    multiple                  2113      \n",
      "=================================================================\n",
      "Total params: 1,077,377\n",
      "Trainable params: 77,377\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTMRNN(64)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "LSTM ä¸å®¹æ˜“å‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚ä½†æ˜¯LSTM ç»“æ„ç›¸å¯¹è¾ƒå¤æ‚ï¼Œè®¡ç®—ä»£ä»·è¾ƒé«˜ï¼Œæ¨¡å‹å‚æ•°é‡è¾ƒå¤§ã€‚\n",
    "é—¨æ§å¾ªç¯ç½‘ç»œ(Gated Recurrent Unitï¼ŒGRU), æ˜¯åº”ç”¨æœ€å¹¿æ³›çš„LSTMç®€åŒ–ç‰ˆæœ¬. å°†å¿˜è®°é—¨å’Œè¾“å…¥é—¨åˆæˆäº†ä¸€ä¸ªå•ä¸€çš„æ›´æ–°é—¨, å†…éƒ¨çŠ¶æ€å‘é‡å’Œè¾“å‡ºå‘é‡åˆå¹¶ï¼Œç»Ÿä¸€ä¸ºçŠ¶æ€å‘é‡h.\n",
    "![](./images/LSTM12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- å¤ä½é—¨(Reset Gate): æ§åˆ¶ä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„çŠ¶æ€$h_{t-1}$è¿›å…¥GRU çš„é‡;\n",
    "- æ›´æ–°é—¨(Update Gate): æ§åˆ¶ä¸Šä¸€æ—¶é—´æˆ³çš„çŠ¶æ€$h_{t-1}$å’Œæ–°è¾“å…¥$\\tilde h_t$å¯¹æ–°çŠ¶æ€å‘é‡$h_t$çš„å½±å“ç¨‹åº¦."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [tf.zeros([2, 64])]\n",
    "cell = layers.GRUCell(64)\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out, h = cell(xt, h)\n",
    "    \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRURNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "#         self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "#         self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "                                          weights=[embedding_matrix],\n",
    "                                         trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.GRU(units, dropout=0.5, return_sequences=True),\n",
    "            layers.GRU(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "#         state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyGRURNN(64)\n",
    "model.build((None, MAX_REVIEW_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(train_db, epochs=20, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], label='test_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='test_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŒå‘RNN\n",
    "Bidirectional RNNs(åŒå‘ç½‘ç»œ)å°†ä¸¤å±‚RNNså åŠ åœ¨ä¸€èµ·ï¼Œå½“å‰æ—¶åˆ»è¾“å‡º(ç¬¬tæ­¥çš„è¾“å‡º)ä¸ä»…ä»…ä¸ä¹‹å‰åºåˆ—æœ‰å…³ï¼Œè¿˜ä¸ä¹‹ååºåˆ—æœ‰å…³ã€‚\n",
    "![](./images/figure_6.6.2_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Bidirectional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
