{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input, Sequential, datasets\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列\n",
    "具有先后顺序的数据一般叫作序列(Sequence). 我们把文字编码为数值的过程叫作**Word Embedding**.\n",
    "\n",
    "one-hot编码的优缺点:\n",
    "- 简单直观，编码过程不需要学习和训练;\n",
    "- 但高维度而且极其稀疏的，大量的位置为0，计算效率较低, 忽略了单词先天具有的语义相关性;\n",
    "\n",
    "余弦相关度(Cosine similarity), 衡量词向量(word vector)之间相关度:\n",
    "$$similarity(a, b) \\triangleq \\frac {a \\cdot b}{|a|\\cdot|b|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层\n",
    "单词的表示层叫作Embedding层, 负责把单词编码为某个词向量𝒗\n",
    "\n",
    "$$v = f_{\\theta}(i|N_{vocab}, n)$$\n",
    "单词数量记为$N_{vocab}$, $v的长度为n$, $i$表示单词编号, 如2 表示“I”，3 表示“me”等."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(10)  # 代表10个不同单词的编码\n",
    "\n",
    "x = tf.random.shuffle(x)\n",
    "# 10个单词, 每个单词用长度4 的向量表示\n",
    "net = layers.Embedding(10, 4)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预训练的词向量\n",
    "\n",
    "应用的比较广泛的预训练模型:Word2Vec 和GloVe模型.利用已预训练好的模型参数初始化Embedding层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # 建立映射关系: 单词: 词向量(长度50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.50d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# 加载20newsgroups数据集\n",
    "news20 = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = news20.target_names  # 一共20类不同的新闻\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news20['target']  # 每条新闻分属的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news20['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20['data'][0], category[news20['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # 最多保留 20000-1 个不同的单词\n",
    "MAX_SEQUENCE_LENGTH = 1000  # 每个序列长度\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 50  # 用50维向量表示一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?  # 令牌化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)  #  Only the most common `num_words-1` words will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates internal vocabulary based on a list of texts\n",
    "tokenizer.fit_on_texts(news20['data'])\n",
    "sequences = tokenizer.texts_to_sequences(news20['data'])  # 语句 -> 单词序列号组成的sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = tokenizer.texts_to_matrix(news20['data'])\n",
    "# matrix.shape  # (11314, 20000)  稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将sequences 转成文本list\n",
    "# tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单词映射为 index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "word_index_list = list(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从1开始编码 用0代表填充\n",
    "word_index_list[:10]  # news20group 出现频率最高的10个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list[19998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "pad_sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每条新闻都被编码成 等长的 用数字表示的 序列\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data), np.min(data) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=VALIDATION_SPLIT, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 单词序号-> 单词向量(长度50)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # 根据glove.6B.50d 将单词转为词向量\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new20group中最常用的19999 词向量 + 填充 + unknow\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    num_words, EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# 使用卷积\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "preds = layers.Dense(len(category), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['loss'], label='loss')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "\n",
    "$$h_t = \\sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)$$\n",
    "在每个时间戳$t$, 网络层接受当前时间戳的输入$x_t$和上一个时间戳的网络状态向量$h_{t-1}$,经过\n",
    "$$h_t = f_{\\theta}(h_{t-1}, x_t)$$\n",
    "变换后得到当前时间戳的新状态向量$h_t$. 在每个时间戳上, 网络层均有输出$o_t = g_{\\phi}(h_t)$\n",
    "\n",
    "对于这种网络结构，我们把它叫做循环网络结构(Recurrent Neural Network，简称RNN)。\n",
    "\n",
    "在循环神经网络中，激活函数更多地采用tanh 函数.并且可以选择不使用偏执𝒃来进一步减少参数量。\n",
    "\n",
    "状态向量$h_t$可以直接用作输出，即$o_t = h_t$，也可以对$t$做一个简单的线性变换."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度传播\n",
    "\n",
    "参数$W_{hh}$的梯度计算, RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。\n",
    "$$\\frac {\\partial L}{\\partial W_{hh}} = \\sum_{i=1}^t \\frac {\\partial L}{\\partial o_t}\n",
    "\\frac {\\partial o_t}{\\partial h_t} \\frac {\\partial h_t}{\\partial h_i}\n",
    "\\frac {\\partial^+ h_i}{\\partial W_{hh}}\n",
    "$$\n",
    "其中\n",
    "$$\\frac {\\partial^+ h_i}{\\partial W_{hh}} = \\frac {\\partial \\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)}{\\partial W_{hh}}$$\n",
    "只考虑一个时间戳的梯度传播, 即\"直接\"偏导数.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial h_t}{\\partial h_i} = \n",
    "\\frac {\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac {\\partial h_{t-1}}{\\partial h_{t-2}}\n",
    "\\cdots\n",
    "\\frac {\\partial h_{i+1}}{\\partial h_i}\n",
    "= \\prod_{k=i}^{t-1}\\frac {\\partial h_{k+1}}{\\partial h_{k}} $$\n",
    "\n",
    "\n",
    "$$\\frac {\\partial h_{k+1}}{\\partial h_{k}}\n",
    "= W^T_{hh}diag(\\sigma'(h_{k+1}))$$\n",
    "\n",
    "所以$$\\frac {\\partial h_t}{\\partial h_i} = \\prod_{j=i}^{t-1}diag(\\sigma'(W_{xh}x_{j+1} + W_{hh}h_j + b))W_{hh}$$\n",
    "\n",
    "在某个时刻的对$W_{xh}$或是$W_{hh}$的偏导数，需要追溯这个时刻之前所有时刻的信息, 整体的偏导公式就是将其按时刻再一一加起来。\n",
    "\n",
    "公式中包含雅克比矩阵和$W_{hh}$的连乘运算, 容易出现梯度消失(激活函数使用sigmoid或tanh时)或梯度爆炸(使用ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN层的使用\n",
    "\n",
    "- SimpleRNNCell: 完成了一个时间戳的前向运算($\\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)$)\n",
    "- SimpleRNN: 基于Cell 层实现的，它在内部已经完成了多个时间戳的循环运算，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.SimpleRNNCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = layers.SimpleRNNCell(3)  # 内存向量h长度 3\n",
    "cell.build(input_shape=(None, 4))  # 输入x特征长度4\n",
    "cell.trainable_variables  # W_xh ,  W_hh, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向运算\n",
    "$$o_t, [h_t] = Cell(x_t, [h_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化状态向量，用列表包裹，统一格式\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "\n",
    "# (b, word_num, word_vec_length)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]  # 所有句子的第一个单词\n",
    "\n",
    "cell = layers.SimpleRNNCell(64)\n",
    "out1, h1 = cell(xt, h0)  # h1用list包裹, out1没有经过变换 = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, h1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(out), id(h1[0]))  # 状态向量直接作为输出向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "for x_t in tf.unstack(x, axis=1):  # 时间维度解开, 按时间输入单词\n",
    "    out, h = cell(x_t, h)\n",
    "out = out  # 只取最后时间戳的输出  N->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2层循环神经网络\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]\n",
    "cell0 = layers.SimpleRNNCell(64)\n",
    "cell1 = layers.SimpleRNNCell(64)\n",
    "# 2个cell的初始状态\n",
    "h0 = [tf.zeros((4, 64))]\n",
    "h1 = [tf.zeros((4, 64))]\n",
    "\n",
    "# 一个时间戳上完成2层传播在到下一个时间戳\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    \n",
    "    out1, h1 = cell1(out0, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先完成第一层所有时间的传播再完成第二层所有时间的传播\n",
    "middle_seqences = []\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    middle_seqences.append(out0)\n",
    "\n",
    "for xt in middle_seqences:\n",
    "    out1, h1 = cell1(xt, h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN  完成多个时间戳的计算\n",
    "layer = layers.SimpleRNN(64)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回所有时间戳上的输出\n",
    "layer = layers.SimpleRNN(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层RNN网络\n",
    "net = Sequential([\n",
    "    # 除最末层外，都需要返回所有时间戳的输出，用作下一层的输入\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN情感分类\n",
    "imdb评分>7 为1 positive; IMDB 评级<5 的用户评价标注为0 \n",
    "\n",
    "利用第2 层RNN 层的最后时间戳的状态向量h, 作为句子的全局语义特征表示, 送入全连接分类网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # 词汇表大小\n",
    "MAX_REVIEW_LEN = 80  # 句子长度\n",
    "EMBEDDING_LEN = 50  # 词向量长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.imdb.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb数据集\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(\n",
    "    num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, len(X_train[0]), y_train.shape)  # X 不等长的list 组成的array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, len(X_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码表\n",
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # 单词-数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'total {len(word_index)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加标志位\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # 表示填充\n",
    "word_index[\"<START>\"] = 1  # 表示起始\n",
    "word_index[\"<UNK>\"] = 2  # 表示未知单词\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# 翻转\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # 数字序列 -> 文本\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断 填充 成等长的序列\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # 舍弃最后一组 \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_db))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 单词序号-> 单词向量(长度50)\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # 根据glove.6B.50d 将单词转为词向量\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # 初始状态向量\n",
    "        self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # 词嵌入层\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "#                                           weights=[embedding_matrix],\n",
    "#                                          trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n",
    "            layers.SimpleRNN(units, dropout=0.5)\n",
    "        ])\n",
    "        # 分类层\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "        state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # 最末层 最后一个时间戳的输出\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(64)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(10e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'],\n",
    "#              experimental_run_tf_function=False  # 以cell方式运行需要设置\n",
    "             )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, MAX_REVIEW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_db, epochs=10, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度弥散和梯度爆炸\n",
    "梯度下降\n",
    "$$\\theta := \\theta - \\eta\\nabla_{\\theta} L$$\n",
    "\n",
    "- 梯度弥散(Gradient Vanishing): $\\nabla_{\\theta} L \\approx 0$, 每次梯度更新后参数基本保持不变, ℒ几乎保持不变，其它评测指标，如准确度，也保持不变\n",
    "- 梯度爆炸(Gradient Exploding): $\\nabla_{\\theta} L \\gg 1$, 梯度更新的步长很大, 更新后的$\\theta$变化很大, L出现突变现象，甚至可能出现来回震荡、不收敛的现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2])\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # 获取特征值\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多次连乘\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2范数\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2]) * 0.4\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # 获取特征值\n",
    "# 多次连乘\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2范数\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))\n",
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度裁剪(Gradient Clipping)\n",
    "\n",
    "梯度爆炸可以通过梯度裁剪(Gradient Clipping)的方式在一定程度上的解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 简单裁剪, 直接对张量的数值进行限幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = tf.random.uniform([2, 2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_value(a, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 限制梯度张量W的范数\n",
    "$$W' = \\frac {W}{||W||_2} \\cdot max$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([2, 2]) * 5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.clip_by_norm(a, 5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(a), tf.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 全局范数裁剪, 考虑所有参数的梯度的范数, 等比例缩放\n",
    "\n",
    "$$global\\_norm = \\sqrt{\\sum_i ||W^{i}||^2_2}$$\n",
    "\n",
    "$$W^{(i)} = \\frac {W^{(i)} \\cdot max\\_norm}{max(global\\_norm, max\\_norm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.random.normal([3, 3])\n",
    "w2 = tf.random.normal([3, 3])\n",
    "global_norm = tf.sqrt(tf.norm(w1) ** 2 + tf.norm(w2) ** 2)\n",
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ww1, ww2), global_norm = tf.clip_by_global_norm([w1, w2], 2)  # 总范数限制为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1, ww2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm2 = tf.sqrt(tf.norm(ww1) ** 2 + tf.norm(ww2) ** 2)\n",
    "global_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网络训练时，梯度裁剪一般在计算出梯度后，梯度更新之前进行\n",
    "\n",
    "### 处理梯度弥散\n",
    "对于梯度弥散现象，可以通过增大学习率、减少网络深度、添加 Skip Connection 等一系列的措施抑.\n",
    "\n",
    "减少网络深度可以减轻梯度弥散现象, 但会影响表达能力.\n",
    "\n",
    "使用深度残差网络"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
