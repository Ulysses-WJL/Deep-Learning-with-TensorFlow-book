{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# å¾ªç¯ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input, Sequential, datasets\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åºåˆ—\n",
    "å…·æœ‰å…ˆåé¡ºåºçš„æ•°æ®ä¸€èˆ¬å«ä½œåºåˆ—(Sequence). æˆ‘ä»¬æŠŠæ–‡å­—ç¼–ç ä¸ºæ•°å€¼çš„è¿‡ç¨‹å«ä½œ**Word Embedding**.\n",
    "\n",
    "one-hotç¼–ç çš„ä¼˜ç¼ºç‚¹:\n",
    "- ç®€å•ç›´è§‚ï¼Œç¼–ç è¿‡ç¨‹ä¸éœ€è¦å­¦ä¹ å’Œè®­ç»ƒ;\n",
    "- ä½†é«˜ç»´åº¦è€Œä¸”æå…¶ç¨€ç–çš„ï¼Œå¤§é‡çš„ä½ç½®ä¸º0ï¼Œè®¡ç®—æ•ˆç‡è¾ƒä½, å¿½ç•¥äº†å•è¯å…ˆå¤©å…·æœ‰çš„è¯­ä¹‰ç›¸å…³æ€§;\n",
    "\n",
    "ä½™å¼¦ç›¸å…³åº¦(Cosine similarity), è¡¡é‡è¯å‘é‡(word vector)ä¹‹é—´ç›¸å…³åº¦:\n",
    "$$similarity(a, b) \\triangleq \\frac {a \\cdot b}{|a|\\cdot|b|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddingå±‚\n",
    "å•è¯çš„è¡¨ç¤ºå±‚å«ä½œEmbeddingå±‚, è´Ÿè´£æŠŠå•è¯ç¼–ç ä¸ºæŸä¸ªè¯å‘é‡ğ’—\n",
    "\n",
    "$$v = f_{\\theta}(i|N_{vocab}, n)$$\n",
    "å•è¯æ•°é‡è®°ä¸º$N_{vocab}$, $vçš„é•¿åº¦ä¸ºn$, $i$è¡¨ç¤ºå•è¯ç¼–å·, å¦‚2 è¡¨ç¤ºâ€œIâ€ï¼Œ3 è¡¨ç¤ºâ€œmeâ€ç­‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(10)  # ä»£è¡¨10ä¸ªä¸åŒå•è¯çš„ç¼–ç \n",
    "\n",
    "x = tf.random.shuffle(x)\n",
    "# 10ä¸ªå•è¯, æ¯ä¸ªå•è¯ç”¨é•¿åº¦4 çš„å‘é‡è¡¨ç¤º\n",
    "net = layers.Embedding(10, 4)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„è®­ç»ƒçš„è¯å‘é‡\n",
    "\n",
    "åº”ç”¨çš„æ¯”è¾ƒå¹¿æ³›çš„é¢„è®­ç»ƒæ¨¡å‹:Word2Vec å’ŒGloVeæ¨¡å‹.åˆ©ç”¨å·²é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°åˆå§‹åŒ–Embeddingå±‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.50d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# åŠ è½½20newsgroupsæ•°æ®é›†\n",
    "news20 = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = news20.target_names  # ä¸€å…±20ç±»ä¸åŒçš„æ–°é—»\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news20['target']  # æ¯æ¡æ–°é—»åˆ†å±çš„ç±»åˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news20['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20['data'][0], category[news20['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # æœ€å¤šä¿ç•™ 20000-1 ä¸ªä¸åŒçš„å•è¯\n",
    "MAX_SEQUENCE_LENGTH = 1000  # æ¯ä¸ªåºåˆ—é•¿åº¦\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 50  # ç”¨50ç»´å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?  # ä»¤ç‰ŒåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)  #  Only the most common `num_words-1` words will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates internal vocabulary based on a list of texts\n",
    "tokenizer.fit_on_texts(news20['data'])\n",
    "sequences = tokenizer.texts_to_sequences(news20['data'])  # è¯­å¥ -> å•è¯åºåˆ—å·ç»„æˆçš„sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = tokenizer.texts_to_matrix(news20['data'])\n",
    "# matrix.shape  # (11314, 20000)  ç¨€ç–çŸ©é˜µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†sequences è½¬æˆæ–‡æœ¬list\n",
    "# tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†å•è¯æ˜ å°„ä¸º index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "word_index_list = list(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»1å¼€å§‹ç¼–ç  ç”¨0ä»£è¡¨å¡«å……\n",
    "word_index_list[:10]  # news20group å‡ºç°é¢‘ç‡æœ€é«˜çš„10ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list[19998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "pad_sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯æ¡æ–°é—»éƒ½è¢«ç¼–ç æˆ ç­‰é•¿çš„ ç”¨æ•°å­—è¡¨ç¤ºçš„ åºåˆ—\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data), np.min(data) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=VALIDATION_SPLIT, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† å•è¯åºå·-> å•è¯å‘é‡(é•¿åº¦50)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.50d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new20groupä¸­æœ€å¸¸ç”¨çš„19999 è¯å‘é‡ + å¡«å…… + unknow\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    num_words, EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# ä½¿ç”¨å·ç§¯\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "preds = layers.Dense(len(category), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['loss'], label='loss')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "\n",
    "\n",
    "$$h_t = \\sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)$$\n",
    "åœ¨æ¯ä¸ªæ—¶é—´æˆ³$t$, ç½‘ç»œå±‚æ¥å—å½“å‰æ—¶é—´æˆ³çš„è¾“å…¥$x_t$å’Œä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„ç½‘ç»œçŠ¶æ€å‘é‡$h_{t-1}$,ç»è¿‡\n",
    "$$h_t = f_{\\theta}(h_{t-1}, x_t)$$\n",
    "å˜æ¢åå¾—åˆ°å½“å‰æ—¶é—´æˆ³çš„æ–°çŠ¶æ€å‘é‡$h_t$. åœ¨æ¯ä¸ªæ—¶é—´æˆ³ä¸Š, ç½‘ç»œå±‚å‡æœ‰è¾“å‡º$o_t = g_{\\phi}(h_t)$\n",
    "\n",
    "å¯¹äºè¿™ç§ç½‘ç»œç»“æ„ï¼Œæˆ‘ä»¬æŠŠå®ƒå«åšå¾ªç¯ç½‘ç»œç»“æ„(Recurrent Neural Networkï¼Œç®€ç§°RNN)ã€‚\n",
    "\n",
    "åœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­ï¼Œæ¿€æ´»å‡½æ•°æ›´å¤šåœ°é‡‡ç”¨tanh å‡½æ•°.å¹¶ä¸”å¯ä»¥é€‰æ‹©ä¸ä½¿ç”¨åæ‰§ğ’ƒæ¥è¿›ä¸€æ­¥å‡å°‘å‚æ•°é‡ã€‚\n",
    "\n",
    "çŠ¶æ€å‘é‡$h_t$å¯ä»¥ç›´æ¥ç”¨ä½œè¾“å‡ºï¼Œå³$o_t = h_t$ï¼Œä¹Ÿå¯ä»¥å¯¹$t$åšä¸€ä¸ªç®€å•çš„çº¿æ€§å˜æ¢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¢¯åº¦ä¼ æ’­\n",
    "\n",
    "å‚æ•°$W_{hh}$çš„æ¢¯åº¦è®¡ç®—, RNNçš„æŸå¤±ä¹Ÿæ˜¯ä¼šéšç€æ—¶é—´ç´¯åŠ çš„ï¼Œæ‰€ä»¥ä¸èƒ½åªæ±‚tæ—¶åˆ»çš„åå¯¼ã€‚\n",
    "$$\\frac {\\partial L}{\\partial W_{hh}} = \\sum_{i=1}^t \\frac {\\partial L}{\\partial o_t}\n",
    "\\frac {\\partial o_t}{\\partial h_t} \\frac {\\partial h_t}{\\partial h_i}\n",
    "\\frac {\\partial^+ h_i}{\\partial W_{hh}}\n",
    "$$\n",
    "å…¶ä¸­\n",
    "$$\\frac {\\partial^+ h_i}{\\partial W_{hh}} = \\frac {\\partial \\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)}{\\partial W_{hh}}$$\n",
    "åªè€ƒè™‘ä¸€ä¸ªæ—¶é—´æˆ³çš„æ¢¯åº¦ä¼ æ’­, å³\"ç›´æ¥\"åå¯¼æ•°.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial h_t}{\\partial h_i} = \n",
    "\\frac {\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac {\\partial h_{t-1}}{\\partial h_{t-2}}\n",
    "\\cdots\n",
    "\\frac {\\partial h_{i+1}}{\\partial h_i}\n",
    "= \\prod_{k=i}^{t-1}\\frac {\\partial h_{k+1}}{\\partial h_{k}} $$\n",
    "\n",
    "\n",
    "$$\\frac {\\partial h_{k+1}}{\\partial h_{k}}\n",
    "= W^T_{hh}diag(\\sigma'(h_{k+1}))$$\n",
    "\n",
    "æ‰€ä»¥$$\\frac {\\partial h_t}{\\partial h_i} = \\prod_{j=i}^{t-1}diag(\\sigma'(W_{xh}x_{j+1} + W_{hh}h_j + b))W_{hh}$$\n",
    "\n",
    "åœ¨æŸä¸ªæ—¶åˆ»çš„å¯¹$W_{xh}$æˆ–æ˜¯$W_{hh}$çš„åå¯¼æ•°ï¼Œéœ€è¦è¿½æº¯è¿™ä¸ªæ—¶åˆ»ä¹‹å‰æ‰€æœ‰æ—¶åˆ»çš„ä¿¡æ¯, æ•´ä½“çš„åå¯¼å…¬å¼å°±æ˜¯å°†å…¶æŒ‰æ—¶åˆ»å†ä¸€ä¸€åŠ èµ·æ¥ã€‚\n",
    "\n",
    "å…¬å¼ä¸­åŒ…å«é›…å…‹æ¯”çŸ©é˜µå’Œ$W_{hh}$çš„è¿ä¹˜è¿ç®—, å®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±(æ¿€æ´»å‡½æ•°ä½¿ç”¨sigmoidæˆ–tanhæ—¶)æˆ–æ¢¯åº¦çˆ†ç‚¸(ä½¿ç”¨ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNå±‚çš„ä½¿ç”¨\n",
    "\n",
    "- SimpleRNNCell: å®Œæˆäº†ä¸€ä¸ªæ—¶é—´æˆ³çš„å‰å‘è¿ç®—($\\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)$)\n",
    "- SimpleRNN: åŸºäºCell å±‚å®ç°çš„ï¼Œå®ƒåœ¨å†…éƒ¨å·²ç»å®Œæˆäº†å¤šä¸ªæ—¶é—´æˆ³çš„å¾ªç¯è¿ç®—ï¼Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.SimpleRNNCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = layers.SimpleRNNCell(3)  # å†…å­˜å‘é‡hé•¿åº¦ 3\n",
    "cell.build(input_shape=(None, 4))  # è¾“å…¥xç‰¹å¾é•¿åº¦4\n",
    "cell.trainable_variables  # W_xh ,  W_hh, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰å‘è¿ç®—\n",
    "$$o_t, [h_t] = Cell(x_t, [h_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–çŠ¶æ€å‘é‡ï¼Œç”¨åˆ—è¡¨åŒ…è£¹ï¼Œç»Ÿä¸€æ ¼å¼\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "\n",
    "# (b, word_num, word_vec_length)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]  # æ‰€æœ‰å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯\n",
    "\n",
    "cell = layers.SimpleRNNCell(64)\n",
    "out1, h1 = cell(xt, h0)  # h1ç”¨liståŒ…è£¹, out1æ²¡æœ‰ç»è¿‡å˜æ¢ = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, h1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(out), id(h1[0]))  # çŠ¶æ€å‘é‡ç›´æ¥ä½œä¸ºè¾“å‡ºå‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "for x_t in tf.unstack(x, axis=1):  # æ—¶é—´ç»´åº¦è§£å¼€, æŒ‰æ—¶é—´è¾“å…¥å•è¯\n",
    "    out, h = cell(x_t, h)\n",
    "out = out  # åªå–æœ€åæ—¶é—´æˆ³çš„è¾“å‡º  N->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2å±‚å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]\n",
    "cell0 = layers.SimpleRNNCell(64)\n",
    "cell1 = layers.SimpleRNNCell(64)\n",
    "# 2ä¸ªcellçš„åˆå§‹çŠ¶æ€\n",
    "h0 = [tf.zeros((4, 64))]\n",
    "h1 = [tf.zeros((4, 64))]\n",
    "\n",
    "# ä¸€ä¸ªæ—¶é—´æˆ³ä¸Šå®Œæˆ2å±‚ä¼ æ’­åœ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æˆ³\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    \n",
    "    out1, h1 = cell1(out0, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆå®Œæˆç¬¬ä¸€å±‚æ‰€æœ‰æ—¶é—´çš„ä¼ æ’­å†å®Œæˆç¬¬äºŒå±‚æ‰€æœ‰æ—¶é—´çš„ä¼ æ’­\n",
    "middle_seqences = []\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    middle_seqences.append(out0)\n",
    "\n",
    "for xt in middle_seqences:\n",
    "    out1, h1 = cell1(xt, h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN  å®Œæˆå¤šä¸ªæ—¶é—´æˆ³çš„è®¡ç®—\n",
    "layer = layers.SimpleRNN(64)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿”å›æ‰€æœ‰æ—¶é—´æˆ³ä¸Šçš„è¾“å‡º\n",
    "layer = layers.SimpleRNN(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šå±‚RNNç½‘ç»œ\n",
    "net = Sequential([\n",
    "    # é™¤æœ€æœ«å±‚å¤–ï¼Œéƒ½éœ€è¦è¿”å›æ‰€æœ‰æ—¶é—´æˆ³çš„è¾“å‡ºï¼Œç”¨ä½œä¸‹ä¸€å±‚çš„è¾“å…¥\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNæƒ…æ„Ÿåˆ†ç±»\n",
    "imdbè¯„åˆ†>7 ä¸º1 positive; IMDB è¯„çº§<5 çš„ç”¨æˆ·è¯„ä»·æ ‡æ³¨ä¸º0 \n",
    "\n",
    "åˆ©ç”¨ç¬¬2 å±‚RNN å±‚çš„æœ€åæ—¶é—´æˆ³çš„çŠ¶æ€å‘é‡h, ä½œä¸ºå¥å­çš„å…¨å±€è¯­ä¹‰ç‰¹å¾è¡¨ç¤º, é€å…¥å…¨è¿æ¥åˆ†ç±»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°\n",
    "MAX_REVIEW_LEN = 80  # å¥å­é•¿åº¦\n",
    "EMBEDDING_LEN = 50  # è¯å‘é‡é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.imdb.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdbæ•°æ®é›†\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(\n",
    "    num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, len(X_train[0]), y_train.shape)  # X ä¸ç­‰é•¿çš„list ç»„æˆçš„array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, len(X_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¼–ç è¡¨\n",
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # å•è¯-æ•°å­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'total {len(word_index)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ æ ‡å¿—ä½\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # è¡¨ç¤ºå¡«å……\n",
    "word_index[\"<START>\"] = 1  # è¡¨ç¤ºèµ·å§‹\n",
    "word_index[\"<UNK>\"] = 2  # è¡¨ç¤ºæœªçŸ¥å•è¯\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# ç¿»è½¬\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # æ•°å­—åºåˆ— -> æ–‡æœ¬\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆªæ–­ å¡«å…… æˆç­‰é•¿çš„åºåˆ—\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # èˆå¼ƒæœ€åä¸€ç»„ \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_db))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† å•è¯åºå·-> å•è¯å‘é‡(é•¿åº¦50)\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.50d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "        self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "#                                           weights=[embedding_matrix],\n",
    "#                                          trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n",
    "            layers.SimpleRNN(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "        state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(64)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(10e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'],\n",
    "#              experimental_run_tf_function=False  # ä»¥cellæ–¹å¼è¿è¡Œéœ€è¦è®¾ç½®\n",
    "             )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, MAX_REVIEW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_db, epochs=10, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¢¯åº¦å¼¥æ•£å’Œæ¢¯åº¦çˆ†ç‚¸\n",
    "æ¢¯åº¦ä¸‹é™\n",
    "$$\\theta := \\theta - \\eta\\nabla_{\\theta} L$$\n",
    "\n",
    "- æ¢¯åº¦å¼¥æ•£(Gradient Vanishing): $\\nabla_{\\theta} L \\approx 0$, æ¯æ¬¡æ¢¯åº¦æ›´æ–°åå‚æ•°åŸºæœ¬ä¿æŒä¸å˜, â„’å‡ ä¹ä¿æŒä¸å˜ï¼Œå…¶å®ƒè¯„æµ‹æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®åº¦ï¼Œä¹Ÿä¿æŒä¸å˜\n",
    "- æ¢¯åº¦çˆ†ç‚¸(Gradient Exploding): $\\nabla_{\\theta} L \\gg 1$, æ¢¯åº¦æ›´æ–°çš„æ­¥é•¿å¾ˆå¤§, æ›´æ–°åçš„$\\theta$å˜åŒ–å¾ˆå¤§, Lå‡ºç°çªå˜ç°è±¡ï¼Œç”šè‡³å¯èƒ½å‡ºç°æ¥å›éœ‡è¡ã€ä¸æ”¶æ•›çš„ç°è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2])\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # è·å–ç‰¹å¾å€¼\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ¬¡è¿ä¹˜\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2èŒƒæ•°\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2]) * 0.4\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # è·å–ç‰¹å¾å€¼\n",
    "# å¤šæ¬¡è¿ä¹˜\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2èŒƒæ•°\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))\n",
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¢¯åº¦è£å‰ª(Gradient Clipping)\n",
    "\n",
    "æ¢¯åº¦çˆ†ç‚¸å¯ä»¥é€šè¿‡æ¢¯åº¦è£å‰ª(Gradient Clipping)çš„æ–¹å¼åœ¨ä¸€å®šç¨‹åº¦ä¸Šçš„è§£å†³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ç®€å•è£å‰ª, ç›´æ¥å¯¹å¼ é‡çš„æ•°å€¼è¿›è¡Œé™å¹…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = tf.random.uniform([2, 2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_value(a, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. é™åˆ¶æ¢¯åº¦å¼ é‡Wçš„èŒƒæ•°\n",
    "$$W' = \\frac {W}{||W||_2} \\cdot max$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([2, 2]) * 5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.clip_by_norm(a, 5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(a), tf.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. å…¨å±€èŒƒæ•°è£å‰ª, è€ƒè™‘æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦çš„èŒƒæ•°, ç­‰æ¯”ä¾‹ç¼©æ”¾\n",
    "\n",
    "$$global\\_norm = \\sqrt{\\sum_i ||W^{i}||^2_2}$$\n",
    "\n",
    "$$W^{(i)} = \\frac {W^{(i)} \\cdot max\\_norm}{max(global\\_norm, max\\_norm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.random.normal([3, 3])\n",
    "w2 = tf.random.normal([3, 3])\n",
    "global_norm = tf.sqrt(tf.norm(w1) ** 2 + tf.norm(w2) ** 2)\n",
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ww1, ww2), global_norm = tf.clip_by_global_norm([w1, w2], 2)  # æ€»èŒƒæ•°é™åˆ¶ä¸º2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1, ww2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm2 = tf.sqrt(tf.norm(ww1) ** 2 + tf.norm(ww2) ** 2)\n",
    "global_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ç½‘ç»œè®­ç»ƒæ—¶ï¼Œæ¢¯åº¦è£å‰ªä¸€èˆ¬åœ¨è®¡ç®—å‡ºæ¢¯åº¦åï¼Œæ¢¯åº¦æ›´æ–°ä¹‹å‰è¿›è¡Œ\n",
    "\n",
    "### å¤„ç†æ¢¯åº¦å¼¥æ•£\n",
    "å¯¹äºæ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œå¯ä»¥é€šè¿‡å¢å¤§å­¦ä¹ ç‡ã€å‡å°‘ç½‘ç»œæ·±åº¦ã€æ·»åŠ  Skip Connection ç­‰ä¸€ç³»åˆ—çš„æªæ–½æŠ‘.\n",
    "\n",
    "å‡å°‘ç½‘ç»œæ·±åº¦å¯ä»¥å‡è½»æ¢¯åº¦å¼¥æ•£ç°è±¡, ä½†ä¼šå½±å“è¡¨è¾¾èƒ½åŠ›.\n",
    "\n",
    "ä½¿ç”¨æ·±åº¦æ®‹å·®ç½‘ç»œ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
