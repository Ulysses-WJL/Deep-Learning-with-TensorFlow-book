{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# å¾ªç¯ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input, Sequential, datasets\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åºåˆ—\n",
    "å…·æœ‰å…ˆåé¡ºåºçš„æ•°æ®ä¸€èˆ¬å«ä½œåºåˆ—(Sequence). æˆ‘ä»¬æŠŠæ–‡å­—ç¼–ç ä¸ºæ•°å€¼çš„è¿‡ç¨‹å«ä½œ**Word Embedding**.\n",
    "\n",
    "one-hotç¼–ç çš„ä¼˜ç¼ºç‚¹:\n",
    "- ç®€å•ç›´è§‚ï¼Œç¼–ç è¿‡ç¨‹ä¸éœ€è¦å­¦ä¹ å’Œè®­ç»ƒ;\n",
    "- ä½†é«˜ç»´åº¦è€Œä¸”æå…¶ç¨€ç–çš„ï¼Œå¤§é‡çš„ä½ç½®ä¸º0ï¼Œè®¡ç®—æ•ˆç‡è¾ƒä½, å¿½ç•¥äº†å•è¯å…ˆå¤©å…·æœ‰çš„è¯­ä¹‰ç›¸å…³æ€§;\n",
    "\n",
    "ä½™å¼¦ç›¸å…³åº¦(Cosine similarity), è¡¡é‡è¯å‘é‡(word vector)ä¹‹é—´ç›¸å…³åº¦:\n",
    "$$similarity(a, b) \\triangleq \\frac {a \\cdot b}{|a|\\cdot|b|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddingå±‚\n",
    "å•è¯çš„è¡¨ç¤ºå±‚å«ä½œEmbeddingå±‚, è´Ÿè´£æŠŠå•è¯ç¼–ç ä¸ºæŸä¸ªè¯å‘é‡ğ’—\n",
    "\n",
    "$$v = f_{\\theta}(i|N_{vocab}, n)$$\n",
    "å•è¯æ•°é‡è®°ä¸º$N_{vocab}$, $vçš„é•¿åº¦ä¸ºn$, $i$è¡¨ç¤ºå•è¯ç¼–å·, å¦‚2 è¡¨ç¤ºâ€œIâ€ï¼Œ3 è¡¨ç¤ºâ€œmeâ€ç­‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(10)  # ä»£è¡¨10ä¸ªä¸åŒå•è¯çš„ç¼–ç \n",
    "\n",
    "x = tf.random.shuffle(x)\n",
    "# 10ä¸ªå•è¯, æ¯ä¸ªå•è¯ç”¨é•¿åº¦4 çš„å‘é‡è¡¨ç¤º\n",
    "net = layers.Embedding(10, 4)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„è®­ç»ƒçš„è¯å‘é‡\n",
    "\n",
    "åº”ç”¨çš„æ¯”è¾ƒå¹¿æ³›çš„é¢„è®­ç»ƒæ¨¡å‹:Word2Vec å’ŒGloVeæ¨¡å‹.åˆ©ç”¨å·²é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°åˆå§‹åŒ–Embeddingå±‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "embedding_map = load_embed('glove.6B.50d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# åŠ è½½20newsgroupsæ•°æ®é›†\n",
    "news20 = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = news20.target_names  # ä¸€å…±20ç±»ä¸åŒçš„æ–°é—»\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news20['target']  # æ¯æ¡æ–°é—»åˆ†å±çš„ç±»åˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news20['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20['data'][0], category[news20['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # æœ€å¤šä¿ç•™ 20000-1 ä¸ªä¸åŒçš„å•è¯\n",
    "MAX_SEQUENCE_LENGTH = 1000  # æ¯ä¸ªåºåˆ—é•¿åº¦\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 50  # ç”¨50ç»´å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)  #  Only the most common `num_words-1` words will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates internal vocabulary based on a list of texts\n",
    "tokenizer.fit_on_texts(news20['data'])\n",
    "sequences = tokenizer.texts_to_sequences(news20['data'])  # è¯­å¥ -> å•è¯åºåˆ—å·ç»„æˆçš„sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = tokenizer.texts_to_matrix(news20['data'])\n",
    "# matrix.shape  # (11314, 20000)  ç¨€ç–çŸ©é˜µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†sequences è½¬æˆæ–‡æœ¬list\n",
    "# tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†å•è¯æ˜ å°„ä¸º index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "word_index_list = list(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»1å¼€å§‹ç¼–ç  ç”¨0ä»£è¡¨å¡«å……\n",
    "word_index_list[:10]  # news20group å‡ºç°é¢‘ç‡æœ€é«˜çš„10ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list[19998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "pad_sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯æ¡æ–°é—»éƒ½è¢«ç¼–ç æˆ ç­‰é•¿çš„ ç”¨æ•°å­—è¡¨ç¤ºçš„ åºåˆ—\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data), np.min(data) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=VALIDATION_SPLIT, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† å•è¯åºå·-> å•è¯å‘é‡(é•¿åº¦50)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.50d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new20groupä¸­æœ€å¸¸ç”¨çš„19999 è¯å‘é‡ + å¡«å…… + unknow\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    num_words, EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "preds = layers.Dense(len(category), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['loss'], label='loss')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "\n",
    "\n",
    "$$h_t = \\sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)$$\n",
    "åœ¨æ¯ä¸ªæ—¶é—´æˆ³$t$, ç½‘ç»œå±‚æ¥å—å½“å‰æ—¶é—´æˆ³çš„è¾“å…¥$x_t$å’Œä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„ç½‘ç»œçŠ¶æ€å‘é‡$h_{t-1}$,ç»è¿‡\n",
    "$$h_t = f_{\\theta}(h_{t-1}, x_t)$$\n",
    "å˜æ¢åå¾—åˆ°å½“å‰æ—¶é—´æˆ³çš„æ–°çŠ¶æ€å‘é‡$h_t$. åœ¨æ¯ä¸ªæ—¶é—´æˆ³ä¸Š, ç½‘ç»œå±‚å‡æœ‰è¾“å‡º$o_t = g_{\\phi}(h_t)$\n",
    "\n",
    "å¯¹äºè¿™ç§ç½‘ç»œç»“æ„ï¼Œæˆ‘ä»¬æŠŠå®ƒå«åšå¾ªç¯ç½‘ç»œç»“æ„(Recurrent Neural Networkï¼Œç®€ç§°RNN)ã€‚\n",
    "\n",
    "åœ¨å¾ªç¯ç¥ç»ç½‘ç»œä¸­ï¼Œæ¿€æ´»å‡½æ•°æ›´å¤šåœ°é‡‡ç”¨tanh å‡½æ•°.å¹¶ä¸”å¯ä»¥é€‰æ‹©ä¸ä½¿ç”¨åæ‰§ğ’ƒæ¥è¿›ä¸€æ­¥å‡å°‘å‚æ•°é‡ã€‚\n",
    "\n",
    "çŠ¶æ€å‘é‡$h_t$å¯ä»¥ç›´æ¥ç”¨ä½œè¾“å‡ºï¼Œå³$o_t = h_t$ï¼Œä¹Ÿå¯ä»¥å¯¹$t$åšä¸€ä¸ªç®€å•çš„çº¿æ€§å˜æ¢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¢¯åº¦ä¼ æ’­\n",
    "\n",
    "å‚æ•°$W_{hh}$çš„æ¢¯åº¦è®¡ç®—\n",
    "$$\\frac {\\partial L}{\\partial W_{hh}} = \\sum_{i=1}^t \\frac {\\partial L}{\\partial o_t}\n",
    "\\frac {\\partial o_t}{\\partial h_t} \\frac {\\partial h_t}{\\partial h_i}\n",
    "\\frac {\\partial^+ h_i}{\\partial W_{hh}}\n",
    "$$\n",
    "å…¶ä¸­\n",
    "$$\\frac {\\partial^+ h_i}{\\partial W_{hh}} = \\frac {\\partial \\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)}{\\partial W_{hh}}$$\n",
    "åªè€ƒè™‘ä¸€ä¸ªæ—¶é—´æˆ³çš„æ¢¯åº¦ä¼ æ’­, å³\"ç›´æ¥\"åå¯¼æ•°.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial h_t}{\\partial h_i} = \n",
    "\\frac {\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac {\\partial h_{t-1}}{\\partial h_{t-2}}\n",
    "\\cdots\n",
    "\\frac {\\partial h_{i+1}}{\\partial h_i}\n",
    "= \\prod_{k=i}^{t-1}\\frac {\\partial h_{k+1}}{\\partial h_{k}} $$\n",
    "\n",
    "\n",
    "$$\\frac {\\partial h_{k+1}}{\\partial h_{k}}\n",
    "= W^T_{hh}diag(\\sigma'(h_{k+1}))$$\n",
    "\n",
    "æ‰€ä»¥$$\\frac {\\partial h_t}{\\partial h_i} = \\prod_{j=i}^{t-1}diag(\\sigma'(W_{xh}x_{j+1} + W_{hh}h_j + b))W_{hh}$$\n",
    "\n",
    "å…¶ä¸­åŒ…å«é›…å…‹æ¯”çŸ©é˜µå’Œ$W_{hh}$çš„è¿ä¹˜è¿ç®—, å®¹æ˜“å‡ºç°æ¢¯åº¦æ¶ˆå¤±(æ¿€æ´»å‡½æ•°ä½¿ç”¨sigmoidæˆ–tanhæ—¶)æˆ–æ¢¯åº¦çˆ†ç‚¸(ä½¿ç”¨ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNå±‚çš„ä½¿ç”¨\n",
    "\n",
    "- SimpleRNNCell: å®Œæˆäº†ä¸€ä¸ªæ—¶é—´æˆ³çš„å‰å‘è¿ç®—($\\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)$)\n",
    "- SimpleRNN: åŸºäºCell å±‚å®ç°çš„ï¼Œå®ƒåœ¨å†…éƒ¨å·²ç»å®Œæˆäº†å¤šä¸ªæ—¶é—´æˆ³çš„å¾ªç¯è¿ç®—ï¼Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.SimpleRNNCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = layers.SimpleRNNCell(3)  # å†…å­˜å‘é‡hé•¿åº¦ 3\n",
    "cell.build(input_shape=(None, 4))  # è¾“å…¥xç‰¹å¾é•¿åº¦4\n",
    "cell.trainable_variables  # W_xh ,  W_hh, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰å‘è¿ç®—\n",
    "$$o_t, [h_t] = Cell(x_t, [h_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–çŠ¶æ€å‘é‡ï¼Œç”¨åˆ—è¡¨åŒ…è£¹ï¼Œç»Ÿä¸€æ ¼å¼\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "\n",
    "# (b, word_num, word_vec_length)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]  # æ‰€æœ‰å¥å­çš„ç¬¬ä¸€ä¸ªå•è¯\n",
    "\n",
    "cell = layers.SimpleRNNCell(64)\n",
    "out1, h1 = cell(xt, h0)  # h1ç”¨liståŒ…è£¹, out1æ²¡æœ‰ç»è¿‡å˜æ¢ = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape, h1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(out), id(h1[0]))  # çŠ¶æ€å‘é‡ç›´æ¥ä½œä¸ºè¾“å‡ºå‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "for x_t in tf.unstack(x, axis=1):  # æ—¶é—´ç»´åº¦è§£å¼€, æŒ‰æ—¶é—´è¾“å…¥å•è¯\n",
    "    out, h = cell(x_t, h)\n",
    "out = out  # åªå–æœ€åæ—¶é—´æˆ³çš„è¾“å‡º  N->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2å±‚å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]\n",
    "cell0 = layers.SimpleRNNCell(64)\n",
    "cell1 = layers.SimpleRNNCell(64)\n",
    "# 2ä¸ªcellçš„åˆå§‹çŠ¶æ€\n",
    "h0 = [tf.zeros((4, 64))]\n",
    "h1 = [tf.zeros((4, 64))]\n",
    "\n",
    "# ä¸€ä¸ªæ—¶é—´æˆ³ä¸Šå®Œæˆ2å±‚ä¼ æ’­åœ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æˆ³\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    \n",
    "    out1, h1 = cell1(out0, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…ˆå®Œæˆç¬¬ä¸€å±‚æ‰€æœ‰æ—¶é—´çš„ä¼ æ’­å†å®Œæˆç¬¬äºŒå±‚æ‰€æœ‰æ—¶é—´çš„ä¼ æ’­\n",
    "middle_seqences = []\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    middle_seqences.append(out0)\n",
    "\n",
    "for xt in middle_seqences:\n",
    "    out1, h1 = cell1(xt, h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN  å®Œæˆå¤šä¸ªæ—¶é—´æˆ³çš„è®¡ç®—\n",
    "layer = layers.SimpleRNN(64)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿”å›æ‰€æœ‰æ—¶é—´æˆ³ä¸Šçš„è¾“å‡º\n",
    "layer = layers.SimpleRNN(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šå±‚RNNç½‘ç»œ\n",
    "net = Sequential([\n",
    "    # é™¤æœ€æœ«å±‚å¤–ï¼Œéƒ½éœ€è¦è¿”å›æ‰€æœ‰æ—¶é—´æˆ³çš„è¾“å‡ºï¼Œç”¨ä½œä¸‹ä¸€å±‚çš„è¾“å…¥\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNæƒ…æ„Ÿåˆ†ç±»\n",
    "imdbè¯„åˆ†>7 ä¸º1 positive; IMDB è¯„çº§<5 çš„ç”¨æˆ·è¯„ä»·æ ‡æ³¨ä¸º0 \n",
    "\n",
    "åˆ©ç”¨ç¬¬2 å±‚RNN å±‚çš„æœ€åæ—¶é—´æˆ³çš„çŠ¶æ€å‘é‡h, ä½œä¸ºå¥å­çš„å…¨å±€è¯­ä¹‰ç‰¹å¾è¡¨ç¤º, é€å…¥å…¨è¿æ¥åˆ†ç±»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # è¯æ±‡è¡¨å¤§å°\n",
    "MAX_REVIEW_LEN = 80  # å¥å­é•¿åº¦\n",
    "EMBEDDING_LEN = 50  # è¯å‘é‡é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.imdb.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdbæ•°æ®é›†\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(\n",
    "    num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25000,) 218 (25000,)\n"
    }
   ],
   "source": [
    "print(X_train.shape, len(X_train[0]), y_train.shape)  # X ä¸ç­‰é•¿çš„list ç»„æˆçš„array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25000,) 68 (25000,)\n"
    }
   ],
   "source": [
    "print(X_test.shape, len(X_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1646592/1641221 [==============================] - 40s 24us/step\n('fawn', 34701)\n('tsukino', 52006)\n('nunnery', 52007)\n('sonja', 16816)\n('vani', 63951)\n('woods', 1408)\n('spiders', 16115)\n('hanging', 2345)\n('woody', 2289)\n('trawling', 52008)\n"
    }
   ],
   "source": [
    "# ç¼–ç è¡¨\n",
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # å•è¯-æ•°å­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total 88584 unique words\n"
    }
   ],
   "source": [
    "print(f'total {len(word_index)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ æ ‡å¿—ä½\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # è¡¨ç¤ºå¡«å……\n",
    "word_index[\"<START>\"] = 1  # è¡¨ç¤ºèµ·å§‹\n",
    "word_index[\"<UNK>\"] = 2  # è¡¨ç¤ºæœªçŸ¥å•è¯\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# ç¿»è½¬\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # æ•°å­—åºåˆ— -> æ–‡æœ¬\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆªæ–­ å¡«å…… æˆç­‰é•¿çš„åºåˆ—\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\""
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "decode_review(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # èˆå¼ƒæœ€åä¸€ç»„ \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(128, 80), dtype=int32, numpy=\n array([[   2,   19,    6, ...,    4,  673,  394],\n        [  10,    4,  226, ...,   10,    4, 8405],\n        [  16, 3444,  402, ...,   23,   61, 7698],\n        ...,\n        [   0,    0,    0, ...,   22,    7, 7553],\n        [ 184,   76,  307, ...,    6, 3322, 1690],\n        [5167,   73,  224, ...,  111,  128,  663]], dtype=int32)>,\n <tf.Tensor: shape=(128,), dtype=int64, numpy=\n array([0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0])>)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sample = next(iter(train_db))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9107ef665f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.100d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found %s word vectors.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fa5a9f5d8a28>\u001b[0m in \u001b[0;36mload_embed\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membedding_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "9793 (10000, 50)\n"
    }
   ],
   "source": [
    "# å°† å•è¯åºå·-> å•è¯å‘é‡(é•¿åº¦50)\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.50d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # åˆå§‹çŠ¶æ€å‘é‡\n",
    "        self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # è¯åµŒå…¥å±‚\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "#                                           weights=[embedding_matrix],\n",
    "#                                          trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n",
    "            layers.SimpleRNN(units, dropout=0.5)\n",
    "        ])\n",
    "        # åˆ†ç±»å±‚\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "        state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # æœ€æœ«å±‚ æœ€åä¸€ä¸ªæ—¶é—´æˆ³çš„è¾“å‡º\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(64)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(10e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'],\n",
    "#              experimental_run_tf_function=False  # ä»¥cellæ–¹å¼è¿è¡Œéœ€è¦è®¾ç½®\n",
    "             )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, MAX_REVIEW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"my_rnn\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  500000    \n_________________________________________________________________\nsequential (Sequential)      multiple                  15616     \n_________________________________________________________________\nsequential_1 (Sequential)    multiple                  2113      \n=================================================================\nTotal params: 517,729\nTrainable params: 517,729\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 195 steps, validate for 195 steps\nEpoch 1/10\n195/195 [==============================] - 20s 103ms/step - loss: 0.7010 - accuracy: 0.4946 - val_loss: 0.6934 - val_accuracy: 0.4998\nEpoch 2/10\n195/195 [==============================] - 18s 94ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.5000\nEpoch 3/10\n195/195 [==============================] - 18s 90ms/step - loss: 0.6933 - accuracy: 0.4998 - val_loss: 0.6932 - val_accuracy: 0.4998\nEpoch 4/10\n195/195 [==============================] - 17s 89ms/step - loss: 0.6933 - accuracy: 0.5003 - val_loss: 0.6934 - val_accuracy: 0.5001\nEpoch 5/10\n195/195 [==============================] - 18s 91ms/step - loss: 0.6934 - accuracy: 0.5032 - val_loss: 0.6932 - val_accuracy: 0.5020\nEpoch 6/10\n195/195 [==============================] - 18s 90ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\nEpoch 7/10\n195/195 [==============================] - 17s 88ms/step - loss: 0.6933 - accuracy: 0.4984 - val_loss: 0.6933 - val_accuracy: 0.5000\nEpoch 8/10\n195/195 [==============================] - 17s 88ms/step - loss: 0.6932 - accuracy: 0.4965 - val_loss: 0.6934 - val_accuracy: 0.5001\nEpoch 9/10\n195/195 [==============================] - 18s 93ms/step - loss: 0.6933 - accuracy: 0.4964 - val_loss: 0.6932 - val_accuracy: 0.4999\nEpoch 10/10\n195/195 [==============================] - 17s 89ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.5001\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f7661d96450>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model.fit(train_db, epochs=10, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¢¯åº¦å¼¥æ•£å’Œæ¢¯åº¦çˆ†ç‚¸\n",
    "æ¢¯åº¦ä¸‹é™\n",
    "$$\\theta := \\theta - \\eta\\nabla_{\\theta} L$$\n",
    "\n",
    "- æ¢¯åº¦å¼¥æ•£(Gradient Vanishing): $\\nabla_{\\theta} L \\approx 0$, æ¯æ¬¡æ¢¯åº¦æ›´æ–°åå‚æ•°åŸºæœ¬ä¿æŒä¸å˜, â„’å‡ ä¹ä¿æŒä¸å˜ï¼Œå…¶å®ƒè¯„æµ‹æŒ‡æ ‡ï¼Œå¦‚å‡†ç¡®åº¦ï¼Œä¹Ÿä¿æŒä¸å˜\n",
    "- æ¢¯åº¦çˆ†ç‚¸(Gradient Exploding): $\\nabla_{\\theta} L \\gg 1$, æ¢¯åº¦æ›´æ–°çš„æ­¥é•¿å¾ˆå¤§, æ›´æ–°åçš„$\\theta$å˜åŒ–å¾ˆå¤§, Lå‡ºç°çªå˜ç°è±¡ï¼Œç”šè‡³å¯èƒ½å‡ºç°æ¥å›éœ‡è¡ã€ä¸æ”¶æ•›çš„ç°è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2])\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # è·å–ç‰¹å¾å€¼\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ¬¡è¿ä¹˜\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2èŒƒæ•°\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2]) * 0.4\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # è·å–ç‰¹å¾å€¼\n",
    "# å¤šæ¬¡è¿ä¹˜\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2èŒƒæ•°\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))\n",
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¢¯åº¦è£å‰ª(Gradient Clipping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ç®€å•è£å‰ª, ç›´æ¥å¯¹å¼ é‡çš„æ•°å€¼è¿›è¡Œé™å¹…\n",
    "\n",
    "a = tf.random.uniform([2, 2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_value(a, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2conda2a75a45106264ceab7472c43279a5d24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}