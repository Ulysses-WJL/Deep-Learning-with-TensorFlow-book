{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input, Sequential, datasets\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(gpu)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列\n",
    "具有先后顺序的数据一般叫作序列(Sequence). 我们把文字编码为数值的过程叫作**Word Embedding**.\n",
    "\n",
    "one-hot编码的优缺点:\n",
    "- 简单直观，编码过程不需要学习和训练;\n",
    "- 但高维度而且极其稀疏的，大量的位置为0，计算效率较低, 忽略了单词先天具有的语义相关性;\n",
    "\n",
    "余弦相关度(Cosine similarity), 衡量词向量(word vector)之间相关度:\n",
    "$$similarity(a, b) \\triangleq \\frac {a \\cdot b}{|a|\\cdot|b|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层\n",
    "单词的表示层叫作Embedding层, 负责把单词编码为某个词向量𝒗\n",
    "\n",
    "$$v = f_{\\theta}(i|N_{vocab}, n)$$\n",
    "单词数量记为$N_{vocab}$, $v的长度为n$, $i$表示单词编号, 如2 表示“I”，3 表示“me”等."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(10)  # 代表10个不同单词的编码\n",
    "\n",
    "x = tf.random.shuffle(x)\n",
    "# 10个单词, 每个单词用长度4 的向量表示\n",
    "net = layers.Embedding(10, 4)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预训练的词向量\n",
    "\n",
    "应用的比较广泛的预训练模型:Word2Vec 和GloVe模型.利用已预训练好的模型参数初始化Embedding层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # 建立映射关系: 单词: 词向量(长度50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_map = load_embed('glove.6B.50d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# 加载20newsgroups数据集\n",
    "news20 = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = news20.target_names  # 一共20类不同的新闻\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news20['target']  # 每条新闻分属的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news20['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20['data'][0], category[news20['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # 最多保留 20000-1 个不同的单词\n",
    "MAX_SEQUENCE_LENGTH = 1000  # 每个序列长度\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 50  # 用50维向量表示一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?  # 令牌化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)  #  Only the most common `num_words-1` words will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates internal vocabulary based on a list of texts\n",
    "tokenizer.fit_on_texts(news20['data'])\n",
    "sequences = tokenizer.texts_to_sequences(news20['data'])  # 语句 -> 单词序列号组成的sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = tokenizer.texts_to_matrix(news20['data'])\n",
    "# matrix.shape  # (11314, 20000)  稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将sequences 转成文本list\n",
    "# tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单词映射为 index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "word_index_list = list(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从1开始编码 用0代表填充\n",
    "word_index_list[:10]  # news20group 出现频率最高的10个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list[19998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "pad_sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每条新闻都被编码成 等长的 用数字表示的 序列\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data), np.min(data) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=VALIDATION_SPLIT, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 单词序号-> 单词向量(长度50)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # 根据glove.6B.50d 将单词转为词向量\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new20group中最常用的19999 词向量 + 填充 + unknow\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    num_words, EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "# 使用卷积\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "preds = layers.Dense(len(category), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['loss'], label='loss')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络\n",
    "\n",
    "\n",
    "$$h^{(t)} = \\sigma(Ux^{(t)} + Wh^{(t-1)} + b)$$\n",
    "在每个时间戳$t$, 网络层接受当前时间戳的输入$x^{(t)}$和上一个时间戳的网络状态向量$h^{(t-1)}$,经过\n",
    "$$h^{(t)} = f_{\\theta}(h^{(t-1)}, x^{(t)})$$\n",
    "变换后得到当前时间戳的新状态向量$h^{(t)}$. 在每个时间戳上, 网络层均有输出$o^{(t)} = g_{\\phi}(h^{(t)})$\n",
    "\n",
    "对于这种网络结构，我们把它叫做循环网络结构(Recurrent Neural Network，简称RNN)。\n",
    "\n",
    "在循环神经网络中，激活函数更多地采用tanh 函数.并且可以选择不使用偏执𝒃来进一步减少参数量。\n",
    "\n",
    "状态向量$h^{(t)}$可以直接用作输出，即$o^{(t)} = h^{(t)}$，也可以对$t$做一个简单的线性变换.\n",
    "![](./images/rnnbp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度传播\n",
    "\n",
    "参数$W_{hh}$的梯度计算, RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。\n",
    "$$\n",
    "\\frac{\\partial L^{(t)}}{\\partial V}=\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\cdot \\frac{\\partial o^{(t)}}{\\partial V}\n",
    "$$\n",
    "$$\n",
    "L=\\sum_{t=1}^{n}L^{(t)}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial V}=\\sum_{t=1}^{n}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\cdot \\frac{\\partial o^{(t)}}{\\partial V}\n",
    "$$\n",
    "对于$W和U$, 求解需要涉及到历史数据\n",
    "$$\n",
    "\\frac{\\partial L^{(t)}}{\\partial W}=\\sum_{k=0}^{t}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}(\\prod_{j=k+1}^{t}\\frac{\\partial h^{(j)}}{\\partial h^{(j-1)}})\\frac{\\partial h^{(k)}}{\\partial W}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L^{(t)}}{\\partial U}=\\sum_{k=0}^{t}\\frac{\\partial L^{(t)}}{\\partial o^{(t)}}\\frac{\\partial o^{(t)}}{\\partial h^{(t)}}(\\prod_{j=k+1}^{t}\\frac{\\partial h^{(j)}}{\\partial h^{(j-1)}})\\frac{\\partial h^{(k)}}{\\partial U}\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\frac {\\partial h^{(k)}}{\\partial W} = \\frac {\\partial \\sigma(Ux^{(k)} + Wh^{(k-1)} +b)}{\\partial W}\n",
    "$$\n",
    "只考虑一个时间戳的梯度传播, 即\"直接\"偏导数.\n",
    "\n",
    "$$\n",
    "\\frac {\\partial h^{(k+1)}}{\\partial h^{(k)}}\n",
    "= W^T diag( \\sigma'(Ux^{(k+1)} + Wh^{(k)} + b))\n",
    "$$\n",
    "\n",
    "整体的偏导公式就是将其按时刻再一一加起来。\n",
    "\n",
    "在某个时刻的对$W$或是$U$的偏导数，需要追溯这个时刻之前所有时刻的信息, 整体的偏导公式就是将其按时刻再一一加起来。\n",
    "\n",
    "公式中包含雅克比矩阵和$W$的连乘运算, 容易出现梯度消失(激活函数使用sigmoid或tanh时)或梯度爆炸(使用ReLU)\n",
    "\n",
    "\n",
    "<!--\n",
    "$$\n",
    "\\frac {\\partial h_t}{\\partial h_i} = \n",
    "\\frac {\\partial h_t}{\\partial h_{t-1}}\n",
    "\\frac {\\partial h_{t-1}}{\\partial h_{t-2}}\n",
    "\\cdots\n",
    "\\frac {\\partial h_{i+1}}{\\partial h_i}\n",
    "= \\prod_{k=i}^{t-1}\\frac {\\partial h_{k+1}}{\\partial h_{k}} \n",
    "$$\n",
    "$$\n",
    "\\frac {\\partial h_t}{\\partial h_i} = \\prod_{j=i}^{t-1}diag(\\sigma'(W_{xh}x_{j+1} + W_{hh}h_j + b))W_{hh}\n",
    "$$\n",
    "-->\n",
    "\n",
    "\n",
    "[循环神经网络(RNN)模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6509630.html)\n",
    "\n",
    "[LSTM模型与前向反向传播算法](https://www.cnblogs.com/pinard/p/6519110.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell:\n",
    "    \"\"\"\n",
    "    一个时间戳的前向运算\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation='tanh', labels=2, random_state=0):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.labels = labels\n",
    "        self.random_state = random_state\n",
    "        self.U = None  # [n_h, n_x]\n",
    "        self.V = None  # [n_o, n_h]\n",
    "        self.W = None  # [n_h, n_h]\n",
    "        self.b = None  # [n_h, 1]\n",
    "        self.c = None  # [n_o, 1]\n",
    "        self._built = False\n",
    "        self.y_hat = None\n",
    "    \n",
    "    @property\n",
    "    def built(self):\n",
    "        return self._built\n",
    "    \n",
    "    @built.setter\n",
    "    def built(self, value):\n",
    "        self._built = value\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        _, n_x = input_shape\n",
    "        n_h, n_o = self.units, self.labels\n",
    "        np.random.seed(self.random_state)\n",
    "        self.W = np.random.randn(n_h, n_h)\n",
    "        self.U = np.random.randn(n_h, n_x)\n",
    "        self.V = np.random.randn(n_o, n_h)\n",
    "        self.b = np.zeros([n_h, 1])\n",
    "        self.c = np.zeros([n_o, 1])\n",
    "        \n",
    "    def softmax(self, o):\n",
    "        ex = np.exp(o - np.max(o))\n",
    "        y_hat = ex / np.sum(ex, axis=0)\n",
    "        return y_hat\n",
    "    \n",
    "    def loss(self, y_true, y_hat):\n",
    "        \"\"\"\n",
    "        某一时刻的损失值\n",
    "        使用softmax函数作为o的激活函数\n",
    "        y_hat: [n_o, b]\n",
    "        \"\"\"\n",
    "        loss = np.sum(-y_true * np.log(y_hat), axis=0)  # [1, b] 各样本的损失\n",
    "        return np.mean(loss)\n",
    "\n",
    "    \n",
    "    def activate(self, z):\n",
    "        if self.activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1/ (1 + np.exp(-z))\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        output = self.call(*args, **kwargs)\n",
    "        return output\n",
    "        \n",
    "    def call(self, xt, ht_1):\n",
    "        \"\"\"\n",
    "        xt: [b, n_x(word_vec_len)]\n",
    "        ht_1:［n_h(units), b］\n",
    "        \"\"\"\n",
    "        if not self.built:\n",
    "            self.build(xt.shape)\n",
    "            self.build = True\n",
    "        xt = xt.T\n",
    "        z = self.U @ xt + self.W @ ht_1 + self.b\n",
    "        ht = self.activate(z)      \n",
    "        o = self.V @ ht + self.c\n",
    "        y_hat = self.softmax(o)\n",
    "        # loss = self.loss(y)\n",
    "        self.y_hat = y_hat\n",
    "        return o, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.random.randn(10, 100)\n",
    "h0 =np.random.randn(10, 64)\n",
    "\n",
    "my_cell = MyRNNCell(64, labels=2)\n",
    "\n",
    "o, h1 = my_cell(xt, h0.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN:\n",
    "    def __init__(self, units, activation='tanh', labels=2, random_state=0):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.labels = labels\n",
    "        self.random_state = random_state\n",
    "        self.state0 = None\n",
    "        self.rnn_cell = MyRNNCell(units, activation, labels, random_state)\n",
    "        self.losses = []\n",
    "        self.y_hats = []\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        _, times, n_x = input_shape\n",
    "        self.rnn_cell.build((None, n_x))\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        output = self.call(*args, **kwargs)\n",
    "        return output  \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if isinstance(inputs, tuple):\n",
    "            X, y = inputs\n",
    "        else:\n",
    "            X = inputs\n",
    "        b, length, _ = X.shape\n",
    "        h = np.zero([self.units, b])\n",
    "        \n",
    "        for t in range(length):\n",
    "            out, h = self.rnn_cell(X[:, t, :], h)\n",
    "    \n",
    "    def softmax(self, o):\n",
    "        ex = np.exp(o - np.max(o))\n",
    "        y_hat = ex / np.sum(ex, axis=0)\n",
    "        return y_hat\n",
    "    \n",
    "    def loss(self, y_true, y_hat):\n",
    "        \"\"\"\n",
    "        某一时刻的损失值\n",
    "        使用softmax函数作为o的激活函数\n",
    "        o: [n_o, b]\n",
    "        y_true: [n_o, b]\n",
    "        \"\"\"\n",
    "        loss = np.sum(-y_true * np.log(y_hat), axis=0)  # [1, b] 各样本的损失\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def feed_forward(self, X, y):\n",
    "        b, length, _ = X.shape\n",
    "        h = np.zero([self.units, b])\n",
    "        for t in range(length):\n",
    "            out, h = self.rnn_cell(X[:, t, :], h)\n",
    "            y_hat = self.rnn_cell.y_hat\n",
    "            self.y_hats.append(y_hat)\n",
    "            loss = self.loss(y.T, y_hat)\n",
    "            self.losses.append(loss)\n",
    "        return out, h\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        dl_dv \n",
    "        time = len(self.losses)\n",
    "        for i in reversed(time):\n",
    "            loss = self.losses[i]\n",
    "            y_hat = self.y_hats[i]\n",
    "            dl_do_t =  y - y_hat  # dl^t/do^t = y^t - y_hat^t\n",
    "            dl_dv += dl_do_t * h_t.T  # dL/dV = \\sum_t dL/do^t  do^t/dV\n",
    "            dh_dz_t = 1 - h_t ** 2  # z= Ux^t+ Wh^t-1 + b,  h^t = tanh(z)\n",
    "            dh_dW_t = dh_dz_t * h_t_1.T\n",
    "    \n",
    "    def fit(self, X, y, epoch=20):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 10), (10, 2))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ht1.shape, o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN层的使用\n",
    "\n",
    "- SimpleRNNCell: 完成了一个时间戳的前向运算($\\sigma(W_{xh}x_t + W_{hh}h_{t-1} +b)$)\n",
    "- SimpleRNN: 基于Cell 层实现的，它在内部已经完成了多个时间戳的循环运算，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.SimpleRNNCell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = layers.SimpleRNNCell(3)  # 内存向量h长度 3\n",
    "# cell.build(input_shape=(None, 4))  # 输入x特征长度4\n",
    "# cell.trainable_variables  # W_xh ,  W_hh, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向运算\n",
    "$$o_t, [h_t] = Cell(x_t, [h_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化状态向量，用列表包裹，统一格式\n",
    "h0 = [tf.zeros([4, 64])]\n",
    "\n",
    "# (b, word_num, word_vec_length)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]  # 所有句子的第一个单词\n",
    "\n",
    "cell = layers.SimpleRNNCell(64)\n",
    "out, h1 = cell(xt, h0)  # h1用list包裹, out1没有经过变换 = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4, 64]), TensorShape([4, 64]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, h1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id(out), id(h1[0]))  # 状态向量直接作为输出向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h0\n",
    "for x_t in tf.unstack(x, axis=1):  # 时间维度解开, 按时间输入单词\n",
    "    out, h = cell(x_t, h)\n",
    "out = out  # 只取最后时间戳的输出  N->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2层循环神经网络\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "xt = x[:, 0, :]\n",
    "cell0 = layers.SimpleRNNCell(64)\n",
    "cell1 = layers.SimpleRNNCell(64)\n",
    "# 2个cell的初始状态\n",
    "h0 = [tf.zeros((4, 64))]\n",
    "h1 = [tf.zeros((4, 64))]\n",
    "\n",
    "# 一个时间戳上完成2层传播在到下一个时间戳\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    \n",
    "    out1, h1 = cell1(out0, h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先完成第一层所有时间的传播再完成第二层所有时间的传播\n",
    "middle_seqences = []\n",
    "\n",
    "for xt in tf.unstack(x, axis=1):\n",
    "    out0, h0 = cell0(xt, h0)\n",
    "    middle_seqences.append(out0)\n",
    "\n",
    "for xt in middle_seqences:\n",
    "    out1, h1 = cell1(xt, h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN  完成多个时间戳的计算\n",
    "layer = layers.SimpleRNN(64)\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回所有时间戳上的输出\n",
    "layer = layers.SimpleRNN(64, return_sequences=True)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层RNN网络\n",
    "net = Sequential([\n",
    "    # 除最末层外，都需要返回所有时间戳的输出，用作下一层的输入\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN情感分类\n",
    "imdb评分>7 为1 positive; IMDB 评级<5 的用户评价标注为0 \n",
    "\n",
    "利用第2 层RNN 层的最后时间戳的状态向量h, 作为句子的全局语义特征表示, 送入全连接分类网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "TOTAL_WORDS = 10000  # 词汇表大小\n",
    "MAX_REVIEW_LEN = 80  # 句子长度\n",
    "EMBEDDING_LEN = 100  # 词向量长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.imdb.load_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb数据集\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(\n",
    "    num_words=TOTAL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) 218 (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, len(X_train[0]), y_train.shape)  # X 不等长的list 组成的array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) 68 (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape, len(X_test[0]), y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fawn', 34701)\n",
      "('tsukino', 52006)\n",
      "('nunnery', 52007)\n",
      "('sonja', 16816)\n",
      "('vani', 63951)\n",
      "('woods', 1408)\n",
      "('spiders', 16115)\n",
      "('hanging', 2345)\n",
      "('woody', 2289)\n",
      "('trawling', 52008)\n"
     ]
    }
   ],
   "source": [
    "# 编码表\n",
    "word_index = datasets.imdb.get_word_index()\n",
    "\n",
    "pre_10 = list(word_index.items())[:10]\n",
    "for item in pre_10:  \n",
    "    print(item)  # 单词-数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88584 unique words\n"
     ]
    }
   ],
   "source": [
    "print(f'total {len(word_index)} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加标志位\n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0  # 表示填充\n",
    "word_index[\"<START>\"] = 1  # 表示起始\n",
    "word_index[\"<UNK>\"] = 2  # 表示未知单词\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# 翻转\n",
    "index_word = dict([(v, k) for k, v in word_index.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    # 数字序列 -> 文本\n",
    "    return ' '.join([index_word.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断 填充 成等长的序列\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices(  # 舍弃最后一组 \n",
    "    (X_train, y_train)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_test, y_test)).shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=208, shape=(128, 80), dtype=int32, numpy=\n",
       " array([[   4,   65,  410, ...,   12,  199,  211],\n",
       "        [2730,   10,   10, ...,   46,    7,  158],\n",
       "        [6018,    5,    4, ..., 2639, 2361, 2916],\n",
       "        ...,\n",
       "        [6857,  949,    4, ...,    5,  901,  128],\n",
       "        [   0,    0,    0, ...,  292,   17,   73],\n",
       "        [6233,  699,  102, ...,   46,    7,  158]])>,\n",
       " <tf.Tensor: id=209, shape=(128,), dtype=int64, numpy=\n",
       " array([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_db))\n",
    "sample[0], sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_map = load_embed('glove.6B.100d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9793 (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "# 将 单词序号-> 单词向量(长度50)\n",
    "num_words = min(TOTAL_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_LEN))\n",
    "\n",
    "applied_vec_count = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= TOTAL_WORDS:\n",
    "        continue\n",
    "    # 根据glove.6B.50d 将单词转为词向量\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        applied_vec_count += 1\n",
    "print(applied_vec_count, embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # 初始状态向量\n",
    "        self.state0 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        self.state1 = [tf.zeros([BATCH_SIZE, units])]\n",
    "        # 词嵌入层\n",
    "        self.embedding = layers.Embedding(TOTAL_WORDS, EMBEDDING_LEN,\n",
    "                                          input_length=MAX_REVIEW_LEN,\n",
    "#                                           weights=[embedding_matrix],\n",
    "#                                          trainable=False\n",
    "                                         )\n",
    "        # RNNCell\n",
    "#         self.runcell0 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "#         self.runcell1 = layers.SimpleRNNCell(units, dropout=0.5)\n",
    "        # RNN layer\n",
    "        self.rnn = Sequential([\n",
    "            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n",
    "            layers.SimpleRNN(units, dropout=0.5)\n",
    "        ])\n",
    "        # 分类层\n",
    "        self.out_layer = Sequential([\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(rate=0.5),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.embedding(inputs)\n",
    "        state0, state1 = self.state0, self.state1\n",
    "#         for word in tf.unstack(x, axis=1):\n",
    "#             out0, state0 = self.runcell0(word, state0, training)\n",
    "#             out1, state1 = self.runcell1(out0, state1, training)\n",
    "        out1 = self.rnn(x)\n",
    "        # 最末层 最后一个时间戳的输出\n",
    "        out = self.out_layer(out1, training)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(64)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(10e-3),\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy'],\n",
    "#              experimental_run_tf_function=False  # 以cell方式运行需要设置\n",
    "             )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build((None, MAX_REVIEW_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_db, epochs=10, validation_data=test_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度弥散和梯度爆炸\n",
    "梯度下降\n",
    "$$\\theta := \\theta - \\eta\\nabla_{\\theta} L$$\n",
    "\n",
    "- 梯度弥散(Gradient Vanishing): $\\nabla_{\\theta} L \\approx 0$, 每次梯度更新后参数基本保持不变, ℒ几乎保持不变，其它评测指标，如准确度，也保持不变\n",
    "- 梯度爆炸(Gradient Exploding): $\\nabla_{\\theta} L \\gg 1$, 梯度更新的步长很大, 更新后的$\\theta$变化很大, L出现突变现象，甚至可能出现来回震荡、不收敛的现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2])\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # 获取特征值\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多次连乘\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2范数\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.ones([2, 2]) * 0.4\n",
    "eigenvalues = tf.linalg.eigh(W)[0]  # 获取特征值\n",
    "# 多次连乘\n",
    "val = [W]\n",
    "for _ in range(10):\n",
    "    val.append(val[-1]@W)\n",
    "\n",
    "# L2范数\n",
    "norm = list(map(lambda x:tf.norm(x).numpy(), val))\n",
    "plt.plot(norm)\n",
    "plt.xlabel('n times')\n",
    "plt.ylabel('L2-norm')\n",
    "# Gradient Vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度裁剪(Gradient Clipping)\n",
    "\n",
    "梯度爆炸可以通过梯度裁剪(Gradient Clipping)的方式在一定程度上的解决"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 简单裁剪, 直接对张量的数值进行限幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = tf.random.uniform([2, 2])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_value(a, 0.4, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 限制梯度张量W的范数\n",
    "$$W' = \\frac {W}{||W||_2} \\cdot max$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform([2, 2]) * 5\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.clip_by_norm(a, 5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(a), tf.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 全局范数裁剪, 考虑所有参数的梯度的范数, 等比例缩放\n",
    "\n",
    "$$global\\_norm = \\sqrt{\\sum_i ||W^{i}||^2_2}$$\n",
    "\n",
    "$$W^{(i)} = \\frac {W^{(i)} \\cdot max\\_norm}{max(global\\_norm, max\\_norm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.random.normal([3, 3])\n",
    "w2 = tf.random.normal([3, 3])\n",
    "global_norm = tf.sqrt(tf.norm(w1) ** 2 + tf.norm(w2) ** 2)\n",
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.clip_by_global_norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ww1, ww2), global_norm = tf.clip_by_global_norm([w1, w2], 2)  # 总范数限制为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww1, ww2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_norm2 = tf.sqrt(tf.norm(ww1) ** 2 + tf.norm(ww2) ** 2)\n",
    "global_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网络训练时，梯度裁剪一般在计算出梯度后，梯度更新之前进行\n",
    "\n",
    "### 处理梯度弥散\n",
    "对于梯度弥散现象，可以通过增大学习率、减少网络深度、添加 Skip Connection 等一系列的措施抑.\n",
    "\n",
    "减少网络深度可以减轻梯度弥散现象, 但会影响表达能力.\n",
    "\n",
    "使用深度残差网络"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
