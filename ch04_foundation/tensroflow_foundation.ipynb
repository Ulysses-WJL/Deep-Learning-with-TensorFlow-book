{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TesorFlow 基础\n",
    "\n",
    "\n",
    "TensorFlow 是一个面向深度学习算法的科学计算库，内部数据保存在张量(Tensor)对象上，所有的运算操作(Operation，简称 OP)也都是基于张量对象进行的。复杂的神经网络算法本质上就是各种张量相乘、相加等基本运算操作的组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, datasets, preprocessing\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['STKaiTi']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "np.set_printoptions(threshold=16, suppress=True, precision=4)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据类型\n",
    "### 1.1数值类型\n",
    "\n",
    "数值类型的张量是 TensorFlow 的主要数据载体，根据维度数来区分，可分为：\n",
    "- 标量(scalar): 单个的实数，如 1.2, 3.4 等，维度(Dimension)数为 0，shape 为[]\n",
    "- 向量(vector): 𝑛个实数的有序集合，通过中括号包裹，如\\[1.2\\]，\\[1.2, 3.4\\]等，维度数为1，长度不定，shape为\\[n\\]\n",
    "- 张量(Tensor): 所有维度数dim > 2的数组统称为张量. 张量的每个维度也作轴(axis), 一般维度代表了具体的物理含义.\n",
    "  \\[2, 32, 32, 3\\] -> 2张图片,高宽均为32,有RGB3个通道\n",
    "\n",
    "在 TensorFlow 中间，为了表达方便，一般把标量、向量、矩阵也统称为张量，不作区分，需要根据张量的维度数或形状自行判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(float, tensorflow.python.framework.ops.EagerTensor, True)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量\n",
    "a = 2.4\n",
    "aa = tf.constant(1.2)  # 需要使用tf的方式创建张量, 否则不能使用tf的功能函数\n",
    "type(a), type(aa), tf.is_tensor(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量\n",
    "a = tf.constant([1, 2])  # 向量的定义须通过 List 容器传给constant()函数\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1. , 2.1], dtype=float32)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.constant([1, 2.1])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1. , 2.1], dtype=float32)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()  # 返回numpy类型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n array([[1, 2],\n        [3, 4]], dtype=int32)>,\n TensorShape([2, 2]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义矩阵\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[1, 2],\n        [3, 4]],\n\n       [[5, 6],\n        [7, 8]]], dtype=int32)>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3维的张量\n",
    "a = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 字符串(String)类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=string, numpy=b'Hello, TensorFlow'>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.constant('Hello, TensorFlow')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(), dtype=string, numpy=b'hello, tensorflow'>,\n <tf.Tensor: shape=(), dtype=string, numpy=b'HELLO, TENSORFLOW'>)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.lower(s), tf.strings.upper(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Hello,', b'TensorFlow'], dtype=object)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.split(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 布尔(Boolean)类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.constant([True, False])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow 的bool类型与python的bool类型不等价\n",
    "a is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == True  # 数值比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 数值精度\n",
    "对于数值类型的张量，可以保存为不同字节长度的精度,位越长，精度越高，同时占用的内存空间也就越大。常用的精度类型有 tf.int16、tf.int32、tf.int64、tf.float16、tf.float32、tf.float64(tf.double)等，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(), dtype=int16, numpy=-13035>,\n <tf.Tensor: shape=(), dtype=int32, numpy=123456789>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 制定数值的精度\n",
    "a = tf.constant(123456789, dtype=tf.int16)  # -32768 ~ +32767\n",
    "b = tf.constant(123456789, dtype=tf.int32)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(), dtype=float32, numpy=3.1415927>,\n <tf.Tensor: shape=(), dtype=float64, numpy=3.141592653589793>)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pi_1 = tf.constant(np.pi, dtype=tf.float32)\n",
    "pi_2 = tf.constant(np.pi, dtype=tf.float64)\n",
    "pi_1, pi_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大部分深度学习算法，一般使用 tf.int32 和 tf.float32 可满足大部分场合的运算精度要求(默认都是int32与float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "before: <dtype: 'float32'>\nafter: <dtype: 'float64'> tf.Tensor(3.1415927410125732, shape=(), dtype=float64)\n"
    }
   ],
   "source": [
    "# dtype 成员属性可以判断张量的保存精度\n",
    "\n",
    "print('before:', pi_1.dtype)\n",
    "if pi_1.dtype != tf.float64:\n",
    "    pi_1 = tf.cast(pi_1, tf.float64)  # 转化精度\n",
    "print('after:', pi_1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float64, numpy=3.140625>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类型转换 \n",
    "# 低->高\n",
    "a = tf.constant(np.pi, dtype=tf.float16)\n",
    "tf.cast(a, tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int16, numpy=-13035>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 高 ->低　溢出风险\n",
    "a = tf.constant(123456789, tf.int32)\n",
    "tf.cast(a, tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 0], dtype=int32)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bool -> int\n",
    "a = tf.constant([True, False])\n",
    "tf.cast(a, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True, False,  True])>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int -> bool\n",
    "a = tf.constant([1, -1, 0, 2])  # 0 为False 其他皆为True\n",
    "tf.cast(a, tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 待优化的张量\n",
    "为了区分需要计算梯度信息的张量与不需要计算梯度信息的张量，TensorFlow 增加了一种专门的数据类型来支持梯度信息的记录：tf.Variable. 它在普通张量类型基础上添加了name, trainable等属性来支持计算图的构建.\n",
    "\n",
    "对于不需要的优化的张量，如神经网络的输入𝑿，不需要通过 tf.Variable 封装；相反，对于需要计算梯度并优化的张量，如神经网络层的𝑾和𝒃，需要通过 tf.Variable 包裹以便 TensorFlow 跟踪相关梯度信息。\n",
    "通过 tf.Variable()函数可以将普通张量转换为待优化张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([1, -1, 2, 0])\n",
    "aa = tf.Variable(a)  # 转为Variable类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Variable:0'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.name  # 用于命名计算图中的变量, 内部维护"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.trainable  # 表征当前张量是否需要被优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接创建tf.Variable类型\n",
    "a = tf.Variable([[1, 2], [3, 4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "待优化张量可视为普通张量的特殊类型，普通张量其实也可以通过`GradientTape.watch()`方法临时加入跟踪梯度信息的列表，从而支持自动求导功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 创建张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从数组 列表 创建\n",
    "tf.convert_to_tensor([1, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\narray([[1., 2.],\n       [3., 4.]])>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从numpy数组中创建\n",
    "\n",
    "tf.convert_to_tensor(np.array([[1, 2.], [3, 4.]]))  # numpy 浮点型默认float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全零向量\n",
    "tf.zeros([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[1., 1., 1.],\n        [1., 1., 1.]],\n\n       [[1., 1., 1.],\n        [1., 1., 1.]]], dtype=float32)>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全1向量\n",
    "tf.ones([2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[0, 0],\n       [0, 0]], dtype=int32)>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "tf.zeros_like(a)  # 与某个张量一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[11, 11],\n       [11, 11]], dtype=int32)>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义数值的张量\n",
    "\n",
    "tf.fill([2, 2], 11)  # 2行2列 数值都为11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.33070412, -1.2998165 , -0.9255365 ],\n       [-1.2821348 , -0.93905365, -1.1543428 ]], dtype=float32)>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定概率分布\n",
    "# 正态分布 均值0 标准差1 shape: (2, 3)\n",
    "tf.random.normal([2, 3], mean=0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(6,), dtype=float32, numpy=\narray([0.8141229 , 0.2862265 , 0.4847622 , 0.07810807, 0.58806574,\n       0.13367999], dtype=float32)>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 采样自[0, 1]的均匀分布\n",
    "\n",
    "tf.random.uniform([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[9.619311, 9.898686],\n       [9.006266, 9.290047]], dtype=float32)>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 采样自区间[0,10)，shape 为[2,2]的矩阵\n",
    "tf.random.uniform([2, 2], maxval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[22, 48],\n       [ 3, 13]], dtype=int32)>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 类型为整数\n",
    "tf.random.uniform([2, 2], maxval=100, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建序列\n",
    "tf.range(10)  # np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 2, 4, 6, 8], dtype=int32)>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10, delta=2)  # 指定步长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 张量的典型应用\n",
    "---\n",
    "### 标量\n",
    "标量是简单的数字, 维度数为0, shape为[].典型用途是表示各种`误差值`, `测量指标`, 如精确度(Accuracy, acc), 查准度(Precisoin)和查全率(Recall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.36886328>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tf.random.uniform([4, 10])  # 随机模拟网络输出\n",
    "y = tf.constant([2, 3, 2, 0])  # 随机构造样本真实标签\n",
    "y = tf.one_hot(y, depth=10)  # one-hot编码处理\n",
    "loss = tf.keras.losses.mse(y, out)  # 计算每个样本的MSE\n",
    "loss = tf.reduce_mean(loss)  # 平均MSE\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量\n",
    "\n",
    "向量是一种非常常见的数据载体，如在全连接层和卷积神经网络层中，偏置张量𝒃就使用向量来表示.\n",
    "\n",
    "例: 2个输出节点的网络层, 创建长度为2的偏置向量**b**, 并累加在每个输出节点上\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\narray([[ 1.2154839 , -0.13426471],\n       [-0.1373093 ,  0.83707845],\n       [ 1.8495959 , -0.5648319 ],\n       [-0.5233701 ,  1.2768668 ]], dtype=float32)>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z = wx , 模拟z\n",
    "z = tf.random.normal([4, 2])\n",
    "b = tf.zeros([2])\n",
    "z = z + b  # 累加偏置向量\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一层网络层 张量𝑾和𝒃存储在类的内部，由类自动创建和管理\n",
    "fc = layers.Dense(3)  # 创建一层Wx + b, 输出节点为3个\n",
    "# 通过 build 函数创建 W,b 张量，输入节点为 4\n",
    "fc.build(input_shape=(2, 4))\n",
    "fc.bias  # 查看偏置向量\n",
    "# 类型为Variable(待优化), 向量的值初始化为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵\n",
    "\n",
    "例如全连接层的批量输入张量$X$的形状为$[b, d_{in}]$, b表示输入样本的个数(Batch size), $d_{in}$表示输入特征的长度.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[-0.07032794, -0.07032794, -0.07032794],\n       [-2.0724502 , -2.0724502 , -2.0724502 ]], dtype=float32)>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性变换网络层, 激活函数为空\n",
    "X = tf.random.normal([2, 4])  # 2个样本, 特征长度4\n",
    "w = tf.ones([4, 3])\n",
    "b = tf.zeros([3])\n",
    "\n",
    "out = X@w + b\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X 和 w张量都是矩阵. 。一般地，$\\sigma(X@W + b)$网络层称为`全连接层`，在 TensorFlow 中可以通过`Dense`类直接实现，特别地，当激活函数𝜎为空时，全连接层也称为`线性层`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\narray([[ 0.59064364,  0.9121344 ,  0.6652602 ],\n       [-0.55234253,  0.15975511,  0.26595378],\n       [-0.11956185,  0.77510226, -0.0103429 ],\n       [ 0.8747679 , -0.36924678, -0.4446111 ]], dtype=float32)>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = layers.Dense(3) # 定义全连接层的输出节点为 3\n",
    "fc.build(input_shape=(2,4)) # 定义全连接层的输入节点为 4\n",
    "fc.kernel # 查看权值矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三维张量\n",
    "三维张量的一个典型应用是表示`序列信号`, 格式是:\n",
    "$$X = [b, sequence \\; len, feature \\; len]$$\n",
    "\n",
    "其中𝑏表示序列信号的数量，sequence len 表示序列信号在时间维度上的采样点数或步数，feature len 表示每个点的特征长度.\n",
    "\n",
    "考虑自然语言处理(Natural Language Processing，简称 NLP)中句子的表示，如评价句子的是否为正面情绪的情感分类任务网络。为了能够方便字符串被神经网络处理，一般将单词通过嵌入层(Embedding Layer)编码为固定长度的向量，比如“a”编码为某个长度 3 的向量，那么 2 个等长(单词数量为 5)的句子序列可以表示为 shape 为\\[2,5,3\\]的 3 维张量，其中 2 示句子个数，5 表示单词数量，3 表示单词向量的长度\n",
    "\n",
    "![](./情感分类网络.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb 电影评价数据集\n",
    "(X_train, y_train), *_ = datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n       ...,\n       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2]),\n       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n      dtype=object)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子填充 截断为等长的80个单词的句子\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(25000, 80)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  # (句子个数, 每个句子单词数)每个单词用数字编码的方式表示 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim, output_dim, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "- input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "- output_dim: int >= 0. Dimension of the dense embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建词向量 embedding层\n",
    "embedding = layers.Embedding(10000, 100)  # 每个单词都编码成长度100的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([25000, 80, 100])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = embedding(X_train)\n",
    "out.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四维张量\n",
    "四维张量在卷积神经网络中应用非常广泛，它用于保存`特征图`(Feature maps)数据，格式一般定义为\n",
    "$$[b, h, w, c]$$\n",
    "b表示输入样本的数量, h/w分别表示特征图的高/宽, c表示特征图的通道数.部分深度学习框架也会使用\\[𝑏, 𝑐, ℎ, w\\]格式的特征图张量，例如 PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([4, 30, 30, 16])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 32x32彩色图片, 数量为4\n",
    "X = tf.random.normal([4, 32, 32, 3])\n",
    "# 创建卷积神经网络\n",
    "layer = layers.Conv2D(16, kernel_size=3)\n",
    "out = layer(X)  # 前向计算\n",
    "out.shape  # 输出大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([3, 3, 3, 16])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel.shape  # 卷积核张量 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引与切片\n",
    "\n",
    "与numpy中的索引和切片类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal([4, 32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(32, 3), dtype=float32, numpy=\narray([[ 0.38626292,  0.48458055,  0.9753598 ],\n       [ 0.6152168 , -0.8783054 ,  1.1422373 ],\n       [ 1.5227828 ,  1.3751835 , -0.72983044],\n       [-0.6166592 ,  0.49567157, -0.5693984 ],\n       [-1.3355407 , -0.9289913 , -1.8335633 ],\n       [-1.1912423 ,  0.01054767, -1.4341239 ],\n       [ 0.30111122,  0.15251623, -0.3987652 ],\n       [-0.29142419,  0.611255  ,  0.16753303],\n       [-0.10661095, -0.55688256, -2.7441502 ],\n       [-0.99143326, -0.13468826, -0.0131203 ],\n       [ 0.50020736, -1.2164325 ,  1.9222027 ],\n       [ 0.95121336, -0.94877595,  0.56226194],\n       [-0.21165627, -1.1136016 , -1.2494175 ],\n       [-1.0335566 ,  0.773933  ,  0.05813967],\n       [-0.9157892 , -0.95739913, -0.07989726],\n       [ 0.5002486 ,  1.5189123 ,  1.4757575 ],\n       [-0.60369754,  0.36891344, -0.14120743],\n       [-1.7896798 ,  0.6288549 ,  1.5512767 ],\n       [-2.5050373 ,  0.5114778 ,  0.8379579 ],\n       [ 1.8362858 ,  0.35752925,  0.9732094 ],\n       [ 0.4081019 ,  1.6997422 ,  0.05624337],\n       [-0.6384228 , -0.66695845, -3.4149184 ],\n       [ 0.70528543,  1.0975934 , -0.56032896],\n       [ 0.07649989,  0.19975507, -0.15706956],\n       [-0.9232116 , -0.41649118, -1.2484378 ],\n       [-0.07918338,  1.1392806 ,  1.3567704 ],\n       [-0.5208137 , -1.6868236 ,  0.6707898 ],\n       [ 0.5644851 ,  0.5037961 , -0.20826899],\n       [ 0.12396188,  1.1647896 , -0.11373389],\n       [ 0.43765298, -1.007331  ,  0.34443325],\n       [ 1.201403  , -1.0205604 , -1.046515  ],\n       [ 0.8112357 ,  0.5972995 , -1.6848481 ]], dtype=float32)>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][-1]  # 第一张图片 最后一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=-0.8416042>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, 1, 2, 1]  # 第一张图片, 2行3列B通道 颜色强度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4, 3), dtype=float32, numpy=\narray([[[-0.1756,  1.4343,  0.3572],\n        [-1.8175, -0.8416,  1.6837],\n        [-2.1726,  0.3714,  0.4909],\n        [ 1.4304,  1.4152, -0.6769]],\n\n       [[ 0.1144,  1.5118,  0.1508],\n        [ 1.3896,  1.543 , -1.93  ],\n        [-0.8983, -0.5082,  0.7367],\n        [ 0.7095, -1.0134, -0.1436]]], dtype=float32)>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start: end: step\n",
    "X[0, 1:3, :8:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 32, 32), dtype=float32, numpy=\narray([[[-1.0976, -1.6566,  0.6943, ...,  0.7708, -1.412 ,  0.3265],\n        [ 1.4343, -0.1896, -0.8416, ...,  0.8973,  1.2024,  1.2788],\n        [ 1.5118,  0.0038,  1.543 , ..., -1.2033, -0.9243,  1.0292],\n        ...,\n        [-0.7541, -2.0584,  2.4961, ...,  0.7793, -0.9003, -0.9882],\n        [-0.4767, -0.9161, -0.8443, ..., -0.7434,  0.9984,  0.0643],\n        [ 0.4846, -0.8783,  1.3752, ..., -1.0073, -1.0206,  0.5973]],\n\n       [[ 0.0845, -1.3828, -0.1253, ..., -0.6026, -0.9142, -1.0079],\n        [ 0.1749,  0.6091,  0.7482, ..., -0.0227, -0.3177, -0.3681],\n        [ 1.2783, -0.3136,  1.6343, ...,  0.1607, -1.0614, -0.1347],\n        ...,\n        [-0.3303, -0.685 ,  1.6297, ...,  1.6337,  1.3217,  1.5199],\n        [-0.8877, -0.0002, -1.9721, ..., -1.4153,  0.046 ,  1.7619],\n        [ 0.9173, -2.0606,  0.6922, ..., -0.1284,  1.2786, -0.2942]]],\n      dtype=float32)>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用⋯符号表示取多个维度上所有的数据，其中维度的数量需根据规则自动推断\n",
    "X[0:2, ..., 1]  # 高和宽全部采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 32, 32, 3), dtype=float32, numpy=\narray([[[[ 0.4494,  0.8135, -0.7077],\n         [ 0.6427,  0.8531,  1.9176],\n         [ 0.59  ,  0.3249, -0.7983],\n         ...,\n         [ 0.632 ,  0.2936,  1.186 ],\n         [-1.3732,  0.4363,  0.8158],\n         [ 0.6319,  1.6061, -0.61  ]],\n\n        [[-1.0711, -1.3315, -0.4079],\n         [ 0.2425, -0.4494, -0.2964],\n         [-0.0149, -0.9606, -0.3528],\n         ...,\n         [ 1.0203, -0.4396,  0.9588],\n         [ 1.3297,  0.67  ,  2.0149],\n         [-0.2295, -0.4115, -0.2137]],\n\n        [[ 0.4033, -0.7814, -0.2517],\n         [ 0.544 , -1.4219, -0.7449],\n         [-0.2854,  0.726 , -1.0103],\n         ...,\n         [ 1.8681,  0.4935, -0.3727],\n         [-0.7118,  0.0807,  3.0895],\n         [-0.103 , -0.183 , -0.2882]],\n\n        ...,\n\n        [[ 2.0299, -0.0994, -1.8217],\n         [-0.7688, -0.5339, -1.3172],\n         [ 1.1124, -0.8966, -0.3222],\n         ...,\n         [-1.7915, -0.102 ,  1.0022],\n         [-0.4167, -1.3346,  0.2084],\n         [ 1.9049, -1.3921,  1.5586]],\n\n        [[-0.2847,  1.2162, -1.0917],\n         [ 0.9528,  0.4289, -1.0166],\n         [-0.2161, -0.6173,  0.7961],\n         ...,\n         [-0.5066, -0.5906, -0.2719],\n         [-1.1225, -0.9709, -0.5916],\n         [-2.1387,  0.0508,  0.3846]],\n\n        [[-0.7528,  0.0087, -1.5872],\n         [ 0.4964, -1.3278,  1.4105],\n         [-1.3062,  0.7542, -0.371 ],\n         ...,\n         [ 0.2183,  1.22  ,  0.3608],\n         [-0.3679,  0.9869,  0.1917],\n         [ 0.0699, -2.1463, -1.4648]]],\n\n\n       [[[ 0.2297,  0.8235,  0.7952],\n         [-1.8491,  0.7634, -0.3802],\n         [-0.8339, -0.1602,  0.3158],\n         ...,\n         [-0.7405, -0.5101,  0.3173],\n         [ 1.4968, -1.8411, -0.1381],\n         [-0.2787,  1.1644, -0.3478]],\n\n        [[ 0.5699, -0.9441, -0.1736],\n         [-1.1933,  0.9403, -0.6194],\n         [ 1.1149,  0.4244,  0.3633],\n         ...,\n         [-0.65  , -0.4937, -0.4437],\n         [-0.7006, -1.3006, -0.5345],\n         [-1.5592,  0.2351, -0.5692]],\n\n        [[-0.7727, -0.5805,  1.6148],\n         [ 0.0148, -1.9884,  0.202 ],\n         [ 0.0679,  0.7925,  0.442 ],\n         ...,\n         [ 0.3506, -0.6404,  2.1088],\n         [-1.9379, -0.4549,  0.8977],\n         [ 2.4779,  1.0048, -0.8226]],\n\n        ...,\n\n        [[-0.6342,  0.1579, -0.2746],\n         [ 0.9008, -1.5341,  0.3385],\n         [-1.5438,  0.9765, -0.6768],\n         ...,\n         [-0.1284,  1.3095,  1.2761],\n         [ 0.3229,  0.2944,  0.7502],\n         [-1.4142, -1.4365, -0.7149]],\n\n        [[ 0.2914,  0.7033, -0.3469],\n         [ 0.8029, -0.7146,  0.0434],\n         [ 0.1108,  0.133 ,  0.991 ],\n         ...,\n         [-1.1958, -1.1235, -0.7268],\n         [ 0.455 ,  0.018 ,  1.5978],\n         [-1.1311, -0.3651,  0.5586]],\n\n        [[-0.8445,  0.4575, -0.0171],\n         [ 2.1965,  1.4456,  2.0179],\n         [-1.5148,  1.2228,  0.7913],\n         ...,\n         [ 1.577 ,  0.2711, -1.219 ],\n         [-0.3355,  1.6145,  1.1187],\n         [-0.1234,  0.917 ,  1.1012]]]], dtype=float32)>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-2:, ...]  # 最后2张图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 32, 32, 2), dtype=float32, numpy=\narray([[[[-0.9676, -1.0976],\n         [ 0.3849, -1.6566],\n         [-0.9155,  0.6943],\n         ...,\n         [ 0.8401,  0.7708],\n         [ 0.1111, -1.412 ],\n         [ 0.1091,  0.3265]],\n\n        [[-0.1756,  1.4343],\n         [-1.7394, -0.1896],\n         [-1.8175, -0.8416],\n         ...,\n         [ 0.1745,  0.8973],\n         [ 0.0008,  1.2024],\n         [-0.5203,  1.2788]],\n\n        [[ 0.1144,  1.5118],\n         [ 1.6678,  0.0038],\n         [ 1.3896,  1.543 ],\n         ...,\n         [ 0.4403, -1.2033],\n         [-0.0553, -0.9243],\n         [-0.9238,  1.0292]],\n\n        ...,\n\n        [[ 1.142 , -0.7541],\n         [-0.5814, -2.0584],\n         [ 1.2934,  2.4961],\n         ...,\n         [ 0.0072,  0.7793],\n         [-0.098 , -0.9003],\n         [-1.2378, -0.9882]],\n\n        [[-1.3678, -0.4767],\n         [-0.4042, -0.9161],\n         [ 1.7686, -0.8443],\n         ...,\n         [-0.6023, -0.7434],\n         [ 1.4283,  0.9984],\n         [-0.7412,  0.0643]],\n\n        [[ 0.3863,  0.4846],\n         [ 0.6152, -0.8783],\n         [ 1.5228,  1.3752],\n         ...,\n         [ 0.4377, -1.0073],\n         [ 1.2014, -1.0206],\n         [ 0.8112,  0.5973]]],\n\n\n       [[[ 0.2042,  0.0845],\n         [-0.5337, -1.3828],\n         [ 0.4417, -0.1253],\n         ...,\n         [-0.3678, -0.6026],\n         [ 0.6046, -0.9142],\n         [-0.0988, -1.0079]],\n\n        [[-1.2971,  0.1749],\n         [ 0.1526,  0.6091],\n         [-0.6774,  0.7482],\n         ...,\n         [-0.5669, -0.0227],\n         [ 0.7106, -0.3177],\n         [-0.1887, -0.3681]],\n\n        [[ 0.766 ,  1.2783],\n         [-0.8204, -0.3136],\n         [-1.2575,  1.6343],\n         ...,\n         [-1.1165,  0.1607],\n         [-1.63  , -1.0614],\n         [-0.6018, -0.1347]],\n\n        ...,\n\n        [[ 0.5817, -0.3303],\n         [ 2.1459, -0.685 ],\n         [-1.3933,  1.6297],\n         ...,\n         [ 2.0846,  1.6337],\n         [ 2.4965,  1.3217],\n         [ 0.0968,  1.5199]],\n\n        [[-1.1478, -0.8877],\n         [-0.0713, -0.0002],\n         [ 0.225 , -1.9721],\n         ...,\n         [-0.6599, -1.4153],\n         [ 0.6481,  0.046 ],\n         [ 1.0635,  1.7619]],\n\n        [[ 0.0986,  0.9173],\n         [ 0.4289, -2.0606],\n         [ 0.6747,  0.6922],\n         ...,\n         [ 1.2905, -0.1284],\n         [ 0.0327,  1.2786],\n         [-1.6098, -0.2942]]],\n\n\n       [[[ 0.4494,  0.8135],\n         [ 0.6427,  0.8531],\n         [ 0.59  ,  0.3249],\n         ...,\n         [ 0.632 ,  0.2936],\n         [-1.3732,  0.4363],\n         [ 0.6319,  1.6061]],\n\n        [[-1.0711, -1.3315],\n         [ 0.2425, -0.4494],\n         [-0.0149, -0.9606],\n         ...,\n         [ 1.0203, -0.4396],\n         [ 1.3297,  0.67  ],\n         [-0.2295, -0.4115]],\n\n        [[ 0.4033, -0.7814],\n         [ 0.544 , -1.4219],\n         [-0.2854,  0.726 ],\n         ...,\n         [ 1.8681,  0.4935],\n         [-0.7118,  0.0807],\n         [-0.103 , -0.183 ]],\n\n        ...,\n\n        [[ 2.0299, -0.0994],\n         [-0.7688, -0.5339],\n         [ 1.1124, -0.8966],\n         ...,\n         [-1.7915, -0.102 ],\n         [-0.4167, -1.3346],\n         [ 1.9049, -1.3921]],\n\n        [[-0.2847,  1.2162],\n         [ 0.9528,  0.4289],\n         [-0.2161, -0.6173],\n         ...,\n         [-0.5066, -0.5906],\n         [-1.1225, -0.9709],\n         [-2.1387,  0.0508]],\n\n        [[-0.7528,  0.0087],\n         [ 0.4964, -1.3278],\n         [-1.3062,  0.7542],\n         ...,\n         [ 0.2183,  1.22  ],\n         [-0.3679,  0.9869],\n         [ 0.0699, -2.1463]]],\n\n\n       [[[ 0.2297,  0.8235],\n         [-1.8491,  0.7634],\n         [-0.8339, -0.1602],\n         ...,\n         [-0.7405, -0.5101],\n         [ 1.4968, -1.8411],\n         [-0.2787,  1.1644]],\n\n        [[ 0.5699, -0.9441],\n         [-1.1933,  0.9403],\n         [ 1.1149,  0.4244],\n         ...,\n         [-0.65  , -0.4937],\n         [-0.7006, -1.3006],\n         [-1.5592,  0.2351]],\n\n        [[-0.7727, -0.5805],\n         [ 0.0148, -1.9884],\n         [ 0.0679,  0.7925],\n         ...,\n         [ 0.3506, -0.6404],\n         [-1.9379, -0.4549],\n         [ 2.4779,  1.0048]],\n\n        ...,\n\n        [[-0.6342,  0.1579],\n         [ 0.9008, -1.5341],\n         [-1.5438,  0.9765],\n         ...,\n         [-0.1284,  1.3095],\n         [ 0.3229,  0.2944],\n         [-1.4142, -1.4365]],\n\n        [[ 0.2914,  0.7033],\n         [ 0.8029, -0.7146],\n         [ 0.1108,  0.133 ],\n         ...,\n         [-1.1958, -1.1235],\n         [ 0.455 ,  0.018 ],\n         [-1.1311, -0.3651]],\n\n        [[-0.8445,  0.4575],\n         [ 2.1965,  1.4456],\n         [-1.5148,  1.2228],\n         ...,\n         [ 1.577 ,  0.2711],\n         [-0.3355,  1.6145],\n         [-0.1234,  0.917 ]]]], dtype=float32)>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[..., :2]  # 所有图片前2个通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 维度变换\n",
    "---\n",
    "张量的存储(Storage)和视图(View):\n",
    "- 视图就是我们理解张量的方式\n",
    "- 存储体现在张量在内存上保存为一段连续的内存区域\n",
    "- 同一个存储, 从不同角度观察数据可以产生不同的视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4, 4, 3), dtype=int32, numpy=\narray([[[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8],\n         [ 9, 10, 11]],\n\n        [[12, 13, 14],\n         [15, 16, 17],\n         [18, 19, 20],\n         [21, 22, 23]],\n\n        [[24, 25, 26],\n         [27, 28, 29],\n         [30, 31, 32],\n         [33, 34, 35]],\n\n        [[36, 37, 38],\n         [39, 40, 41],\n         [42, 43, 44],\n         [45, 46, 47]]],\n\n\n       [[[48, 49, 50],\n         [51, 52, 53],\n         [54, 55, 56],\n         [57, 58, 59]],\n\n        [[60, 61, 62],\n         [63, 64, 65],\n         [66, 67, 68],\n         [69, 70, 71]],\n\n        [[72, 73, 74],\n         [75, 76, 77],\n         [78, 79, 80],\n         [81, 82, 83]],\n\n        [[84, 85, 86],\n         [87, 88, 89],\n         [90, 91, 92],\n         [93, 94, 95]]]], dtype=int32)>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape 改变视图  numpy reshape\n",
    "\n",
    "X = tf.range(96)\n",
    "X = tf.reshape(X, [2, 4, 4, 3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在存储数据时，内存并不支持这个维度层级概念，只能以平铺方式按序写入内存. 在优先写入小维度的设定下，上述张量的内存布局为\n",
    "\n",
    "0, 1, 2, 3, ..., 95\n",
    "\n",
    "在通过 reshape 改变视图时，必须始终记住张量的存储顺序，新视图的维度顺序不能与存储顺序相悖，否则需要通过交换维度操作将存储顺序同步过来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(4, TensorShape([2, 4, 4, 3]))"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.ndim, X.shape  # 获取维度和形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 24), dtype=int32, numpy=\narray([[ 0,  1,  2, ..., 21, 22, 23],\n       [24, 25, 26, ..., 45, 46, 47],\n       [48, 49, 50, ..., 69, 70, 71],\n       [72, 73, 74, ..., 93, 94, 95]], dtype=int32)>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(X, [4, -1])  # -1当前轴上长度需要根据张量总元素不变的法则自动推导，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(28, 28), dtype=int32, numpy=\narray([[227,  14,  78, ...,  22,  49, 208],\n       [ 60,   8, 147, ..., 105,   7,  15],\n       [ 63, 191, 251, ..., 147, 206, 139],\n       ...,\n       [193, 186, 231, ..., 251, 212, 183],\n       [ 84,  71, 155, ..., 201, 207, 227],\n       [229, 205, 187, ...,  59,  71, 136]], dtype=int32)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 增加维度  增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念\n",
    "# 28x28的灰度图片\n",
    "x = tf.random.uniform([28, 28], maxval=255, dtype=tf.int32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(28, 28, 1), dtype=int32, numpy=\narray([[[227],\n        [ 14],\n        [ 78],\n        ...,\n        [ 22],\n        [ 49],\n        [208]],\n\n       [[ 60],\n        [  8],\n        [147],\n        ...,\n        [105],\n        [  7],\n        [ 15]],\n\n       [[ 63],\n        [191],\n        [251],\n        ...,\n        [147],\n        [206],\n        [139]],\n\n       ...,\n\n       [[193],\n        [186],\n        [231],\n        ...,\n        [251],\n        [212],\n        [183]],\n\n       [[ 84],\n        [ 71],\n        [155],\n        ...,\n        [201],\n        [207],\n        [227]],\n\n       [[229],\n        [205],\n        [187],\n        ...,\n        [ 59],\n        [ 71],\n        [136]]], dtype=int32)>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.expand_dims(x, axis=2)  # axis=2 表示宽维度后面的一个维度\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 28, 28, 1), dtype=int32, numpy=\narray([[[[227],\n         [ 14],\n         [ 78],\n         ...,\n         [ 22],\n         [ 49],\n         [208]],\n\n        [[ 60],\n         [  8],\n         [147],\n         ...,\n         [105],\n         [  7],\n         [ 15]],\n\n        [[ 63],\n         [191],\n         [251],\n         ...,\n         [147],\n         [206],\n         [139]],\n\n        ...,\n\n        [[193],\n         [186],\n         [231],\n         ...,\n         [251],\n         [212],\n         [183]],\n\n        [[ 84],\n         [ 71],\n         [155],\n         ...,\n         [201],\n         [207],\n         [227]],\n\n        [[229],\n         [205],\n         [187],\n         ...,\n         [ 59],\n         [ 71],\n         [136]]]], dtype=int32)>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.expand_dims(x, axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis 为正时，表示在当前维度之前插入一个新维度；为负时，表示当前维度之后插入一个新的维度\n",
    "\n",
    "![](./增加维度.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(28, 28, 1), dtype=int32, numpy=\narray([[[227],\n        [ 14],\n        [ 78],\n        ...,\n        [ 22],\n        [ 49],\n        [208]],\n\n       [[ 60],\n        [  8],\n        [147],\n        ...,\n        [105],\n        [  7],\n        [ 15]],\n\n       [[ 63],\n        [191],\n        [251],\n        ...,\n        [147],\n        [206],\n        [139]],\n\n       ...,\n\n       [[193],\n        [186],\n        [231],\n        ...,\n        [251],\n        [212],\n        [183]],\n\n       [[ 84],\n        [ 71],\n        [155],\n        ...,\n        [201],\n        [207],\n        [227]],\n\n       [[229],\n        [205],\n        [187],\n        ...,\n        [ 59],\n        [ 71],\n        [136]]], dtype=int32)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除维度只能删除长度为 1 的维度，\n",
    "x = tf.squeeze(x, axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(28, 28), dtype=int32, numpy=\narray([[227,  14,  78, ...,  22,  49, 208],\n       [ 60,   8, 147, ..., 105,   7,  15],\n       [ 63, 191, 251, ..., 147, 206, 139],\n       ...,\n       [193, 186, 231, ..., 251, 212, 183],\n       [ 84,  71, 155, ..., 201, 207, 227],\n       [229, 205, 187, ...,  59,  71, 136]], dtype=int32)>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.squeeze(x, axis=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(8, 8), dtype=float32, numpy=\narray([[0.0162, 0.8471, 0.3252, ..., 0.0686, 0.8832, 0.1266],\n       [0.1285, 0.9692, 0.2529, ..., 0.5457, 0.2686, 0.8352],\n       [0.7545, 0.0208, 0.6539, ..., 0.5697, 0.6033, 0.9746],\n       ...,\n       [0.2471, 0.9987, 0.9648, ..., 0.4483, 0.3133, 0.7795],\n       [0.0709, 0.8017, 0.9218, ..., 0.0332, 0.8879, 0.0278],\n       [0.3776, 0.4904, 0.1002, ..., 0.3115, 0.7275, 0.3854]],\n      dtype=float32)>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不指定维度参数 axis，即 tf.squeeze(x)，那么它会默认删除所有长度为 1 的维度\n",
    "x = tf.random.uniform([1, 8, 8, 1])\n",
    "tf.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=\narray([[[[ 1.6445, -0.2121, -1.0621, ...,  1.121 ,  0.9723,  1.4606],\n         [ 0.868 , -0.4396, -1.2934, ...,  1.777 ,  0.8638,  2.306 ],\n         [ 0.003 ,  0.0575,  1.9425, ...,  2.4887, -0.7341,  0.8839],\n         ...,\n         [-0.0633,  0.8911,  0.3263, ...,  0.2943, -0.3763, -2.0715],\n         [-0.1403, -1.203 , -0.8123, ..., -0.4894, -1.0556, -1.6299],\n         [-1.7972, -1.2354, -0.1114, ...,  1.2531, -1.6232,  0.8396]],\n\n        [[-0.9919,  0.1023,  1.4613, ..., -1.6071,  1.4713, -2.1798],\n         [ 0.3426,  0.6119,  0.5327, ..., -0.4732, -1.1353,  0.1271],\n         [-0.9392,  1.004 ,  0.2645, ...,  0.8801, -0.2601,  0.4133],\n         ...,\n         [-1.2034, -0.7826, -0.9239, ..., -0.2851, -1.1869,  0.2215],\n         [-2.3126, -0.8373,  1.0206, ...,  0.8212, -0.3453, -2.0803],\n         [ 0.5528, -0.6221, -1.6782, ..., -2.0969,  0.1865, -0.3055]],\n\n        [[-0.6711,  0.3085, -0.5608, ..., -1.8704,  0.3606, -0.8953],\n         [ 1.6784,  0.5808, -0.1662, ...,  0.4201, -0.8203,  0.0734],\n         [-0.7683, -0.3484, -0.0868, ...,  1.4526, -0.5221,  1.3409],\n         ...,\n         [ 0.7294, -0.5022, -2.2663, ...,  1.2613,  0.7938, -1.529 ],\n         [-0.4625, -0.271 , -1.3552, ..., -0.0768,  0.1188, -1.0853],\n         [ 1.0534, -0.7027, -0.1518, ..., -0.2991, -1.4643,  1.5654]]],\n\n\n       [[[-0.1636,  0.1352,  0.9479, ...,  0.5299, -0.1556, -2.2035],\n         [-0.7369, -0.3928, -0.4362, ...,  0.4891, -0.9871,  2.0788],\n         [-1.2321,  2.3606,  0.9821, ...,  0.01  , -0.4505,  0.8018],\n         ...,\n         [-0.293 ,  0.0775,  0.088 , ..., -1.9124,  2.0119,  0.5721],\n         [-3.4679, -1.4528,  1.5238, ..., -0.8345,  1.5752,  0.6385],\n         [-0.6081,  1.2572,  0.2895, ...,  0.8539, -1.6428, -0.0225]],\n\n        [[ 0.1168, -1.9001,  0.4037, ...,  0.159 , -0.2319,  0.104 ],\n         [-0.0972, -0.1505,  1.9076, ...,  0.2949, -0.1547, -0.451 ],\n         [ 1.1614,  0.3322,  0.2948, ..., -1.5235,  0.4544,  1.9234],\n         ...,\n         [ 2.8428,  0.0436, -0.1547, ..., -0.2101, -0.197 , -0.2533],\n         [-0.7709,  0.8262,  0.9583, ..., -0.6464, -0.2529, -0.3246],\n         [-1.0657,  0.2993,  0.1073, ...,  0.3205, -0.3429,  1.072 ]],\n\n        [[ 0.8165, -1.0459,  0.7252, ...,  0.7422, -0.8703,  0.1185],\n         [-1.0295,  1.0324,  1.6434, ...,  0.4995,  0.2517, -0.1517],\n         [-0.9437, -0.046 ,  0.1351, ..., -0.6582,  0.4488, -1.861 ],\n         ...,\n         [ 2.3939, -1.316 , -0.1081, ...,  0.8991, -0.352 ,  1.1353],\n         [-1.1717,  2.3534, -1.5631, ...,  1.441 , -2.3683,  0.9379],\n         [-0.8145, -0.2935,  0.4689, ...,  0.6929,  0.8689, -0.0257]]]],\n      dtype=float32)>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交换维度 transpose\n",
    "x = tf.random.normal([2, 32, 32, 3])  # b, h, w, c -> b, c, h, w\n",
    "tf.transpose(x, perm=[0, 3, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 tf.transpose 完成维度交换后，张量的存储顺序已经改变，视图也随之改变，后续的所有操作必须基于新的存续顺序和视图进行。相对于改变视图操作，维度交换操作的计算代价更高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]], dtype=int32)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制数据\n",
    "b = tf.constant([1, 2, 3])  # (3, )\n",
    "b = tf.expand_dims(b, axis=0)  # 插入新维度 -> (1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [1, 2, 3]], dtype=int32)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.tile(b, multiples=[2, 1])  # 0轴复制一次 1轴不复制\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[0, 1],\n       [2, 3]], dtype=int32)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动实现插入维度\n",
    "\n",
    "x = tf.range(4)\n",
    "x = tf.reshape(x, [2, 2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[0, 1],\n       [2, 3],\n       [0, 1],\n       [2, 3]], dtype=int32)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(x, multiples=[2, 1])  # 行复制一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[0, 1, 0, 1],\n       [2, 3, 2, 3]], dtype=int32)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(x, multiples=[1, 2])  # 列复制一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\narray([[0, 1, 0, 1],\n       [2, 3, 2, 3],\n       [0, 1, 0, 1],\n       [2, 3, 2, 3]], dtype=int32)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(x, [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting 广播(自动扩展)\n",
    "\n",
    "类似numpy 中的广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.6466,  0.5488,  3.4398],\n       [ 1.0131,  2.4012, -0.0811]], dtype=float32)>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([2, 4])\n",
    "w = tf.random.normal([4, 3])\n",
    "b = tf.random.normal([3])\n",
    "\n",
    "y = x @ w + b  # broadcasting\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.6466,  0.5488,  3.4398],\n       [ 1.0131,  2.4012, -0.0811]], dtype=float32)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x @ w + tf.broadcast_to(b, [2,3])# 手动广播\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。在验证普适性之前，需要先将张量 shape 靠右对齐，然后进行普适性判断：对于长度为 1 的维度，默认这个数据普遍适合于当前维度的其他位置；对于不存在的维度，则在增加新维度后默认当前数据也是普适于新维度的，从而可以扩展为更多维度数、任意长度的张量形状\n",
    "\n",
    "广播成功的例子:\n",
    "\n",
    "![](./广播成功.png)\n",
    "\n",
    "广播失败的例子:\n",
    "\n",
    "![](./广播失败.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数学运算\n",
    "--- \n",
    "加、减、乘、除是最基本的数学运算，分别通过 tf.add, tf.subtract, tf.multiply, tf.divide函数实现，TensorFlow 已经重载了+、 − 、 ∗ 、/运算符. 整除和取余数: //, %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 1, 1, 2], dtype=int32)>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.range(5)\n",
    "b = tf.constant(2)\n",
    "a // b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 0, 1, 0], dtype=int32)>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1,  8, 27], dtype=int32)>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乘方 power\n",
    "x = tf.range(1, 4)\n",
    "tf.pow(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 4, 9], dtype=int32)>,\n <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 4, 9], dtype=int32)>)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x ** 2 , tf.square(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>,\n <tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开方\n",
    "x = tf.constant([1., 4., 9.])\n",
    "x ** (0.5), tf.sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2., 4., 8.], dtype=float32)>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指数 tf.pow , ** \n",
    "x = tf.constant([1., 2, 3])\n",
    "2 ** x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.7182817>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自然指数 e\n",
    "tf.exp(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自然对数\n",
    "x = tf.exp(3.)\n",
    "tf.math.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 换底公式log10(x)\n",
    "x = tf.constant([1., 2.])\n",
    "x = 10 ** x\n",
    "tf.math.log(x) / tf.math.log(10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  矩阵乘法\n",
    "TensorFlow 中的矩阵相乘可以使用批量方式，也就是张量𝑨和𝑩的维度数可以大于 2。当张量𝑨和𝑩维度数大于 2 时，TensorFlow 会选择𝑨和𝑩的最后两个维度进行矩阵相乘，前面所有的维度都视作Batch 维度。\n",
    "\n",
    "根据矩阵相乘的定义，𝑨和𝑩能够矩阵相乘的条件是，𝑨的倒数第一个维度长度(列)和𝑩的倒数第二个维度长度(行)必须相等。比如张量 a shape:\\[4,3,28,32\\]可以与张量 bshape:\\[4,3,32,2\\]进行矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 3, 28, 2), dtype=float32, numpy=\narray([[[[ -1.0934,   2.2801],\n         [  7.191 ,  -4.5564],\n         [  5.615 ,  -0.7883],\n         ...,\n         [ -5.4651, -18.2195],\n         [  1.4142,   1.4663],\n         [ -1.1554,   1.713 ]],\n\n        [[  2.5084,  -4.3073],\n         [  4.7264,   9.0906],\n         [  0.2972,   9.9935],\n         ...,\n         [ 13.4975,  -7.6285],\n         [  1.6347,   3.6777],\n         [  3.311 ,  -9.0082]],\n\n        [[  3.1924,   6.1756],\n         [ -3.6798,  12.1598],\n         [  3.6036,  -4.7887],\n         ...,\n         [  2.2526,   2.4374],\n         [ -0.6003,   8.3296],\n         [ -8.5861,   7.0863]]],\n\n\n       [[[  4.4293,   1.147 ],\n         [  3.6171,   5.3617],\n         [ 10.7696,   0.8449],\n         ...,\n         [  6.4209,   6.9002],\n         [  6.4495,   6.3447],\n         [ -1.5835,   4.6992]],\n\n        [[  3.0381,   3.4137],\n         [ -4.5553,   3.6361],\n         [ -3.4704,  -3.2785],\n         ...,\n         [  1.2236,  -4.6076],\n         [  8.0657,   2.2008],\n         [  6.7475,  -9.3116]],\n\n        [[ -3.4222,   2.3745],\n         [  0.5884,  11.817 ],\n         [  6.5467,  -1.0322],\n         ...,\n         [  3.2023,  -1.6419],\n         [  4.5564,  -7.8335],\n         [  9.933 ,  -4.1094]]],\n\n\n       [[[  3.0546,   3.6826],\n         [ -2.0884,   1.7004],\n         [ -2.6613,  -0.751 ],\n         ...,\n         [ -5.1646,   7.612 ],\n         [ -1.0209,   7.5943],\n         [ -4.252 ,  -7.7034]],\n\n        [[  2.1462,  -3.979 ],\n         [  2.6541,   6.3256],\n         [ 10.0295,   4.7876],\n         ...,\n         [  1.1298,  -0.4751],\n         [ 18.587 ,   5.1403],\n         [  3.013 ,   7.9275]],\n\n        [[ -5.3654,   0.9556],\n         [ -1.0362,  -1.6778],\n         [  2.2467,  -8.0838],\n         ...,\n         [ -5.2743,   1.6186],\n         [ -8.1768,   5.2064],\n         [  0.4849,  -3.4544]]],\n\n\n       [[[ -2.4122,  -0.263 ],\n         [ -3.0638,   6.6072],\n         [  5.9438,   8.3495],\n         ...,\n         [  5.1732,  -3.9244],\n         [ -9.0572,   1.4014],\n         [  0.7876,   1.5975]],\n\n        [[-10.2856,  -0.5561],\n         [ -1.6112,  -0.4504],\n         [ -5.0974,  -7.1571],\n         ...,\n         [  7.2643,  -4.47  ],\n         [ -0.4408,   0.3155],\n         [  0.8482,  -0.3668]],\n\n        [[  7.4087,  -0.1004],\n         [  4.4797,   2.4929],\n         [ -2.346 ,  -7.4076],\n         ...,\n         [  7.5688,  -0.5353],\n         [ -3.8206,   5.8859],\n         [ -2.1591,  -1.9346]]]], dtype=float32)>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([4, 3, 28, 32])\n",
    "b = tf.random.normal([4, 3, 32, 2])\n",
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 3, 16), dtype=float32, numpy=\narray([[[  2.9606,   2.2013,   0.0198, ...,   7.1217,  -7.1193,\n           2.0865],\n        [  5.6084,  10.2564,  -6.916 , ...,   0.1774,   2.428 ,\n          -1.0326],\n        [  0.3027,  -7.7884,   4.1111, ...,   4.1389,  -4.255 ,\n           2.0816]],\n\n       [[ -0.9867,  -2.1044,  -2.3265, ...,  -8.6665,   9.8111,\n          -0.9919],\n        [ -8.6593,   6.3916,   3.2853, ...,  11.795 ,  -7.6118,\n          -8.131 ],\n        [  4.2047,  -0.7984,   8.431 , ...,  -0.3777,  -3.4946,\n           2.0063]],\n\n       [[  9.9131,   2.8745,  -6.1947, ...,   9.8241,   6.1531,\n          -0.6255],\n        [  6.9507, -12.186 ,   6.8275, ..., -16.4829,  -2.0571,\n           1.3972],\n        [  1.1048,   2.4879,  -5.5001, ...,   6.224 ,   0.3461,\n          -7.3151]],\n\n       [[ -0.8993,   1.2174,  -7.5666, ...,   3.2458, -10.0192,\n           3.5283],\n        [  0.6466,  -4.6528,   2.4219, ...,   0.9483,  -0.5631,\n          -9.5658],\n        [ -0.5276,   4.6382,  -2.0319, ...,  -2.8188,   5.2267,\n           6.6509]]], dtype=float32)>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法的自动广播\n",
    "\n",
    "a = tf.random.normal([4, 3, 32])\n",
    "b = tf.random.normal([32, 16])\n",
    "tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播实战\n",
    "\n",
    "创建三层神经网络:\n",
    "$$ out = ReLU\\{ReLU\\{ReLU[X@W_1+b_1]@W_2 + b_2\\}@W_3 + b_3\\}$$\n",
    "输入节点784(28*28), 第一层输出256, 第二层输出128, 第三层输出10(10类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), _ = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(28, 28)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(X, dtype=tf.float32) / 255.\n",
    "y = tf.convert_to_tensor(y, dtype=tf.int32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<BatchDataset shapes: ((None, 28, 28), (None,)), types: (tf.float32, tf.int32)>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 每步梯度下降取处理128张图片\n",
    "train_db = tf.data.Dataset.from_tensor_slices((X,y)).batch(128)\n",
    "train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每层w使用截断正态分布 (u-2sigma, u+2sigma)\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "# 偏置b初始化为0\n",
    "b1 = tf.Variable(tf.zeros([256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch:0, step:0, loss:0.4920053482055664\nepoch:0, step:200, loss:0.17404800653457642\nepoch:0, step:400, loss:0.15899842977523804\nepoch:1, step:0, loss:0.15582861006259918\nepoch:1, step:200, loss:0.14573532342910767\nepoch:1, step:400, loss:0.1365160197019577\nepoch:2, step:0, loss:0.1346619874238968\nepoch:2, step:200, loss:0.1287241131067276\nepoch:2, step:400, loss:0.12174016237258911\nepoch:3, step:0, loss:0.12059202045202255\nepoch:3, step:200, loss:0.11689981073141098\nepoch:3, step:400, loss:0.11146266758441925\nepoch:4, step:0, loss:0.11056281626224518\nepoch:4, step:200, loss:0.10808692872524261\nepoch:4, step:400, loss:0.10395302623510361\nepoch:5, step:0, loss:0.10303044319152832\nepoch:5, step:200, loss:0.10118375718593597\nepoch:5, step:400, loss:0.09826679527759552\nepoch:6, step:0, loss:0.0971221774816513\nepoch:6, step:200, loss:0.0956956148147583\nepoch:6, step:400, loss:0.09382468461990356\nepoch:7, step:0, loss:0.09238891303539276\nepoch:7, step:200, loss:0.09129025042057037\nepoch:7, step:400, loss:0.0902177095413208\nepoch:8, step:0, loss:0.08849339187145233\nepoch:8, step:200, loss:0.08764257282018661\nepoch:8, step:400, loss:0.0872049480676651\nepoch:9, step:0, loss:0.08523491024971008\nepoch:9, step:200, loss:0.0845458060503006\nepoch:9, step:400, loss:0.08463634550571442\nepoch:10, step:0, loss:0.08241938799619675\nepoch:10, step:200, loss:0.08187153190374374\nepoch:10, step:400, loss:0.08236783742904663\nepoch:11, step:0, loss:0.07997403293848038\nepoch:11, step:200, loss:0.07951856404542923\nepoch:11, step:400, loss:0.08034754544496536\nepoch:12, step:0, loss:0.07782571017742157\nepoch:12, step:200, loss:0.07743604481220245\nepoch:12, step:400, loss:0.07854615151882172\nepoch:13, step:0, loss:0.07591051608324051\nepoch:13, step:200, loss:0.07556179910898209\nepoch:13, step:400, loss:0.07692418247461319\nepoch:14, step:0, loss:0.07419765740633011\nepoch:14, step:200, loss:0.07387278974056244\nepoch:14, step:400, loss:0.07546628266572952\nepoch:15, step:0, loss:0.07263574749231339\nepoch:15, step:200, loss:0.0723416656255722\nepoch:15, step:400, loss:0.07414871454238892\nepoch:16, step:0, loss:0.0712033361196518\nepoch:16, step:200, loss:0.07092635333538055\nepoch:16, step:400, loss:0.07294107973575592\nepoch:17, step:0, loss:0.06987552344799042\nepoch:17, step:200, loss:0.06961719691753387\nepoch:17, step:400, loss:0.07182615995407104\nepoch:18, step:0, loss:0.0686420276761055\nepoch:18, step:200, loss:0.06841439753770828\nepoch:18, step:400, loss:0.07079524546861649\nepoch:19, step:0, loss:0.06749674677848816\nepoch:19, step:200, loss:0.06729631125926971\nepoch:19, step:400, loss:0.06982756406068802\nfindfont: Font family ['STKaiTi'] not found. Falling back to DejaVu Sans.\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 648x504 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"444.6325pt\" version=\"1.1\" viewBox=\"0 0 578.7625 444.6325\" width=\"578.7625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 444.6325 \nL 578.7625 444.6325 \nL 578.7625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 69.3625 387.72 \nL 571.5625 387.72 \nL 571.5625 7.2 \nL 69.3625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m96a0f04a42\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"92.189773\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(85.827273 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.570358\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(156.845358 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.950944\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(234.225944 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.331529\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(311.606529 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.712115\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(388.987115 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"479.0927\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(466.3677 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"556.473286\" xlink:href=\"#m96a0f04a42\" y=\"387.72\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(543.748286 409.916875)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Epoch -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(289.840625 433.273125)scale(0.2 -0.2)\">\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdd8268ec1e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.3625\" xlink:href=\"#mdd8268ec1e\" y=\"343.786346\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.1 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(30.55625 351.384784)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.3625\" xlink:href=\"#mdd8268ec1e\" y=\"262.335931\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.2 -->\n      <g transform=\"translate(30.55625 269.934369)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.3625\" xlink:href=\"#mdd8268ec1e\" y=\"180.885516\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.3 -->\n      <g transform=\"translate(30.55625 188.483954)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.3625\" xlink:href=\"#mdd8268ec1e\" y=\"99.435101\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.4 -->\n      <g transform=\"translate(30.55625 107.033539)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.3625\" xlink:href=\"#mdd8268ec1e\" y=\"17.984687\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.5 -->\n      <g transform=\"translate(30.55625 25.583124)scale(0.2 -0.2)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- MSE -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     </defs>\n     <g transform=\"translate(22.396875 218.755312)rotate(-90)scale(0.2 -0.2)\">\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"149.755859\" xlink:href=\"#DejaVuSans-69\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p5415b5f4b7)\" d=\"M 92.189773 24.496364 \nL 99.927831 283.473938 \nL 107.66589 295.73188 \nL 115.403948 298.313712 \nL 123.142007 306.534736 \nL 130.880065 314.043897 \nL 138.618124 315.554014 \nL 146.356183 320.390437 \nL 154.094241 326.078894 \nL 161.8323 327.01406 \nL 169.570358 330.02138 \nL 177.308417 334.449956 \nL 185.046475 335.182889 \nL 192.784534 337.199509 \nL 200.522592 340.56659 \nL 208.260651 341.318038 \nL 215.99871 342.822171 \nL 223.736768 345.198049 \nL 231.474827 346.130345 \nL 239.212885 347.292286 \nL 246.950944 348.816166 \nL 254.689002 349.985608 \nL 262.427061 350.880473 \nL 270.165119 351.754062 \nL 277.903178 353.158526 \nL 285.641237 353.851522 \nL 293.379295 354.207969 \nL 301.117354 355.812573 \nL 308.855412 356.373851 \nL 316.593471 356.300107 \nL 324.331529 358.105828 \nL 332.069588 358.552059 \nL 339.807646 358.147816 \nL 347.545705 360.097579 \nL 355.283763 360.468561 \nL 363.021822 359.793352 \nL 370.759881 361.847397 \nL 378.497939 362.164781 \nL 386.235998 361.260595 \nL 393.974056 363.407331 \nL 401.712115 363.691362 \nL 409.450173 362.581695 \nL 417.188232 364.802461 \nL 424.92629 365.067067 \nL 432.664349 363.769161 \nL 440.402408 366.074643 \nL 448.140466 366.314174 \nL 455.878525 364.842325 \nL 463.616583 367.241348 \nL 471.354642 367.466952 \nL 479.0927 365.825949 \nL 486.830759 368.322857 \nL 494.568817 368.533265 \nL 502.306876 366.734056 \nL 510.044935 369.327545 \nL 517.782993 369.51295 \nL 525.521052 367.57374 \nL 533.25911 370.260381 \nL 540.997169 370.423636 \nL 548.735227 368.36192 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <defs>\n     <path d=\"M -3 3 \nL 3 3 \nL 3 -3 \nL -3 -3 \nz\n\" id=\"m49b71369a3\" style=\"stroke:#1f77b4;stroke-linejoin:miter;\"/>\n    </defs>\n    <g clip-path=\"url(#p5415b5f4b7)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"92.189773\" xlink:href=\"#m49b71369a3\" y=\"24.496364\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"99.927831\" xlink:href=\"#m49b71369a3\" y=\"283.473938\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"107.66589\" xlink:href=\"#m49b71369a3\" y=\"295.73188\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"115.403948\" xlink:href=\"#m49b71369a3\" y=\"298.313712\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"123.142007\" xlink:href=\"#m49b71369a3\" y=\"306.534736\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"130.880065\" xlink:href=\"#m49b71369a3\" y=\"314.043897\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"138.618124\" xlink:href=\"#m49b71369a3\" y=\"315.554014\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"146.356183\" xlink:href=\"#m49b71369a3\" y=\"320.390437\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"154.094241\" xlink:href=\"#m49b71369a3\" y=\"326.078894\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"161.8323\" xlink:href=\"#m49b71369a3\" y=\"327.01406\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"169.570358\" xlink:href=\"#m49b71369a3\" y=\"330.02138\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"177.308417\" xlink:href=\"#m49b71369a3\" y=\"334.449956\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"185.046475\" xlink:href=\"#m49b71369a3\" y=\"335.182889\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"192.784534\" xlink:href=\"#m49b71369a3\" y=\"337.199509\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"200.522592\" xlink:href=\"#m49b71369a3\" y=\"340.56659\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"208.260651\" xlink:href=\"#m49b71369a3\" y=\"341.318038\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"215.99871\" xlink:href=\"#m49b71369a3\" y=\"342.822171\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"223.736768\" xlink:href=\"#m49b71369a3\" y=\"345.198049\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"231.474827\" xlink:href=\"#m49b71369a3\" y=\"346.130345\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"239.212885\" xlink:href=\"#m49b71369a3\" y=\"347.292286\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"246.950944\" xlink:href=\"#m49b71369a3\" y=\"348.816166\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"254.689002\" xlink:href=\"#m49b71369a3\" y=\"349.985608\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"262.427061\" xlink:href=\"#m49b71369a3\" y=\"350.880473\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"270.165119\" xlink:href=\"#m49b71369a3\" y=\"351.754062\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"277.903178\" xlink:href=\"#m49b71369a3\" y=\"353.158526\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"285.641237\" xlink:href=\"#m49b71369a3\" y=\"353.851522\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"293.379295\" xlink:href=\"#m49b71369a3\" y=\"354.207969\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"301.117354\" xlink:href=\"#m49b71369a3\" y=\"355.812573\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"308.855412\" xlink:href=\"#m49b71369a3\" y=\"356.373851\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"316.593471\" xlink:href=\"#m49b71369a3\" y=\"356.300107\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"324.331529\" xlink:href=\"#m49b71369a3\" y=\"358.105828\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"332.069588\" xlink:href=\"#m49b71369a3\" y=\"358.552059\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"339.807646\" xlink:href=\"#m49b71369a3\" y=\"358.147816\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"347.545705\" xlink:href=\"#m49b71369a3\" y=\"360.097579\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"355.283763\" xlink:href=\"#m49b71369a3\" y=\"360.468561\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"363.021822\" xlink:href=\"#m49b71369a3\" y=\"359.793352\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"370.759881\" xlink:href=\"#m49b71369a3\" y=\"361.847397\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"378.497939\" xlink:href=\"#m49b71369a3\" y=\"362.164781\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"386.235998\" xlink:href=\"#m49b71369a3\" y=\"361.260595\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"393.974056\" xlink:href=\"#m49b71369a3\" y=\"363.407331\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"401.712115\" xlink:href=\"#m49b71369a3\" y=\"363.691362\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"409.450173\" xlink:href=\"#m49b71369a3\" y=\"362.581695\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"417.188232\" xlink:href=\"#m49b71369a3\" y=\"364.802461\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"424.92629\" xlink:href=\"#m49b71369a3\" y=\"365.067067\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"432.664349\" xlink:href=\"#m49b71369a3\" y=\"363.769161\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"440.402408\" xlink:href=\"#m49b71369a3\" y=\"366.074643\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"448.140466\" xlink:href=\"#m49b71369a3\" y=\"366.314174\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"455.878525\" xlink:href=\"#m49b71369a3\" y=\"364.842325\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"463.616583\" xlink:href=\"#m49b71369a3\" y=\"367.241348\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"471.354642\" xlink:href=\"#m49b71369a3\" y=\"367.466952\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"479.0927\" xlink:href=\"#m49b71369a3\" y=\"365.825949\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"486.830759\" xlink:href=\"#m49b71369a3\" y=\"368.322857\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"494.568817\" xlink:href=\"#m49b71369a3\" y=\"368.533265\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"502.306876\" xlink:href=\"#m49b71369a3\" y=\"366.734056\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"510.044935\" xlink:href=\"#m49b71369a3\" y=\"369.327545\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"517.782993\" xlink:href=\"#m49b71369a3\" y=\"369.51295\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"525.521052\" xlink:href=\"#m49b71369a3\" y=\"367.57374\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"533.25911\" xlink:href=\"#m49b71369a3\" y=\"370.260381\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"540.997169\" xlink:href=\"#m49b71369a3\" y=\"370.423636\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"548.735227\" xlink:href=\"#m49b71369a3\" y=\"368.36192\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 69.3625 387.72 \nL 69.3625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 571.5625 387.72 \nL 571.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 69.3625 387.72 \nL 571.5625 387.72 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 69.3625 7.2 \nL 571.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 469.55625 52.55625 \nL 557.5625 52.55625 \nQ 561.5625 52.55625 561.5625 48.55625 \nL 561.5625 21.2 \nQ 561.5625 17.2 557.5625 17.2 \nL 469.55625 17.2 \nQ 465.55625 17.2 465.55625 21.2 \nL 465.55625 48.55625 \nQ 465.55625 52.55625 469.55625 52.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 473.55625 33.396875 \nL 513.55625 33.396875 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\">\n     <g>\n      <use style=\"fill:#1f77b4;stroke:#1f77b4;stroke-linejoin:miter;\" x=\"493.55625\" xlink:href=\"#m49b71369a3\" y=\"33.396875\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 训练 -->\n     <defs>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-35757\"/>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-32451\"/>\n     </defs>\n     <g transform=\"translate(529.55625 40.396875)scale(0.2 -0.2)\">\n      <use xlink:href=\"#DejaVuSans-35757\"/>\n      <use x=\"60.009766\" xlink:href=\"#DejaVuSans-32451\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5415b5f4b7\">\n   <rect height=\"380.52\" width=\"502.2\" x=\"69.3625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "losses = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    for step , (x, y) in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 28*28])\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 第一层计算 [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n",
    "            h1 = x @ w1 + b1\n",
    "            h1 = tf.nn.relu(h1)  # 通过激活函数(ReLU)\n",
    "            \n",
    "            h2 = h1 @ w2 + b2\n",
    "            h2 = tf.nn.relu(h2)\n",
    "\n",
    "            out = h2 @ w3 + b3\n",
    "            \n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            # 计算误差\n",
    "            loss = tf.reduce_mean(tf.square(out - y_onehot))\n",
    "        # 自动计算梯度 \n",
    "        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
    "        # 梯度更新，assign_sub 将当前值减去参数值，原地更新\n",
    "        for para, grad in zip([w1, b1, w2, b2, w3, b3], grads):\n",
    "            para.assign_sub(lr * grad)\n",
    "        # w1.assign_sub(lr * grads[0])\n",
    "        # b1.assign_sub(lr * grads[1])\n",
    "        # w2.assign_sub(lr * grads[2])\n",
    "        # b2.assign_sub(lr * grads[3])\n",
    "        # w3.assign_sub(lr * grads[4])\n",
    "        # b3.assign_sub(lr * grads[5])\n",
    "        if step %  200 == 0:\n",
    "            losses.append(loss)\n",
    "            print(f'epoch:{epoch}, step:{step}, loss:{loss}')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses, color='C0', marker='s', label='训练')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "plt.savefig('forward.svg')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bittf2conda2a75a45106264ceab7472c43279a5d24",
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}