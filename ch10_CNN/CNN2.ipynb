{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 与 VGG13 实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.family'] = ['SimHei'] # ['Noto Sans CJK JP']\n",
    "plt.rcParams['axes.unicode_minus']=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, losses, metrics, Sequential, layers, optimizers, Model, Input\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y_onehot = tf.one_hot(y, depth=10)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_db = train_db.shuffle(10000).batch(128).map(preprocessing)\n",
    "test_db = test_db.shuffle(10000).batch(128).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_db))\n",
    "tf.reduce_max(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[4]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer = layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "out_1 = pool_layer(X_train[4][np.newaxis])\n",
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_layer = layers.MaxPool2D(pool_size=(2, 2), strides=1, padding='same')\n",
    "out_2 = pool_layer(X_train[4][np.newaxis])\n",
    "out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.array_equal(out_1, out_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 图片识别任务并不简单，这主要是由于 CIFAR10 的图片内容需要大量细节才能呈现，而保存的图片分辨率仅有32 × 32，使得部分主体信息较为模糊，甚至人眼都很难分辨。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 卷积子网络\n",
    "conv_layers = [\n",
    "    # 64个 3X3 的卷积核 输出与输入同大小\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(32,32,3)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    # 池化层 高宽减半\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元2  输出通道提升至 128，高宽大小减半\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元3  输出通道提升至 256，高宽大小减半\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元4  输出通道提升至 512，高宽大小减半\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元4  输出通道提升至 512，高宽大小减半\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),                       \n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu', input_shape=(1, 1, 512)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation=None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG13 = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(VGG13, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG13.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3 * 3 * 3 + 1) * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3 * 3 * 64 + 1) * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3*3*64 + 1) * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50000 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG13():\n",
    "    loss = []\n",
    "    acc = []\n",
    "    metric_acc = metrics.Accuracy()\n",
    "    optimizer = optimizers.Adam(0.0001)\n",
    "    conv_net.build(input_shape=(None, 32, 32, 3))\n",
    "    fc_net.build(input_shape=(None, 512)) \n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = VGG(x)\n",
    "                # print(out.shape)\n",
    "                # y_onehot = tf.one_hot(y, depth=10)\n",
    "                cost = losses.categorical_crossentropy(y, out, from_logits=True)\n",
    "                cost =tf.reduce_mean(cost)\n",
    "            grads = tape.gradient(cost, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                loss.append(float(cost))\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {float(cost)}\")\n",
    "        \n",
    "        # 每个epoch 检测一次test\n",
    "        metric_acc.reset_states()\n",
    "        for x, y in test_db:\n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, (-1, 512))\n",
    "\n",
    "            out = fc_net(out)\n",
    "            y_pred = tf.argmax(out, axis=-1)\n",
    "            metric_acc.update_state(y, y_pred)\n",
    "        print(f\"epoch :{epoch},  accuracy: {float(metric_acc.result())}\")\n",
    "        acc.append(float(metric_acc.result()))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层变种\n",
    "\n",
    "### 空洞卷积(Dilated/Atrous Convolution)\n",
    "\n",
    "普通的卷积层为了减少网络的参数量，卷积核的设计通常选择较小的 $1\\times1$ 和$3 \\times 3$感受野大小。小卷积核使得网络提取特征时的感受野区域有限，但是增大感受野的区域又会增加网络的参数量和计算代价.\n",
    "\n",
    "空洞卷积在普通卷积的感受野上增加一个Dilation Rate 参数, 用于控制感受野区域的采样步长\n",
    "![](./空洞卷积.png)\n",
    "\n",
    "尽管Dilation Rate 的增大会使得感受野区域增大，但是实际参与运算的点数仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([1, 7, 7, 1])\n",
    "# 空洞卷积  1个 3 X 3 的核\n",
    "layer = layers.Conv2D(1, kernel_size=(3, 3), strides=1, dilation_rate=2)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转置卷积\n",
    "转置卷积(Transposed Convolution，或Fractionally Strided Convolution)通过在输入之间填充大量的padding 来实现输出高宽大于输入高宽的效果，从而实现向上采样的目的.转置卷积具有“**放大特征图**”的功能\n",
    "\n",
    "转置卷积与普通卷积并不是互为逆过程，不能恢复出对方的输入内容，仅能恢复出等大小的张量.\n",
    "\n",
    "例: \n",
    "- 输入$i = 2 \\times 2$ 单通道特征图, 转置卷积核$k=3 \\times 3$, 填充$p=0$, 步长$s=2$;\n",
    "- $2 \\times 2$ 内部均匀填充$s-1$个空白输入点 ---> $3 \\times 3$;\n",
    "- $3 \\times 3$ 周围填充$𝑘 − 𝑝 −1 = 3 − 0−1 = 2行/列$, ---> $7 \\times 7$;\n",
    "- $7 \\times 7$ 与 $3 \\times 3$的卷积核卷积, 步长$s' = 1$(固定), 填充$p=0$ ---> $o = 5 \\times 5$的输出;\n",
    "\n",
    "- 恢复: $5 \\times 5$ 与 $3 \\times 3$的卷积核卷积, 填充$p=0$, 步长$s=2$ ----> $2 \\times 2$的输入 \n",
    "\n",
    "在𝑜 + 2𝑝 − 𝑘为s 倍数时，满足关系:\n",
    "$$o = (i-1)s +k-2p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(25) + 1\n",
    "# [b, h, w, c]\n",
    "x = tf.reshape(x, [1, 5, 5, 1])\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "\n",
    "# 3X3 卷积核\n",
    "w = tf.constant([[-1, 2, -3.], [4, -5, 6], [-7, 8, -9]])\n",
    "w = tf.expand_dims(w, axis=2)\n",
    "w =  tf.expand_dims(w, axis=3)\n",
    "# [f_w, f_h, c_in, c_out]\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充0, 步长2\n",
    "out = tf.nn.conv2d(x, w, strides=2, padding='VALID')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d_transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将普通卷积作为转置卷积的输入\n",
    "\n",
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 5, 5, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积运算的输出大小\n",
    "$$o = ⌊\\frac {i + 2\\cdot p_h - k}{s}⌋ + 1$$\n",
    "当𝒐 + 𝟐𝒑 − 𝒌不为𝒔倍数时, 当s>1, 向下取整会使得多种不同大小的输入i得到相同大小的输出o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([1,6,6,1])\n",
    "out = tf.nn.conv2d(x, w, strides=2, padding='VALID')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 5, 5, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定output_shape 得到输出尺寸\n",
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 6, 6, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = tf.nn.conv2d_transpose(out, w, strides=3, padding='SAME', output_shape=[1, 6, 6, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵角度理解\n",
    "普通Conv2d运算:\n",
    "- X:(4, 4), W:(3, 3), 步长为1，无padding\n",
    "- X打平X'(1, 16), W转为稀疏矩阵W'(4, 16)\n",
    "- 输出$O'= W'@X'$ (4,1), reshape 得到O(2, 2)\n",
    "\n",
    "转置卷积\n",
    "- $W'$转置后与$O'$矩阵相乘$X'=W'^T@O'$大小为(16, 1), reshape得到(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  转置卷积层\n",
    "\n",
    "- 当padding=’VALID’时，输出大小表达为:\n",
    "   $$o=(i-1)s+k$$\n",
    "- 当设置padding=’SAME’时，输出大小表达为:\n",
    "   $$o = i \\cdot s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer = layers.Conv2DTranspose(1, kernel_size=3, strides=1, padding='VALID')\n",
    "xx2 = layer(out)\n",
    "xx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = layers.Conv2DTranspose(1, kernel_size=3, strides=3, padding='SAME')\n",
    "xx2 = layer(out)\n",
    "xx2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分离卷积(Separable Convolution)\n",
    "\n",
    "普通卷积在对多通道输入进行运算时，卷积核的每个通道与输入的每个通道分别进行卷积运算，得到多通道的特征图，再对应元素相加产生单个卷积核的最终输出\n",
    "$$输入[1, h, w, 3] \\otimes 多卷积核[3, 3, 3, 4] \\rightarrow 中间特征 \\rightarrow \\sum对应元素求和 = \n",
    "输出[1, h', w', 4]\n",
    "$$\n",
    "\n",
    "分离卷积流程:\n",
    "$$输入[1, h, w, 3] \\otimes 单卷积核[3, 3, 3, 1] \\rightarrow 中间特征 \\rightarrow \\\\ \\otimes 4个1\\times 1 卷积核[1, 1, 3, 4] =\n",
    "输出[1, h', w', 4]\n",
    "$$\n",
    "\n",
    "可以看到，分离卷积层包含了两步卷积运算，第一步卷积运算是单个卷积核，第二个卷积运算包含了多个卷积核。\n",
    "\n",
    "优势:\n",
    "- 相同输入要产生相同大小输出, 分离卷积的参数量约是普通卷积的$\\frac 1 3$\n",
    "\n",
    "$1 \\times 1$卷积核的作用:\n",
    "- 实现信息的跨通道交互和整合。\n",
    "- 对卷积核通道数进行降维和升维，减小参数量, 降低计算成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度残差网络ResNet\n",
    "当模型加深以后，网络变得越来越难训练，这主要是由于**梯度弥散**和**梯度爆炸**现象造成的。在较深层数的神经网络中，梯度信息由网络的末层逐层传向网络的首层时，传递的过程中会出现梯度接近于0 或梯度值非常大的现象。网络层数越深，这种现象可能会越严重。\n",
    "\n",
    "通过在输入和输出之间添加一条直接连接的**Skip Connection**可以让神经网络具有回退的能力.它可以从某一层网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层。我们可以利用跳跃连接构建能够训练深度网络的**ResNet**.\n",
    "\n",
    "### ResNet 原理\n",
    "ResNet 通过在卷积层的输入和输出之间添加Skip Connection 实现层数回退机制, 输入$x$通过2个卷积层, 得到特征变换后的输出$\\mathcal F(x)$, 与输入$x$进行对应元素的相加运算, 得到最终输出$\\mathcal H(x)$:\n",
    "$$\\mathcal H(x) = x + \\mathcal F(x)$$\n",
    "> 需要保持输入$x$的shape与$\\mathcal F(x)$的shape完全一致\n",
    "\n",
    "![](./残差模块.png)\n",
    "$\\mathcal H(x)$叫作残差模块(Residual Block, ResBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResBlock的实现\n",
    "class ResBlock(layers.Layer):\n",
    "    def __init__(self, fliter_num, stride=1):\n",
    "        super().__init__()\n",
    "        # f(x)函数包括2个普通的卷积层\n",
    "        self.conv = layers.Conv2D(fliter_num, (3,3), strides=stride, padding='SAME')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "        # 第二个卷积层步长为1,\n",
    "        self.conv2 = layers.Conv2D(fliter_num, (3,3), strides=1, padding='SAME')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        if stride != 1:  # F 与 x的形状不相同\n",
    "            self.downsample = Sequential()\n",
    "            # 使用 1X1 卷积核\n",
    "            self.downsample.add(layers.Conv2D(fliter_num, (1, 1), strides=stride))\n",
    "        else:\n",
    "            # 直接连接\n",
    "            self.downsample = lambda x:x\n",
    "            \n",
    "    def call(self, x):\n",
    "        # 前向传播  H(x) = x + F(x)\n",
    "        out = self.conv(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        identity = self.downsample(x)\n",
    "        \n",
    "        output = layers.add([out, identity])\n",
    "        # 得到H(x)后再进过激活函数 \n",
    "        output = tf.nn.relu(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(Model):\n",
    "    def __init__(self, layer_dims, num_classe=10):\n",
    "        super().__init__()\n",
    "         # 根网络\n",
    "        self.stem = Sequential([\n",
    "            layers.Conv2D(64, (3, 3), strides=(1, 1), input_shape=(32, 32, 3)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n",
    "        ])\n",
    "        # 4个block 每个block有多个残差模块\n",
    "        self.layer1 = self.build_resblock(64, layer_dims[0])\n",
    "        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "        \n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(num_classe)\n",
    "    \n",
    "    def build_resblock(self, fliter_num, block_num, stride=1):\n",
    "        res_block = Sequential([])\n",
    "        # 第一个block 要注意进行 1x1卷积处理\n",
    "        res_block.add(ResBlock(fliter_num, stride))\n",
    "        \n",
    "        for _ in range(1, block_num):\n",
    "            # 其他的resblock  步长全为1\n",
    "            res_block.add(ResBlock(fliter_num, stride=1))\n",
    "        return res_block\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 根网络\n",
    "        x = self.stem(inputs)\n",
    "        # 4个模块\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # 池化层\n",
    "        x = self.avgpool(x)\n",
    "        # 全连接层\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(layer_dims, input_shape, num_classe=10):\n",
    "    def resblock(X, fliter_num, stride=1):\n",
    "        conv = layers.Conv2D(fliter_num, (3,3), strides=stride, padding='SAME')\n",
    "        bn1 = layers.BatchNormalization()\n",
    "        relu = layers.ReLU()\n",
    "\n",
    "        # 第二个卷积层步长为1\n",
    "        conv2 = layers.Conv2D(fliter_num, (3,3), strides=1, padding='SAME')\n",
    "        bn2 = layers.BatchNormalization()\n",
    "\n",
    "        if stride != 1:  # F 与 x的形状不相同\n",
    "            downsample = layers.Conv2D(fliter_num, (1, 1), strides=stride)\n",
    "            # 使用 1X1 卷积核\n",
    "        else:\n",
    "            # 直接连接\n",
    "            downsample = lambda x:x\n",
    "\n",
    "        out = conv(X)\n",
    "        out = bn1(out)\n",
    "        out = relu(out)\n",
    "        out = conv2(out)\n",
    "        out = bn2(out)\n",
    "        identity = downsample(X)\n",
    "        out = layers.add([out, identity])\n",
    "        out = layers.ReLU()(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def build_resblock(X, fliter_num, block_num, stride=1):\n",
    "\n",
    "        # 第一个block 要注意进行 1x1卷积处理\n",
    "\n",
    "        out = resblock(X, fliter_num, stride)\n",
    "        \n",
    "        for _ in range(1, block_num):\n",
    "            # 其他的resblock  步长全为1\n",
    "            out = resblock(out, fliter_num, stride=1)\n",
    "        return out\n",
    "    \n",
    "    X_inputs = Input(input_shape)\n",
    "    x = layers.Conv2D(64, (3, 3), strides=(1, 1), input_shape=(32, 32, 3))(X_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')(x)\n",
    "    x = build_resblock(x, 64, layer_dims[0])\n",
    "    x = build_resblock(x, 128, layer_dims[1], stride=2)\n",
    "    x = build_resblock(x, 256, layer_dims[2], stride=2)\n",
    "    x = build_resblock(x, 512, layer_dims[3], stride=2)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    out = layers.Dense(num_classe)(x)\n",
    "    \n",
    "    model = Model(inputs=X_inputs, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = resnet([2,2,2,2], (32, 32, 3), 10)\n",
    "plot_model(model2, show_shapes=True, to_file='ResNet18.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_dot(model2, show_shapes=True).write('ResNet18', prog='dot', format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydot.Dot.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(model_to_dot(model2, show_shapes=True)).render('ResNet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18([2, 2, 2, 2], 10)\n",
    "model.build(input_shape=(None, 32, 32, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3*3*3+1)*64 + 4*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((3*3*64 +1) *64 *2 + 4*64*2)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_db, epochs=20, validation_data=test_db, validation_freq=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet\n",
    "\n",
    "DenseNet 将前面所有层的特征图信息通过Skip Connection 与当前层输出进行聚合，与ResNet的对应位置相加方式不同，DenseNet 采用在通道轴𝑐维度进行拼接操作，聚合特征信息。\n",
    "\n",
    "DenseNet:\n",
    "\n",
    "https://github.com/liuzhuang13/DenseNet"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
