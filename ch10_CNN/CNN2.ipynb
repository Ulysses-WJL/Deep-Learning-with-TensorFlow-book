{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 与 VGG13 实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['figure.titlesize'] = 20\n",
    "plt.rcParams['figure.figsize'] = [12, 10]\n",
    "plt.rcParams['font.family'] = ['SimHei'] # ['Noto Sans CJK JP']\n",
    "plt.rcParams['axes.unicode_minus']=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, losses, metrics, Sequential, layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3) (50000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_db = train_db.shuffle(10000).batch(128).map(preprocessing)\n",
    "test_db = test_db.shuffle(10000).batch(128).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=55, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_db))\n",
    "tf.reduce_max(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2604eba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[4]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 16, 16, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_layer = layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='valid')\n",
    "out_1 = pool_layer(X_train[4][np.newaxis])\n",
    "out_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 16, 16, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_layer = layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "out_2 = pool_layer(X_train[4][np.newaxis])\n",
    "out_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array_equal(out_1, out_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 图片识别任务并不简单，这主要是由于 CIFAR10 的图片内容需要大量细节才能呈现，而保存的图片分辨率仅有32 × 32，使得部分主体信息较为模糊，甚至人眼都很难分辨。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 卷积子网络\n",
    "conv_layers = [\n",
    "    # 64个 3X3 的卷积核 输出与输入同大小\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    # 池化层 高宽减半\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元2  输出通道提升至 128，高宽大小减半\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元3  输出通道提升至 256，高宽大小减半\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(256, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元4  输出通道提升至 512，高宽大小减半\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "    # Conv-Conv-Pooling 单元4  输出通道提升至 512，高宽大小减半\n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),                       \n",
    "    layers.Conv2D(512, kernel_size=(3, 3), padding='SAME', activation='relu'),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_net = Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.build(input_shape=(None, 32, 32, 3))\n",
    "fc_net.build(input_shape=(None, 512)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  1792      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            multiple                  73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            multiple                  295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            multiple                  590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 multiple                  0         \n",
      "=================================================================\n",
      "Total params: 9,404,992\n",
      "Trainable params: 9,404,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3 * 3 * 3 + 1) * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36928"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 * 3 * 64 + 1) * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73856"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3*3*64 + 1) * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50000 / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16():\n",
    "    loss = []\n",
    "    acc = []\n",
    "    metric_acc = metrics.Accuracy()\n",
    "    optimizer = optimizers.Adam(0.0001)\n",
    "    conv_net.build(input_shape=(None, 32, 32, 3))\n",
    "    fc_net.build(input_shape=(None, 512)) \n",
    "    variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        for step, (x, y) in enumerate(train_db):\n",
    "            with tf.GradientTape() as tape:\n",
    "                out = conv_net(x)\n",
    "                out = tf.reshape(out, (-1, 512))\n",
    "\n",
    "                out = fc_net(out)\n",
    "                # print(out.shape)\n",
    "                y_onehot = tf.one_hot(y, depth=10)\n",
    "                cost = losses.categorical_crossentropy(y_onehot, out, from_logits=True)\n",
    "                cost =tf.reduce_mean(cost)\n",
    "            grads = tape.gradient(cost, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                loss.append(float(cost))\n",
    "                print(f\"epoch: {epoch}, step: {step}, loss: {float(cost)}\")\n",
    "        \n",
    "        # 每个epoch 检测一次test\n",
    "        metric_acc.reset_states()\n",
    "        for x, y in test_db:\n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, (-1, 512))\n",
    "\n",
    "            out = fc_net(out)\n",
    "            y_pred = tf.argmax(out, axis=-1)\n",
    "            metric_acc.update_state(y, y_pred)\n",
    "        print(f\"epoch :{epoch},  accuracy: {float(metric_acc.result())}\")\n",
    "        acc.append(float(metric_acc.result()))\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 2.3026978969573975\n",
      "epoch: 0, step: 100, loss: 1.867992877960205\n",
      "epoch: 0, step: 200, loss: 1.670213222503662\n",
      "epoch: 0, step: 300, loss: 1.7289377450942993\n",
      "epoch :0,  accuracy: 0.42239999771118164\n",
      "epoch: 1, step: 0, loss: 1.5254759788513184\n",
      "epoch: 1, step: 100, loss: 1.601982593536377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0d5565bab80d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-2af9a5693e88>\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32md:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    607\u001b[0m   ]\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3.7\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mfilter_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m         explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss, acc = VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层变种\n",
    "\n",
    "### 空洞卷积(Dilated/Atrous Convolution)\n",
    "\n",
    "普通的卷积层为了减少网络的参数量，卷积核的设计通常选择较小的 $1\\times1$ 和$3 \\times 3$感受野大小。小卷积核使得网络提取特征时的感受野区域有限，但是增大感受野的区域又会增加网络的参数量和计算代价.\n",
    "\n",
    "空洞卷积在普通卷积的感受野上增加一个Dilation Rate 参数, 用于控制感受野区域的采样步长\n",
    "![](./空洞卷积.png)\n",
    "\n",
    "尽管Dilation Rate 的增大会使得感受野区域增大，但是实际参与运算的点数仍然保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 3, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([1, 7, 7, 1])\n",
    "# 空洞卷积  1个 3 X 3 的核\n",
    "layer = layers.Conv2D(1, kernel_size=(3, 3), strides=1, dilation_rate=2)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转置卷积\n",
    "转置卷积(Transposed Convolution，或Fractionally Strided Convolution)通过在输入之间填充大量的padding 来实现输出高宽大于输入高宽的效果，从而实现向上采样的目的.转置卷积具有“**放大特征图**”的功能\n",
    "\n",
    "转置卷积与普通卷积并不是互为逆过程，不能恢复出对方的输入内容，仅能恢复出等大小的张量.\n",
    "\n",
    "例: \n",
    "- 输入$i = 2 \\times 2$ 单通道特征图, 转置卷积核$k=3 \\times 3$, 填充$p=0$, 步长$s=2$;\n",
    "- $2 \\times 2$ 内部均匀填充$s-1$个空白输入点 ---> $3 \\times 3$;\n",
    "- $3 \\times 3$ 周围填充$𝑘 − 𝑝 −1 = 3 − 0−1 = 2行/列$, ---> $7 \\times 7$;\n",
    "- $7 \\times 7$ 与 $3 \\times 3$的卷积核卷积, 步长$s' = 1$(固定), 填充$p=0$ ---> $o = 5 \\times 5$的输出;\n",
    "\n",
    "- 恢复: $5 \\times 5$ 与 $3 \\times 3$的卷积核卷积, 填充$p=0$, 步长$s=2$ ----> $2 \\times 2$的输入 \n",
    "\n",
    "在𝑜 + 2𝑝 − 𝑘为s 倍数时，满足关系:\n",
    "$$o = (i-1)s +k-2p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(25) + 1\n",
    "# [b, h, w, c]\n",
    "x = tf.reshape(x, [1, 5, 5, 1])\n",
    "x = tf.cast(x, dtype=tf.float32)\n",
    "\n",
    "# 3X3 卷积核\n",
    "w = tf.constant([[-1, 2, -3.], [4, -5, 6], [-7, 8, -9]])\n",
    "w = tf.expand_dims(w, axis=2)\n",
    "w =  tf.expand_dims(w, axis=3)\n",
    "# [f_w, f_h, c_in, c_out]\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=135864, shape=(1, 2, 2, 1), dtype=float32, numpy=\n",
       "array([[[[ -67.],\n",
       "         [ -77.]],\n",
       "\n",
       "        [[-117.],\n",
       "         [-127.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充0, 步长2\n",
    "out = tf.nn.conv2d(x, w, strides=2, padding='VALID')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d_transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 5, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将普通卷积作为转置卷积的输入\n",
    "\n",
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 5, 5, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积运算的输出大小\n",
    "$$o = ⌊\\frac {i + 2\\cdot p_h - k}{s}⌋ + 1$$\n",
    "当𝒐 + 𝟐𝒑 − 𝒌不为𝒔倍数时, 当s>1, 向下取整会使得多种不同大小的输入i得到相同大小的输出o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([1,6,6,1])\n",
    "out = tf.nn.conv2d(x, w, strides=2, padding='VALID')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 5, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 5, 5, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6, 6, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定output_shape 得到输出尺寸\n",
    "xx = tf.nn.conv2d_transpose(out, w, strides=2, padding='VALID', output_shape=[1, 6, 6, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6, 6, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = tf.nn.conv2d_transpose(out, w, strides=3, padding='SAME', output_shape=[1, 6, 6, 1])\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵角度理解\n",
    "普通Conv2d运算:\n",
    "- X:(4, 4), W:(3, 3), 步长为1，无padding\n",
    "- X打平X'(1, 16), W转为稀疏矩阵W'(4, 16)\n",
    "- 输出$O'= W'@X'$ (4,1), reshape 得到O(2, 2)\n",
    "\n",
    "转置卷积\n",
    "- $W'$转置后与$O'$矩阵相乘$X'=W'^T@O'$大小为(16, 1), reshape得到(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  转置卷积层\n",
    "\n",
    "- 当padding=’VALID’时，输出大小表达为:\n",
    "   $$o=(i-1)s+k$$\n",
    "- 当设置padding=’SAME’时，输出大小表达为:\n",
    "   $$o = i \\cdot s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 4, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "layer = layers.Conv2DTranspose(1, kernel_size=3, strides=1, padding='VALID')\n",
    "xx2 = layer(out)\n",
    "xx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6, 6, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers.Conv2DTranspose(1, kernel_size=3, strides=3, padding='SAME')\n",
    "xx2 = layer(out)\n",
    "xx2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分离卷积(Separable Convolution)\n",
    "\n",
    "普通卷积在对多通道输入进行运算时，卷积核的每个通道与输入的每个通道分别进行卷积运算，得到多通道的特征图，再对应元素相加产生单个卷积核的最终输出\n",
    "$$输入[1, h, w, 3] \\otimes 单卷积核[3, 3, 3, 1] \\rightarrow 中间特征 \\rightarrow \\sum对应元素求和 = \n",
    "输出[1, h', w', 1]\n",
    "$$\n",
    "\n",
    "分离卷积流程:\n",
    "$$输入[1, h, w, 3] \\otimes 单卷积核[3, 3, 3, 1] \\rightarrow 中间特征 \\rightarrow \\\\ \\otimes 4个1\\times 1 卷积核[1, 1, 3, 4] =\n",
    "输出[1, h', w', 4]\n",
    "$$\n",
    "\n",
    "可以看到，分离卷积层包含了两步卷积运算，第一步卷积运算是单个卷积核，第二个卷积运算包含了多个卷积核。\n",
    "\n",
    "优势:\n",
    "- 相同输入要产生相同大小输出, 分离卷积的参数量约是普通卷积的$\\frac 1 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
