{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络(Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[最容易理解的对卷积(convolution)的解释](https://blog.csdn.net/bitcarmanlee/article/details/54729807)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage import data, color\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积的意义\n",
    "\n",
    "卷积的重要的物理意义是：一个函数（如：单位响应）在另一个函数（如：输入信号）上的**加权叠加**。\n",
    "\n",
    "对于线性时不变系统，如果知道该系统的单位响应，那么将单位响应和输入信号求卷积，就相当于把输入信号的各个时间点的单位响应 加权叠加，就直接得到了输出信号。\n",
    "\n",
    "例子:\n",
    "\n",
    "已知$x[0] = a, x[1] = b, x[2]=c$, 已知$y[0] = i, y[1] = j, y[2]=k$, 求$(x \\otimes y)(n)$\n",
    "![](./卷积1.png)\n",
    "\n",
    "最后，把上面三个图叠加，就得到了$(x \\otimes y)(n)$\n",
    "![](./卷积2.png)\n",
    "\n",
    "## 1D连续卷积\n",
    "\n",
    "数学定义: $$(f \\otimes g)(n) = \\int_{-\\infty}^{\\infty} f(\\tau)g(n-\\tau)d\\tau$$\n",
    "\n",
    "例子: 做馒头\n",
    "\n",
    "假设馒头的生产速度是 $f(t)$, 馒头生产出来之后，就会慢慢腐败，假设腐败函数为$g(t)$, 那么一天后馒头总共腐败了：\n",
    "$$\\int_0^{24}f(t)g(24-t)dt$$\n",
    "\n",
    "## 离散卷积\n",
    "\n",
    "将积分运算换成累加运算:\n",
    "$$s(n) = (f\\otimes g)(n) = \\sum_{\\tau=-\\infty}^{\\infty}f(\\tau)g(n-\\tau)$$\n",
    "例子: 掷骰子\n",
    "\n",
    "设现有两枚骰子, 这两枚骰子掷出去, 求两枚骰子点数加起来为4的概率是多少?\n",
    "\n",
    "令$f(n), g(n)$分别代表2枚骰子点数的概率, 两枚骰子点数加起来为4的概率为：\n",
    "$$f(1)g(3)+f(2)g(2)+f(3)g(1)$$\n",
    "符合卷积的定义，把它写成标准的形式就是：\n",
    "$$(f\\otimes g)(4) = \\sum_{m=1}^{3}f(4-m)g(m)$$\n",
    "\n",
    "> 暴力解释: 扇巴掌 - -\n",
    "\n",
    "在卷积网络的术语中, 卷积的第一个参数(函数f)通常叫作**输入**(Input), 第二个参数(函数g)叫作**核函数**(kernel function). 输出有时被称作**特征映射**(feature map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([[2., 3, 1], [0, 5, 1], [1, 0, 8]])\n",
    "g = np.array([[-1, -1., -1], [-1, 8, -1], [-1, -1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.convolve([1, 2, 3, 4], [1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D离散卷积\n",
    "在计算机视觉中，卷积运算基于2D 图片函数$f(m, n)$和2D卷积核$g(m, n)$，其中$f(i, j)$和$g(i, j)$仅在各自窗口有效区域存在值，其它区域视为0.此时, 2D离散卷积定义为:\n",
    "$$S(m,n) = [f\\otimes g](m, n) = \\sum_{i=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty}f(i, j)g(m-i, n-j)$$\n",
    "卷积是可交换的(commutative), 可以等价的写作:\n",
    "$$S(m, n) = [f\\otimes g](m, n) = \\sum_i \\sum_j f(m-i, n-j)g(i, j)$$\n",
    "\n",
    "卷积运算的可交换性的出现是因为我们将核相对输入进行了**翻转**(filp), 这里二维的反转就是将卷积核沿x轴和y轴各翻转一次，比如：\n",
    "![](./核翻转.png)\n",
    "\n",
    "将核翻转的唯一目的是实现可交换性, 但在神经网络中的应用却不是一个重要的性质, 许多神经网络库会实现一个相关的函数,称为**互相关函数**(cross-correlation), 和卷积运算几乎一样但是并没有对核进行翻转:\n",
    "$$S(m, n) =[f\\otimes g](m, n) = \\sum_i \\sum_j f(m+i, n+j)g(i, j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部相关性\n",
    "\n",
    "全连接层也称为稠密连接层(Dense Layer)，输出与输入的关系为:\n",
    "$$o_j = \\sigma(\\sum_{i \\in nodes(I)} w_{ij}x_i + b_j)$$\n",
    "简化上述模型, 分析输入节点对输出节点的重要性分布, 仅考虑较重要的一部分输入节点而抛弃重要性较低的部分节点, 输出节点只需要与部分输入节点相连接:\n",
    "$$o_j = \\sigma(\\sum_{i \\in top(I, J, k)} w_{ij}x_i + b_j)$$\n",
    "其中$top(I, J, k)$表示第I层中对第J层中的j号节点重要性最高的k个节点集合.将全连接层的$||I|| \\cdot ||J||$个权值连接减少到$k\\cdot ||J||$个.\n",
    "\n",
    "根据先验知识, 可以得知图片每个像素点和周边像素点的关联度更大(位置相关). 以2D 图片数据为例，如果简单地认为与当前像素欧式距离(Euclidean Distance)小于和等于$\\frac {k}{\\sqrt 2}$的像素点重要性较高，欧式距离大于$\\frac {k}{\\sqrt 2}$的像素点重要性较低，那么我们就很轻松地简化了每个像素点的重要性分布问题。\n",
    "\n",
    "以实心网格所在的像素为参考点，它周边欧式距离小于或等于$\\frac {k}{\\sqrt 2}$的像素点以矩形网格表示，网格内的像素点重要性较高，网格外的像素点较低。这个高宽为𝑘的窗口称为**感受域**(Receptive Field)，它表征了每个像素对于中心像素的重要性分布情况，网格内的像素才会被考虑，网格外的像素对于中心像素会被简单忽略.\n",
    "\n",
    "这种基于距离的重要性分布假设特性称为**局部相关性**.\n",
    "\n",
    "当前位置的节点与大小为𝑘的窗口内的所有像素相连接，与窗口外的其它像素点无关，此时网络层的输入输出关系:\n",
    "$$o_j = \\sigma \\left(\\sum_{dist(i, j)\\leq \\frac {k}{\\sqrt 2}} w_{ij}x_i + b_j\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数共享\n",
    "参数共享(parameter sharing)是指在一个模型的多个函数汇总使用相同的参数.在卷积神经网络中, 核的每一个元素都作用在输入的每一个位置上, 保证了我们只需要学习一个参数集合, 而不是对于每一位置都需要学习一个单独的参数集合.\n",
    "\n",
    "即每个输出节点仅与感受域区域内$k \\times k$个输入节点相连接，输出层节点数为$‖J‖$，则当前层的参数量为$k \\times k \\times ||J||$，对于每个输出节点$o_j$，均使用相同的权值矩阵$𝑾$，那么无论输出节点的数量$‖J‖$是多少，网络层的参数量总是$k\\times k$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|卷积作用|卷积核|\n",
    "|:---------------------------:|:------------------------------:|\n",
    "|输出原图|$\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$|\n",
    "|边缘检测（突出边缘差异）|$\\begin{bmatrix} 1 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}$|\n",
    "|边缘检测（突出中间值）|$\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}$|\n",
    "|方块模糊|$\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix} \\times \\frac{1}{9}$|\n",
    "|图像锐化|$\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$|\n",
    "|高斯模糊|$\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 1 \\end{bmatrix} \\times \\frac{1}{16}$|\n",
    "|**Sobel**垂直边缘检测|$\\begin{bmatrix}1 & 0 & - 1 \\\\ 2 & 0 & - 2 \\\\ 1 & 0 & - 1 \\\\\\end{bmatrix}$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = data.chelsea()\n",
    "cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = cat / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_1 = np.rollaxis(cat, 2, 0)\n",
    "cat_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_2 = np.swapaxes(cat, 2, 0)\n",
    "cat_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_grey = color.rgb2gray(cat)\n",
    "cat_grey.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cat_grey.T, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实现2D卷积**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.insert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.ones([3,3 ])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.ones([3,3 ])\n",
    "image = np.insert(image, 3, np.zeros(3), axis=0)\n",
    "image = np.insert(image, 0, np.zeros(3), axis=0)\n",
    "image = np.insert(image, 3, np.zeros(5), axis=1)\n",
    "image = np.insert(image, 0, np.zeros(5), axis=1)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多通道输入 单卷积核\n",
    "def conv2d(image, kernel, padding='SAME', strides=[1]):\n",
    "    # image  (height, width, in_channels)\n",
    "    # kernel (filter_height, f_width, in_channels, out_channels)\n",
    "    h, w, channels = image.shape\n",
    "    res = np.zeros_like(image)\n",
    "    kh, kw, in_c, out_c = kernel.shape\n",
    "#     kernel = kernel[..., np.newaxis]\n",
    "    image = np.rollaxis(image, 2, 0)  # 转为 c, h, w, 排布\n",
    "    if padding == 'SAME': \n",
    "        # 需要填充一圈0(步长为1时)\n",
    "        image = np.insert(image, h, np.zeros(w), axis=1)\n",
    "        image = np.insert(image, 0, np.zeros(w), axis=1)\n",
    "        image = np.insert(image, w, np.zeros(h + 2), axis=2)\n",
    "        image = np.insert(image, 0, np.zeros(h + 2), axis=2)\n",
    "#         new = np.zeros([h+2, w+2])\n",
    "#         new[1:-1, 1:-1] = image\n",
    "    if out_c == 1:  # 3 通道相加  输出到1个通道 \n",
    "        res = res[..., 0]\n",
    "        for i in range(h):  # 使用原来的h和w\n",
    "            for j in range(w):\n",
    "                temp = image[:, i:i+kh, j:j+kh]\n",
    "                res[i, j] = np.sum(temp * kernel)\n",
    "    else:  # 各通道 各自进行卷积\n",
    "        for c in range(channels):\n",
    "            for i in range(h):  # 使用原来的h和w\n",
    "                for j in range(w):\n",
    "                    temp = image[c, i:i+kh, j:j+kh]\n",
    "                    res[i, j, c] = np.sum(temp * kernel)\n",
    "        \n",
    "    res = (res - res.min())/(res.max() - res.min()) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = np.ones((3,3 )) * 1/ 9  # 方块模糊\n",
    "\n",
    "# kernel = np.zeros([3, 3])\n",
    "# kernel[1, 1] = 1  # 原图\n",
    "# kernel = np.array([[1, 0, -1], [0, 0, 0], [-1, 0, 1.]])  # 突出边缘差异\n",
    "kernel = np.ones((3,3 )) * -1\n",
    "kernel[1, 1] = 8  # 突出中间值\n",
    "kernel = np.tile(kernel[..., np.newaxis, np.newaxis], [1, 1, 3, 3])\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2 = conv2d(cat, kernel)\n",
    "cat2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tensorflow实现2d卷积**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [batch, in_height, in_width, in_channels]\n",
    "cat1 = tf.nn.conv2d(cat[np.newaxis, ...], kernel, strides=[1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = (cat1 - tf.reduce_min(cat1)) / (tf.reduce_max(cat1) - tf.reduce_min(cat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(cat1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 垂直边缘检测\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert = np.zeros((100, 100))\n",
    "vert[:, ::40] = 10\n",
    "vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vert, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 垂直边缘检测\n",
    "kernel_vert = np.array([[1, 0, -1]] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_vert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.nn.conv2d(vert[np.newaxis, ..., np.newaxis], kernel_vert[..., np.newaxis, np.newaxis], padding='VALID', strides=1)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(res), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert1 = np.ones((100, 100)) * 10\n",
    "vert1[:, ::40] = 0\n",
    "vert1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vert1, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.nn.conv2d(vert1[np.newaxis, ..., np.newaxis], kernel_vert[..., np.newaxis, np.newaxis], padding='VALID', strides=1)\n",
    "plt.imshow(np.squeeze(res), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 水平边缘检测\n",
    "kernel_hor = kernel_vert.T\n",
    "kernel_hor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于卷积, 参数共享的特殊形式使得神经网络层具有对平移**等变**(equivariance)的性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步长(Stride)\n",
    "步长是指感受域窗口每次移动的长度单位，对于2D输入来说，分为沿𝑥(向右)方向和𝑦(向下)方向的移动长度.\n",
    "\n",
    "可以看到，通过设定步长𝑠，可以有效地控制信息密度的提取。当步长设计的较小时，感受域以较小幅度移动窗口，有利于提取到更多的特征信息，输出张量的尺寸也更大；当步长设计的较大时，感受域以较大幅度移动窗口，有利于减少计算代价，过滤冗余信息，输出张量的尺寸也更小."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充(Padding)\n",
    "经过卷积运算后的输出𝑶的高宽一般会小于输入𝑿的高宽，即使是步长s = 1时，输出𝑶的高宽也会略小于输入𝑿高宽. \n",
    "\n",
    "为了让输出𝑶的高宽能够与输入𝑿的相等，一般通过在原输入𝑿的高和宽维度上面进行填充(Padding)若干无效元素操作，得到增大的输入𝑿′"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经层地输出尺寸$[b, h', w', c_{out}]$由卷积核的数量$c_{out}$卷积核的大小𝑘，步长𝑠，填充数𝑝(只考虑上下填充数量$𝑝_ℎ$相同，左右填充数量$𝑝_𝑤$相同的情况)以及输入𝑿的高宽ℎ/𝑤共同决定:\n",
    "$$h' = ⌊\\frac {h + 2\\cdot p_h - k}{s}⌋ + 1 \\\\\n",
    "w' = ⌊\\frac {w + 2 \\cdot p_w -k}{s}⌋ + 1 \n",
    "$$\n",
    "$⌊⌋$表示向下取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 5, 5, 3])  # 3通道 宽高为5 \n",
    "w = tf.random.normal([3, 3, 3, 4])  # [k, k, cin, cout]  3通道输入  4个3x3的卷积核\n",
    "out = tf.nn.conv2d(x, w, strides=1, padding=[[0,0],[0,0],[0,0],[0,0]])  # 不填充\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中padding 参数的设置格式为：\n",
    "\n",
    "padding=[[0,0],[上,下],[左,右],[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.nn.conv2d(x, w, strides=1, padding=[[0,0],[1,1],[1,1],[0,0]])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.nn.conv2d(x, w, strides=1, padding='SAME')  # 步长为1  SAME 模式下 输出与输入同大小\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.nn.conv2d(x, w, strides=1, padding='VALID')\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积层**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential, datasets, losses, optimizers, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Conv2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下创建了4 个3 × 3大小的卷积核的卷积层，步长为1，padding 方案为'SAME'\n",
    "layer = layers.Conv2D(4, kernel_size=3, padding='SAME', strides=1)\n",
    "out = layer(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h=3, w=3, c_in=3 c_out=4\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4个3X4大小卷积核, 竖直方向步长2 水平方向步长1\n",
    "layer = layers.Conv2D(4, kernel_size=(3, 4), padding='SAME', strides=(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5\n",
    "LeNet-5是由$LeCun$ 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络. 一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.mnist.load_data()\n",
    "(X_train, y_train), (X_dev, y_dev) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[10])\n",
    "plt.imshow(X_train[10], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X, y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.\n",
    "    X = tf.expand_dims(X, axis=3)\n",
    "    # X = tf.reshape(X, (-1, 28*28))\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    y_onehot = tf.one_hot(y, depth=10)\n",
    "    return X, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_dev, y_dev))\n",
    "train_db = train_db.shuffle(100000).batch(256).map(preprocessing)\n",
    "test_db = test_db.shuffle(100000).batch(256).map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = next(iter(train_db))\n",
    "X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential([\n",
    "    layers.Conv2D(6, kernel_size=3, strides=1),  # 6个 3X3的 卷积核 6 * 9 + 6 = 60个参数  \n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),  # 高宽各减半\n",
    "    layers.ReLU(),\n",
    "    layers.Conv2D(16, kernel_size=3, strides=1), \n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),  # 高宽各减半\n",
    "    layers.ReLU(),\n",
    "    # layers.Conv2D(120, kernel_size=5, strides=1, activation='relu'),\n",
    "    layers.Flatten(),  # 展平 方便全连接层处理\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "network.build(input_shape=(None, 28, 28, 1))  # 28X28 1通道\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16 * 5 * 5 * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵\n",
    "criten = losses.CategoricalCrossentropy(from_logits=True) \n",
    "optimizer = optimizers.Adam(0.001)\n",
    "metric_acc = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.apply_gradients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss=losses.CategoricalCrossentropy(from_logits=True),\n",
    "               optimizer=optimizers.Adam(0.001),\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = network.fit(train_db, epochs=20, validation_data=test_db, validation_freq=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵\n",
    "criten = losses.CategoricalCrossentropy(from_logits=True) \n",
    "optimizer = optimizers.Adam(0.001)\n",
    "metric_acc = metrics.Accuracy()\n",
    "loss = []\n",
    "\n",
    "max_epoch = 20\n",
    "metric_acc.reset_states()\n",
    "st = time.process_time()\n",
    "for epoch in range(max_epoch):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.expand_dims(x, axis=3)  # [b, 28, 28, 1]\n",
    "            out = network(x)\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            cost = criten(y_onehot, out)\n",
    "        grads = tape.gradient(cost, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "\n",
    "#     if step % 100 == 0:\n",
    "    total_cost = 0\n",
    "    count = 0\n",
    "    for x, y in train_db:\n",
    "        x = tf.expand_dims(x, axis=3)\n",
    "        out = network(x)\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        count += 1\n",
    "        total_cost += criten(y_onehot, out)\n",
    "    total_cost = total_cost / count\n",
    "    loss.append(total_cost)\n",
    "    print(f'Epoch:{epoch} cost:{float(total_cost)}', )\n",
    "\n",
    "    \n",
    "for x, y in test_db:\n",
    "    x = tf.expand_dims(x, axis=3)\n",
    "    out = network(x)\n",
    "    # 可以不进过softmax\n",
    "    y_pred = tf.argmax(out, axis=-1)\n",
    "    metric_acc.update_state(y, y_pred)\n",
    "print(f\"dev 正确率:{float(metric_acc.result())}\")\n",
    "\n",
    "\n",
    "metric_acc.reset_states()\n",
    "\n",
    "for x, y in train_db:\n",
    "    x = tf.expand_dims(x, axis=3)\n",
    "    out = network(x)\n",
    "    # 可以不进过softmax\n",
    "    y_pred = tf.argmax(out, axis=-1)\n",
    "    metric_acc.update_state(y, y_pred)\n",
    "print(f\"train 正确率:{float(metric_acc.result())}\")\n",
    "\n",
    "\n",
    "end = time.process_time()\n",
    "print('time: ', end - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.xticks(range(max_epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.xticks(range(max_epoch))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('dev acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化(Pooling)\n",
    "在卷积层中, 除了通过设置步长(Stride), 还有一种专门的网络层可以实现尺寸缩减功能-池化.\n",
    "\n",
    "池化层同样基于局部相关性的思想，通过从局部相关的一组元素中进行采样或信息聚合，从而得到新的元素值。特别地，**最大池化**层(Max Pooling)从局部相关元素集中选取最大的一个元素值，**平均池化**层(Average Pooling)从局部相关元素集中计算平均值并返回。还有其他的池化函数如$L^2$范数以及基于距中心像素距离的加权平均函数.\n",
    "\n",
    "不管采用什么样的池化函数, 当输入做出少量平移时, 池化能够帮助输入的表示近似**不变**(invariant).局部平移不变性是一个很有用的性质, 尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时.\n",
    "\n",
    "由于池化层没有需要学习的参数，计算简单，并且可以有效减低特征图的尺寸，非常适合图片这种类型的数据.\n",
    "\n",
    "通过精心设计池化层感受野的高宽𝑘和步长𝑠参数，可以实现各种降维运算。比如，一种常用的池化层设定是感受域大小𝑘 = 2，步长𝑠 = 2，这样可以实现输出只有输入高宽一半的目的.\n",
    "\n",
    "**卷积层与池化层地区别**\n",
    "\n",
    "|            |                 卷积层                 |              池化层              |\n",
    "| :--------: | :------------------------------------: | :------------------------------: |\n",
    "|  **结构**  |   零填充时输出维度不变，而通道数改变   |  通常特征维度会降低，通道数不变  |\n",
    "| **稳定性** | 输入特征发生细微改变时，输出结果会改变 | 感受域内的细微变化不影响输出结果 |\n",
    "|  **作用**  |        感受域内提取局部关联特征        |  感受域内提取泛化特征，降低维度  |\n",
    "| **参数量** |      与卷积核尺寸、卷积核个数相关      |          不引入额外参数          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\t\t\t\t\t\t\t网络参数配置\n",
    "\n",
    "|         网络层         |       输入尺寸        |         核尺寸          |        输出尺寸        |            参数个数             |\n",
    "| :--------------------: | :-------------------: | :---------------------: | :--------------------: | :-----------------------------: |\n",
    "| 卷积层$C^{[l]}$ | $ n^{[l-1]}_{h} \\times  n^{[l-1]}_{w} \\times n_c^{[l-1]}$ | $f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]}/1, n_c^{[l]} $ | $ n^{[l]}_{h} \\times  n^{[l]}_{w} \\times n_c^{[l]}$ | $(f^{[l]} \\times f^{[l]} \\times n_c^{[l-1]} + 1) \\times n_c^{[l]}$ | \n",
    "|池化层$S$| $ n^{[l]}_{h} \\times  n^{[l]}_{w} \\times n_c^{[l]}$ | $2 \\times 2 / 2$| $ \\frac {n^{[l]}_{h}} {2} \\times \\frac {n^{[l]}_{w}} {2} \\times n_c^{[l]}$ | $0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批归一化与BatchNorm层\n",
    "\n",
    "以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过 $ \\sigma(WX+b) $ 这样的矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。 \n",
    "\n",
    "这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization（BN）。\n",
    "\n",
    "\n",
    "**批归一化（BN）算法流程**\n",
    "\n",
    "输入：上一层输出结果 $ X = {x_1, x_2, ..., x_m} $，学习参数 $ \\gamma, \\beta $\n",
    "\n",
    "算法流程：\n",
    "\n",
    "1. 计算上一层输出数据的均值\n",
    "\n",
    "$$\n",
    "\\mu_{\\beta} = \\frac{1}{m} \\sum_{i=1}^m(x_i)\n",
    "$$\n",
    "\n",
    "其中，$ m $ 是此次训练样本 batch 的大小。\n",
    "\n",
    "2. 计算上一层输出数据的标准差\n",
    "\n",
    "$$\n",
    "\\sigma_{\\beta}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\beta})^2\n",
    "$$\n",
    "\n",
    "3. 在训练阶段, 通过归一化处理，得到\n",
    "\n",
    "$$\n",
    "\\hat x_{train} = \\frac{x_{train} - \\mu_{\\beta}}{\\sqrt{\\sigma_{\\beta}^2} + \\epsilon}\n",
    "$$\n",
    "在测试阶段，根据记录的每个Batch的$\\mu_B, \\sigma_B^2$估计出所有训练数据的$\\mu_r, \\sigma_r^2$(平均法或者指数加权平均法), \n",
    "$$\n",
    "\\mu_r \\leftarrow momentum \\cdot \\mu_r + (1-momentum)\\cdot \\mu_B  \\\\\n",
    "\\sigma_r^2 \\leftarrow momentum \\cdot \\sigma_r^2 + (1-momentum)\\cdot \\sigma_B\n",
    "$$\n",
    "\n",
    "按着\n",
    "$$\n",
    "\\hat x_{test} = \\frac {x_{test} - \\mu_r}{\\sqrt{\\sigma_{\\beta}^2}+ \\epsilon}\n",
    "$$\n",
    "将每层输入标准化\n",
    "\n",
    "其中 $ \\epsilon $ 是为了避免分母为 0 而加进去的接近于 0 的很小值\n",
    "\n",
    "4. 重构，对经过上面归一化处理得到的数据进行重构，得到\n",
    "\n",
    "$$\n",
    "\\tilde x = \\gamma \\hat x_i + \\beta\n",
    "$$\n",
    "\n",
    "其中，$ \\gamma, \\beta $ 为可学习参数(每层的偏置bias参数$b$不再需要, 其功能由$\\beta$代替)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向传播更新参数**\n",
    "\n",
    "在训练模式下的反向更新阶段, 根据损失L求解梯度$\\frac {\\partial L}{\\partial \\gamma}和 \\frac {\\partial L}{\\partial \\beta}$\n",
    "对于2D图片输入$X: [b, h, w, c]$, BN层不是计算每个点的均值和方差, 而是在通道轴c上统计每个通道上所有数据的均值和方差."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization使用的时机\n",
    "\n",
    "在CNN中，BN应作用在非线性映射前。在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。\n",
    "\n",
    "BN比较适用的场景是：每个mini-batch比较大，数据分布比较接近。在进行训练之前，要做好充分的shuffle，否则效果会差很多。另外，由于BN需要在运行过程中统计每个mini-batch的一阶统计量和二阶统计量，因此不适用于动态的网络结构和RNN网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例\n",
    "# 构造输入\n",
    "x = tf.random.normal([100, 32, 32, 3])\n",
    "# 将其他维度合并, 仅保留通道维度\n",
    "x = tf.reshape(x, (-1, 3))\n",
    "# 计算其他维度的均值\n",
    "ub = tf.reduce_mean(x, axis=0)\n",
    "ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BN 层  通过设置training 标志位来区分训练模式还是测试模式\n",
    "layers.BatchNormalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入BN层\n",
    "network_bn = Sequential([\n",
    "    layers.Conv2D(6, kernel_size=3, strides=1),  # 6个 3X3的 卷积核 6 * 9 + 6 = 60个参数  \n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),  # 高宽各减半\n",
    "    layers.BatchNormalization(),  # mu, sigma, beta gamma  *6  = 24个参数\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Conv2D(16, kernel_size=3, strides=1), \n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),  # 高宽各减半\n",
    "    layers.BatchNormalization(),\n",
    "    layers.ReLU(),\n",
    "    \n",
    "    layers.Flatten(),  # 展平 方便全连接层处理\n",
    "    \n",
    "    layers.Dense(120, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "network_bn.build(input_shape=(None, 28, 28, 1))  # 28X28 1通道\n",
    "network_bn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵\n",
    "criten = losses.CategoricalCrossentropy(from_logits=True) \n",
    "optimizer = optimizers.Adam(0.001)\n",
    "metric_acc = metrics.Accuracy()\n",
    "loss = []\n",
    "acc = []\n",
    "max_epoch = 20\n",
    "metric_acc.reset_states()\n",
    "st = time.process_time()\n",
    "for epoch in range(max_epoch):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = tf.expand_dims(x, axis=3)  # [b, 28, 28, 1]\n",
    "            out = network_bn(x, training=True)  # 训练模型\n",
    "            y_onehot = tf.one_hot(y, depth=10)\n",
    "            cost = criten(y_onehot, out)\n",
    "        grads = tape.gradient(cost, network_bn.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, network_bn.trainable_variables))\n",
    "\n",
    "    total_cost = 0\n",
    "    count = 0\n",
    "    for x, y in train_db:\n",
    "        x = tf.expand_dims(x, axis=3)\n",
    "        out = network_bn(x, training=False)\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        count += 1\n",
    "        total_cost += criten(y_onehot, out)\n",
    "    total_cost = total_cost / count\n",
    "    loss.append(total_cost)\n",
    "    print(f'Epoch:{epoch} cost:{float(total_cost)}', )\n",
    "\n",
    "    for x, y in test_db:\n",
    "        x = tf.expand_dims(x, axis=3)\n",
    "        out = network_bn(x, training=False)  # 测试模型\n",
    "        # 可以不进过softmax\n",
    "        y_pred = tf.argmax(out, axis=-1)\n",
    "        metric_acc.update_state(y, y_pred)\n",
    "    print(f\"dev 正确率:{float(metric_acc.result())}\")\n",
    "    acc.append(float(metric_acc.result()))\n",
    "end = time.process_time()\n",
    "print('time: ', end-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.xticks(range(max_epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('dev acc')\n",
    "plt.xticks(range(max_epoch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $1\\times1$卷积作用\n",
    "\n",
    "对于输入shape 为$[b, h, w, c_{in}]$，$1 \\times 1$卷积层的输出为$[b, h, w, c_{out}]$，其中$c_{in}$为输入数据的通道数，$c_{out}$为输出数据的通道\n",
    "数，也是$1\\times 1$卷积核的数量。 $1 \\times 1$卷积核的一个特别之处在于，它可以不改变特征图的宽高，而只对通道数𝑐进行变换.\n",
    "\n",
    "- 实现信息的跨通道交互和整合。\n",
    "- 对卷积核通道数进行降维和升维，减小参数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
