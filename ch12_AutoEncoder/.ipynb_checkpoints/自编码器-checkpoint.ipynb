{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœºå™¨å­¦ä¹ çš„ä¸¤ç§åŸºæœ¬èŒƒå¼: ç›‘ç£å­¦ä¹ (Supervised Learning)å’Œæ— ç›‘ç£å­¦ä¹ (Unsupervised Learning).\n",
    "\n",
    "ä¸¤è€…æœ€ä¸»è¦çš„åŒºåˆ«æ˜¯åœ¨äºæ¨¡å‹åœ¨è®­ç»ƒæ—¶æ˜¯å¦éœ€è¦**äººå·¥æ ‡æ³¨**çš„**æ ‡ç­¾ä¿¡æ¯**ã€‚\n",
    "\n",
    "è‡ªç›‘ç£å­¦ä¹ (Self-Supervised Learning): ç®—æ³•æŠŠæ•°æ®**$x$æœ¬èº«**ä½œä¸ºç›‘ç£ä¿¡å·æ¥å­¦ä¹ . åˆ©ç”¨è¾…åŠ©ä»»åŠ¡ï¼ˆpretextï¼‰ä»å¤§è§„æ¨¡çš„æ— ç›‘ç£æ•°æ®ä¸­æŒ–æ˜è‡ªèº«çš„ç›‘ç£ä¿¡æ¯ï¼Œé€šè¿‡è¿™ç§æ„é€ çš„ç›‘ç£ä¿¡æ¯å¯¹ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œä»è€Œå¯ä»¥å­¦ä¹ åˆ°å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ä»·å€¼çš„è¡¨å¾.\n",
    "\n",
    "\n",
    "[è‡ªç›‘ç£å­¦ä¹ ](https://blog.csdn.net/sdu_hao/article/details/104515917) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªç¼–ç å™¨åŸç†\n",
    "\n",
    "æœ‰ç›‘ç£å­¦ä¹ ä¸­ç¥ç»ç½‘ç»œçš„åŠŸèƒ½å¯ä»¥çœ‹åšæ˜¯ç‰¹ç§é™ç»´(Dimensionality Reduction)çš„è¿‡ç¨‹: $é«˜ç»´è¾“å…¥ç‰¹å¾x \\rightarrow ä½ç»´å˜é‡o$.\n",
    "\n",
    "è‡ªç›‘ç£å­¦ä¹ åˆ©ç”¨æ•°æ®$x$æœ¬èº«ä½œä¸ºç›‘ç£ä¿¡å·æ¥æŒ‡å¯¼ç½‘ç»œçš„è®­ç»ƒï¼Œå³å¸Œæœ›ç¥ç»ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ åˆ°æ˜ å°„$f_{\\theta}: x \\rightarrow \\bar x$. å°†ç½‘ç»œåˆ†æˆä¸¤éƒ¨åˆ†:\n",
    "- $g_{\\theta_1}:x \\rightarrow z$, Encoderç½‘ç»œ: è¾“å…¥$x$æ•°æ®ç¼–ç æˆä½ç»´éšå˜é‡(Latent Variable).\n",
    "- $h_{\\theta_2}:z \\rightarrow \\bar x$, Decoderç½‘ç»œ: ç¼–ç è¿‡åçš„è¾“å…¥$z$è§£ç ä¸ºé«˜ç»´åº¦çš„$\\bar x$\n",
    "æŠŠæ•´ä¸ªæ¨¡å‹$f_{\\theta}$ç§°ä¸ºè‡ªåŠ¨ç¼–ç å™¨(Auto-Encoder)\n",
    "\n",
    "![è‡ªç¼–ç å™¨æ¨¡å‹](è‡ªç¼–ç å™¨æ¨¡å‹.png)\n",
    "\n",
    "æˆ‘ä»¬å¸Œæœ›è§£ç å™¨çš„è¾“å‡ºèƒ½å¤Ÿå®Œç¾åœ°æˆ–è€…è¿‘ä¼¼é‡å»º(Reconstruct, æˆ–æ¢å¤)å‡ºåŸæ¥çš„è¾“å…¥, å³$\\bar x \\approx x$. ä¼˜åŒ–ç›®æ ‡å¯ä»¥å†™æˆ:\n",
    "$$\n",
    "Minimize L = dist(x, \\bar x) \\\\\n",
    "\\bar x = h_{\\theta_2}(g_{\\theta_1}(x))\n",
    "$$\n",
    "\n",
    "$dist$è·ç¦»åº¦é‡, å¸¸ç”¨æ¬§æ°è·ç¦»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32) / 255. \n",
    "X_test = X_test.astype(np.float32)/ 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åªéœ€è¦å›¾ç‰‡åŸå§‹æ•°æ® ä¸éœ€è¦æ ‡ç­¾y\n",
    "train_db = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 251.565 248.518125 \n",
       "L 251.565 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "L 26.925 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p6f0420f6aa)\">\n",
       "    <image height=\"218\" id=\"image8c510762f6\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mada214fdc9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mec82afbcc1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"11.082857\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"49.911429\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"88.74\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"127.568571\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"166.397143\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"205.225714\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 26.925 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 7.2 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6f0420f6aa\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in train_db.take(1):\n",
    "    print(x.shape)\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "train_db = train_db.shuffle(10000).batch(BATCH_SIZE)\n",
    "test_db = test_db.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # 784 -> 20\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(hid_dim)\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "    def call(self, inputs, training=None):\n",
    "        hidden = self.encoder(inputs)\n",
    "        x = self.decoder(hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      multiple                  236436    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  237200    \n",
      "=================================================================\n",
      "Total params: 473,636\n",
      "Trainable params: 473,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Computes sigmoid cross entropy given `logits`.\n",
      "\n",
      "Measures the probability error in discrete classification tasks in which each\n",
      "class is independent and not mutually exclusive.  For instance, one could\n",
      "perform multilabel classification where a picture can contain both an elephant\n",
      "and a dog at the same time.\n",
      "\n",
      "For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n",
      "\n",
      "      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
      "    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n",
      "    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n",
      "    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n",
      "    = (1 - z) * x + log(1 + exp(-x))\n",
      "    = x - x * z + log(1 + exp(-x))\n",
      "\n",
      "For x < 0, to avoid overflow in exp(-x), we reformulate the above\n",
      "\n",
      "      x - x * z + log(1 + exp(-x))\n",
      "    = log(exp(x)) - x * z + log(1 + exp(-x))\n",
      "    = - x * z + log(1 + exp(x))\n",
      "\n",
      "Hence, to ensure stability and avoid overflow, the implementation uses this\n",
      "equivalent formulation\n",
      "\n",
      "    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
      "\n",
      "`logits` and `labels` must have the same type and shape.\n",
      "\n",
      "Args:\n",
      "  labels: A `Tensor` of the same type and shape as `logits`.\n",
      "  logits: A `Tensor` of type `float32` or `float64`.\n",
      "  name: A name for the operation (optional).\n",
      "\n",
      "Returns:\n",
      "  A `Tensor` of the same shape as `logits` with the componentwise\n",
      "  logistic losses.\n",
      "\n",
      "Raises:\n",
      "  ValueError: If `logits` and `labels` do not have the same shape.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "tf.nn.sigmoid_cross_entropy_with_logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 10), dtype=float32, numpy=\n",
       "array([[ 0.9972955 ,  0.11835951,  0.37546238,  0.81246483,  0.6811694 ,\n",
       "        -0.4075523 ,  0.20997974,  0.56174517,  2.3335836 ,  1.7735332 ],\n",
       "       [ 2.441628  ,  1.0275384 , -1.8056978 ,  2.8142114 ,  0.3980029 ,\n",
       "         0.4609581 ,  1.3966328 ,  3.3343709 ,  0.24670157,  1.0903974 ],\n",
       "       [ 1.028021  ,  1.2927948 ,  0.6225021 , -1.0816448 ,  1.9404129 ,\n",
       "         1.3503171 , -0.80758274,  0.66555625,  0.6770658 ,  0.09982179],\n",
       "       [-0.47754583,  0.81485367,  0.5431217 ,  0.9108361 ,  0.22581324,\n",
       "         0.01963142,  1.80678   ,  0.10277647,  0.7110898 ,  0.5084593 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([4, 10])\n",
    "b = tf.random.normal([4, 10])\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(labels=a, logits=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_rec_logist = model(inp)\n",
    "        # æˆ–è€…ç›´æ¥ä½¿ç”¨MSEæŸå¤±\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=inp, logits=x_rec_logist)\n",
    "        loss =tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Creates an image memory from an object exporting the array interface\n",
      "(using the buffer protocol).\n",
      "\n",
      "If **obj** is not contiguous, then the tobytes method is called\n",
      "and :py:func:`~PIL.Image.frombuffer` is used.\n",
      "\n",
      "If you have an image in NumPy::\n",
      "\n",
      "  from PIL import Image\n",
      "  import numpy as np\n",
      "  im = Image.open('hopper.jpg')\n",
      "  a = np.asarray(im)\n",
      "\n",
      "Then this can be used to convert it to a Pillow image::\n",
      "\n",
      "  im = Image.fromarray(a)\n",
      "\n",
      ":param obj: Object with array interface\n",
      ":param mode: Mode to use (will be determined from type if None)\n",
      "  See: :ref:`concept-modes`.\n",
      ":returns: An image object.\n",
      "\n",
      ".. versionadded:: 1.1.6\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tf2/lib/python3.7/site-packages/PIL/Image.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "Image.fromarray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(images, name):\n",
    "    # 'L': 8-bit pixels, black and white\n",
    "    new_im  = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    # 10è¡Œ 10åˆ—  100å¼  = 50 + 50 \n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = images[index] \n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            # å°†å°å›¾ç‰‡å†™å…¥å¯¹åº”ä½ç½®  åˆ—æ–¹å‘æ’å¸ƒ\n",
    "            new_im.paste(im, (i, j))  # (xè½´, yè½´) ä¸€åˆ—\n",
    "            index += 1\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        loss = train_step(x)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {step}, Loss {float(loss)}\")\n",
    "    \n",
    "    # æ¯ä¸ªepoch è¿›è¡Œä¸€æ¬¡é‡å»º\n",
    "    x = next(iter(test_db))\n",
    "    logits = model(tf.reshape(x, [-1, 784]))\n",
    "    x_hat = tf.sigmoid(logits)  # å°†è¾“å‡ºè½¬æ¢ä¸º0è‡³1çš„åƒç´ å€¼ï¼Œä½¿ç”¨sigmoid å‡½æ•°\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28])  # æ¢å¤åŸæ¥å½¢çŠ¶\n",
    "    \n",
    "    # åŸå§‹å›¾ç‰‡ + é‡å»ºå›¾ç‰‡ å¯¹æ¯”\n",
    "    x_concat = tf.concat([x[:50], x_hat[:50]], axis=0)\n",
    "    x_concat = x_concat.numpy() * 255  # åƒç´ å€¼æ¢å¤\n",
    "    x_concat = x_concat.astype(np.uint8)\n",
    "    \n",
    "    save_image(x_concat, f\"ae_images/rec_epoch_{epoch}.png\")\n",
    "print('Time taken for {} epochs {} sec\\n'.format(EPOCHS, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('ae_images/rec_epoch_99.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªç¼–ç å™¨å˜ç§\n",
    "### Denising Auto-Encoder\n",
    "é˜²æ­¢ç¥ç»ç½‘ç»œè®°å¿†ä½è¾“å…¥æ•°æ®çš„åº•å±‚ç‰¹å¾, ç»™è¾“å…¥æ•°æ®æ·»åŠ éšæœºçš„å™ªå£°æ‰°åŠ¨:\n",
    "$$\n",
    "\\tilde x = x + \\epsilon, \\epsilon \\sim \\mathcal N(0, var)\n",
    "$$\n",
    "\n",
    "### Dropout Auto-Encoder\n",
    "åœ¨ç½‘ç»œå±‚ä¹‹é—´æ’å…¥Dropout å±‚å®ç°ç½‘ç»œè¿æ¥çš„éšå³æ–­å¼€, é˜²æ­¢è¿‡æ‹Ÿåˆ.\n",
    "\n",
    "### Adversarial Auto-Encoder\n",
    "å¯¹æŠ—è‡ªç¼–ç å™¨åˆ©ç”¨é¢å¤–çš„åˆ¤åˆ«å™¨ç½‘ç»œæ¥åˆ¤æ–­é™ç»´çš„éšè—å˜é‡$z$æ˜¯å¦é‡‡æ ·è‡ªå…ˆéªŒåˆ†å¸ƒ$P(z)$, æ–¹ä¾¿åˆ©ç”¨$P(z)$æ¥é‡å»ºè¾“å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Auto-Encoder\n",
    "\n",
    "è§†é¢‘æ•™å­¦: https://www.bilibili.com/video/BV15E411w7Pz\n",
    "\n",
    "åŸºæœ¬çš„è‡ªç¼–ç å™¨æ˜¯ä¸€ä¸ªåˆ¤åˆ«æ¨¡å‹, è€Œä¸æ˜¯ç”Ÿæˆæ¨¡å‹. \n",
    "\n",
    "å˜åˆ†è‡ªç¼–ç å™¨(VAE)å¯ä»¥å®ç°ç»™å®šéšè—å˜é‡çš„åˆ†å¸ƒ$P(z)$, é€šè¿‡å­¦ä¹ æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$P(x|z)$, å¯¹è”åˆæ¦‚ç‡åˆ†å¸ƒ$P(x, z) = P(x|z)P(z)$è¿›è¡Œé‡‡æ ·, ç”Ÿæˆä¸åŒçš„æ ·æœ¬.\n",
    "\n",
    "![VAE](VAE.png)\n",
    "\n",
    "å¯¹æ¯”è‡ªç¼–ç å™¨, VAEæ¨¡å‹å¯¹éšè—å˜é‡$z$çš„åˆ†å¸ƒæœ‰æ˜¾ç¤ºçš„çº¦æŸ, å¸Œæœ›å…¶ç¬¦åˆé¢„è®¾çš„å…ˆéªŒåˆ†å¸ƒ$P(z)$. å› æ­¤ï¼Œåœ¨æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸Šï¼Œé™¤äº†åŸæœ‰çš„é‡å»ºè¯¯å·®é¡¹å¤–ï¼Œè¿˜æ·»åŠ äº†éšå˜é‡ğ’›åˆ†å¸ƒçš„çº¦æŸé¡¹ã€‚\n",
    "\n",
    "æœ€å¤§åŒ–ç›®æ ‡  \n",
    "$$L(\\phi, \\theta) = -D_{KL}(q_{\\phi}(z|x)||p(z)) + E_{z \\sim q}[log p_{\\theta}(x|z)]$$\n",
    "\n",
    "ç”¨ç¼–ç å™¨ç½‘ç»œå‚æ•°åŒ–$q_{\\phi}(z|x)$å‡½æ•°, è§£ç å™¨ç½‘ç»œå‚æ•°åŒ–$p_{\\theta}(x|z)$.\n",
    "\n",
    "ç‰¹åˆ«åœ°, å½“$q_{\\phi}(z|x)$å’Œ$p(z)$éƒ½ä¸º**æ­£æ€åˆ†å¸ƒ**æ—¶, ç¬¬ä¸€é¡¹æ•£åº¦çš„è®¡ç®—å¯ä»¥ç®€åŒ–ä¸º:\n",
    "$$\n",
    "D_{KL}(q_{\\phi}(z|x)||p(z)) = log\\frac {\\sigma_2}{\\sigma_1} + \\frac {\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{x\\sigma_2^2} - \\frac 1 2\n",
    "$$\n",
    "\n",
    "æ›´ç‰¹åˆ«åœ°, å½“$p(z) \\sim \\mathcal N(0, 1)$æ—¶, å³$\\mu_2=0, \\sigma_2=1$\n",
    "$$\n",
    "D_{KL}(q_{\\phi}(z|x)||p(z)) = -log\\sigma_1 + \\frac {\\sigma_1^2 + \\mu_1^2}{2} - \\frac 1 2\n",
    "$$\n",
    "\n",
    "ä¾¿äºè®¡ç®—, ç¬¬äºŒé¡¹$E_{z \\sim q}[log p_{\\theta}(x|z)]$åŒæ ·å¯ä»¥åŸºäºè‡ªç¼–ç å™¨ä¸­çš„é‡å»ºè¯¯å·®å‡½æ•°å®ç°.\n",
    "\n",
    "VAEæ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡è½¬æ¢ä¸º:\n",
    "$$\n",
    "min D_{KL}(q_{\\phi}(z|x)||p(z)) \\\\\n",
    "max E_{z \\sim q}[log p_{\\theta}(x|z)]\n",
    "$$\n",
    "\n",
    "è¯¦ç»†æ¨å¯¼è¿‡ç¨‹: [KLæ•£åº¦æ¨å¯¼](https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### é‡å‚æ•°æŠ€å·§\n",
    "\n",
    "\n",
    "![](VAEé‡å‚æ•°.png)\n",
    "\n",
    "Reparameterization Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(h_dim)\n",
    "        self.fc3 = layers.Dense(h_dim)\n",
    "\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "   \n",
    "    def encoder(self, x):\n",
    "        # ç¼–ç å™¨\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        # å‡å€¼\n",
    "        mu = self.fc2(h)\n",
    "        # æ–¹å·®\n",
    "        log_var = self.fc3(h)\n",
    "\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        # è§£ç å™¨\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "    def reparamentize(self, mu, log_var):\n",
    "        # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        var = tf.exp(log_var * 0.5)\n",
    "        z = mu + var * eps\n",
    "        return z\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "\n",
    "        z = self.reparamentize(mu, log_var)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "\n",
    "        return out, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "vae_model = VAE(h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_model.build(input_shape=(4, 784))  # tf.random.normal(log_var.shape) éœ€è¦ç¡®å®šçš„shape\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_vae_step(model, inp):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_rec_logist, mu, log_var = model(inp)\n",
    "        # é‡å»ºæŸå¤±  [b, 784]\n",
    "        rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=inp, logits=x_rec_logist)\n",
    "        rec_loss = tf.reduce_sum(rec_loss) / inp.shape[0]\n",
    "        # éœ€è¦åŠ ä¸Šçº¦æŸéšå˜é‡z  (b, h_dim)\n",
    "        # log_var = log(sigma ** 2) = 2log(sigma)\n",
    "        kl = 0.5 * (tf.exp(log_var) + mu ** 2 -1 - log_var)\n",
    "        kl = tf.reduce_sum(kl) / inp.shape[0]\n",
    "        loss = rec_loss + kl * 1.0\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rec_loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, rec loss 545.8084716796875, kl 2.203584671020508\n",
      "Epoch 0, Batch 100, rec loss 287.5150146484375, kl 14.860334396362305\n",
      "Epoch 1, Batch 0, rec loss 280.6857604980469, kl 15.699186325073242\n",
      "Epoch 1, Batch 100, rec loss 257.4855041503906, kl 16.035247802734375\n",
      "Epoch 2, Batch 0, rec loss 254.42315673828125, kl 15.800569534301758\n",
      "Epoch 2, Batch 100, rec loss 247.7906036376953, kl 15.3215913772583\n",
      "Epoch 3, Batch 0, rec loss 243.2098388671875, kl 15.417115211486816\n",
      "Epoch 3, Batch 100, rec loss 251.05303955078125, kl 14.903411865234375\n",
      "Epoch 4, Batch 0, rec loss 245.0662841796875, kl 15.521085739135742\n",
      "Epoch 4, Batch 100, rec loss 250.90475463867188, kl 15.257770538330078\n",
      "Epoch 5, Batch 0, rec loss 244.802734375, kl 14.578432083129883\n",
      "Epoch 5, Batch 100, rec loss 238.14170837402344, kl 14.743280410766602\n",
      "Epoch 6, Batch 0, rec loss 242.73626708984375, kl 14.670919418334961\n",
      "Epoch 6, Batch 100, rec loss 241.05422973632812, kl 15.013476371765137\n",
      "Epoch 7, Batch 0, rec loss 235.83160400390625, kl 14.9539794921875\n",
      "Epoch 7, Batch 100, rec loss 242.90347290039062, kl 15.181007385253906\n",
      "Epoch 8, Batch 0, rec loss 232.73193359375, kl 14.33578872680664\n",
      "Epoch 8, Batch 100, rec loss 242.777099609375, kl 14.61133861541748\n",
      "Epoch 9, Batch 0, rec loss 241.61734008789062, kl 14.99118709564209\n",
      "Epoch 9, Batch 100, rec loss 240.18499755859375, kl 14.690179824829102\n",
      "Epoch 10, Batch 0, rec loss 238.6923828125, kl 15.339764595031738\n",
      "Epoch 10, Batch 100, rec loss 236.4783935546875, kl 14.33108901977539\n",
      "Epoch 11, Batch 0, rec loss 235.54266357421875, kl 14.578170776367188\n",
      "Epoch 11, Batch 100, rec loss 230.94107055664062, kl 15.086729049682617\n",
      "Epoch 12, Batch 0, rec loss 231.73512268066406, kl 14.854093551635742\n",
      "Epoch 12, Batch 100, rec loss 234.53924560546875, kl 14.802177429199219\n",
      "Epoch 13, Batch 0, rec loss 231.24566650390625, kl 14.666435241699219\n",
      "Epoch 13, Batch 100, rec loss 234.44589233398438, kl 15.101644515991211\n",
      "Epoch 14, Batch 0, rec loss 231.51658630371094, kl 15.067808151245117\n",
      "Epoch 14, Batch 100, rec loss 231.513427734375, kl 14.947068214416504\n",
      "Epoch 15, Batch 0, rec loss 229.2972412109375, kl 15.371150016784668\n",
      "Epoch 15, Batch 100, rec loss 236.37615966796875, kl 14.6906156539917\n",
      "Epoch 16, Batch 0, rec loss 230.3271026611328, kl 15.360008239746094\n",
      "Epoch 16, Batch 100, rec loss 231.8116455078125, kl 14.82005500793457\n",
      "Epoch 17, Batch 0, rec loss 230.79800415039062, kl 14.921131134033203\n",
      "Epoch 17, Batch 100, rec loss 232.12960815429688, kl 15.127669334411621\n",
      "Epoch 18, Batch 0, rec loss 236.00001525878906, kl 15.306396484375\n",
      "Epoch 18, Batch 100, rec loss 228.46058654785156, kl 14.46934986114502\n",
      "Epoch 19, Batch 0, rec loss 232.4032745361328, kl 15.2178955078125\n",
      "Epoch 19, Batch 100, rec loss 239.2562255859375, kl 15.517318725585938\n",
      "Epoch 20, Batch 0, rec loss 229.59474182128906, kl 14.57783317565918\n",
      "Epoch 20, Batch 100, rec loss 232.64109802246094, kl 15.095540046691895\n",
      "Epoch 21, Batch 0, rec loss 225.4127197265625, kl 15.090032577514648\n",
      "Epoch 21, Batch 100, rec loss 234.95449829101562, kl 15.308486938476562\n",
      "Epoch 22, Batch 0, rec loss 232.10528564453125, kl 15.392109870910645\n",
      "Epoch 22, Batch 100, rec loss 240.89561462402344, kl 15.227855682373047\n",
      "Epoch 23, Batch 0, rec loss 225.58819580078125, kl 15.145315170288086\n",
      "Epoch 23, Batch 100, rec loss 233.4103240966797, kl 14.857958793640137\n",
      "Epoch 24, Batch 0, rec loss 231.22694396972656, kl 15.627706527709961\n",
      "Epoch 24, Batch 100, rec loss 226.5390625, kl 14.941547393798828\n",
      "Epoch 25, Batch 0, rec loss 234.28713989257812, kl 14.48676872253418\n",
      "Epoch 25, Batch 100, rec loss 230.67922973632812, kl 15.388776779174805\n",
      "Epoch 26, Batch 0, rec loss 225.62765502929688, kl 15.388120651245117\n",
      "Epoch 26, Batch 100, rec loss 233.58157348632812, kl 14.937129974365234\n",
      "Epoch 27, Batch 0, rec loss 227.02151489257812, kl 15.013650894165039\n",
      "Epoch 27, Batch 100, rec loss 235.7631378173828, kl 15.17831039428711\n",
      "Epoch 28, Batch 0, rec loss 228.6434783935547, kl 15.353065490722656\n",
      "Epoch 28, Batch 100, rec loss 224.79225158691406, kl 15.554228782653809\n",
      "Epoch 29, Batch 0, rec loss 230.55039978027344, kl 15.382116317749023\n",
      "Epoch 29, Batch 100, rec loss 240.554931640625, kl 15.526281356811523\n",
      "Epoch 30, Batch 0, rec loss 231.55918884277344, kl 15.378150939941406\n",
      "Epoch 30, Batch 100, rec loss 229.79391479492188, kl 15.68019962310791\n",
      "Epoch 31, Batch 0, rec loss 225.93603515625, kl 15.16878890991211\n",
      "Epoch 31, Batch 100, rec loss 227.30764770507812, kl 15.440010070800781\n",
      "Epoch 32, Batch 0, rec loss 227.70468139648438, kl 15.71022891998291\n",
      "Epoch 32, Batch 100, rec loss 234.1063995361328, kl 15.463868141174316\n",
      "Epoch 33, Batch 0, rec loss 224.54766845703125, kl 15.512845993041992\n",
      "Epoch 33, Batch 100, rec loss 230.48992919921875, kl 14.921018600463867\n",
      "Epoch 34, Batch 0, rec loss 232.66770935058594, kl 15.559028625488281\n",
      "Epoch 34, Batch 100, rec loss 235.0677947998047, kl 15.429471969604492\n",
      "Epoch 35, Batch 0, rec loss 225.8807373046875, kl 15.279857635498047\n",
      "Epoch 35, Batch 100, rec loss 234.6699981689453, kl 15.51052474975586\n",
      "Epoch 36, Batch 0, rec loss 227.02108764648438, kl 15.150484085083008\n",
      "Epoch 36, Batch 100, rec loss 230.90701293945312, kl 15.560379981994629\n",
      "Epoch 37, Batch 0, rec loss 235.49993896484375, kl 16.08946990966797\n",
      "Epoch 37, Batch 100, rec loss 231.72378540039062, kl 15.53588581085205\n",
      "Epoch 38, Batch 0, rec loss 233.46070861816406, kl 14.986763000488281\n",
      "Epoch 38, Batch 100, rec loss 228.11248779296875, kl 15.761370658874512\n",
      "Epoch 39, Batch 0, rec loss 230.36117553710938, kl 15.425479888916016\n",
      "Epoch 39, Batch 100, rec loss 224.49002075195312, kl 15.381847381591797\n",
      "Epoch 40, Batch 0, rec loss 227.35240173339844, kl 15.176485061645508\n",
      "Epoch 40, Batch 100, rec loss 231.44277954101562, kl 15.644342422485352\n",
      "Epoch 41, Batch 0, rec loss 225.13632202148438, kl 15.661201477050781\n",
      "Epoch 41, Batch 100, rec loss 225.9834747314453, kl 15.176229476928711\n",
      "Epoch 42, Batch 0, rec loss 228.49966430664062, kl 15.213052749633789\n",
      "Epoch 42, Batch 100, rec loss 233.27346801757812, kl 15.421608924865723\n",
      "Epoch 43, Batch 0, rec loss 226.33018493652344, kl 16.139751434326172\n",
      "Epoch 43, Batch 100, rec loss 228.33541870117188, kl 15.277801513671875\n",
      "Epoch 44, Batch 0, rec loss 230.35147094726562, kl 15.02495288848877\n",
      "Epoch 44, Batch 100, rec loss 229.46104431152344, kl 15.068397521972656\n",
      "Epoch 45, Batch 0, rec loss 230.86053466796875, kl 15.897178649902344\n",
      "Epoch 45, Batch 100, rec loss 231.94839477539062, kl 15.406871795654297\n",
      "Epoch 46, Batch 0, rec loss 227.1590576171875, kl 14.83413314819336\n",
      "Epoch 46, Batch 100, rec loss 230.73941040039062, kl 14.991497039794922\n",
      "Epoch 47, Batch 0, rec loss 231.65621948242188, kl 15.233951568603516\n",
      "Epoch 47, Batch 100, rec loss 228.9786376953125, kl 15.258265495300293\n",
      "Epoch 48, Batch 0, rec loss 228.08834838867188, kl 15.475151062011719\n",
      "Epoch 48, Batch 100, rec loss 232.95761108398438, kl 15.464564323425293\n",
      "Epoch 49, Batch 0, rec loss 226.04647827148438, kl 15.238323211669922\n",
      "Epoch 49, Batch 100, rec loss 226.18092346191406, kl 15.193704605102539\n",
      "Epoch 50, Batch 0, rec loss 227.76873779296875, kl 15.182446479797363\n",
      "Epoch 50, Batch 100, rec loss 228.667724609375, kl 15.249063491821289\n",
      "Epoch 51, Batch 0, rec loss 222.00631713867188, kl 15.449186325073242\n",
      "Epoch 51, Batch 100, rec loss 232.13674926757812, kl 15.084578514099121\n",
      "Epoch 52, Batch 0, rec loss 229.72975158691406, kl 15.095390319824219\n",
      "Epoch 52, Batch 100, rec loss 230.85411071777344, kl 15.518477439880371\n",
      "Epoch 53, Batch 0, rec loss 230.87864685058594, kl 15.381686210632324\n",
      "Epoch 53, Batch 100, rec loss 229.6859588623047, kl 14.987796783447266\n",
      "Epoch 54, Batch 0, rec loss 225.45205688476562, kl 15.766867637634277\n",
      "Epoch 54, Batch 100, rec loss 229.8700714111328, kl 15.148429870605469\n",
      "Epoch 55, Batch 0, rec loss 224.09805297851562, kl 14.889766693115234\n",
      "Epoch 55, Batch 100, rec loss 230.92123413085938, kl 15.433218002319336\n",
      "Epoch 56, Batch 0, rec loss 225.5115509033203, kl 15.499868392944336\n",
      "Epoch 56, Batch 100, rec loss 232.858642578125, kl 15.02985954284668\n",
      "Epoch 57, Batch 0, rec loss 222.02391052246094, kl 15.568073272705078\n",
      "Epoch 57, Batch 100, rec loss 229.61000061035156, kl 15.084177017211914\n",
      "Epoch 58, Batch 0, rec loss 230.29354858398438, kl 15.632808685302734\n",
      "Epoch 58, Batch 100, rec loss 228.5897216796875, kl 15.183363914489746\n",
      "Epoch 59, Batch 0, rec loss 223.95278930664062, kl 15.802167892456055\n",
      "Epoch 59, Batch 100, rec loss 230.31866455078125, kl 15.612465858459473\n",
      "Epoch 60, Batch 0, rec loss 227.49346923828125, kl 14.800804138183594\n",
      "Epoch 60, Batch 100, rec loss 227.30319213867188, kl 15.253202438354492\n",
      "Epoch 61, Batch 0, rec loss 227.73440551757812, kl 14.465019226074219\n",
      "Epoch 61, Batch 100, rec loss 229.09925842285156, kl 15.293550491333008\n",
      "Epoch 62, Batch 0, rec loss 225.1915740966797, kl 14.348112106323242\n",
      "Epoch 62, Batch 100, rec loss 230.17474365234375, kl 15.0762357711792\n",
      "Epoch 63, Batch 0, rec loss 226.66964721679688, kl 15.180123329162598\n",
      "Epoch 63, Batch 100, rec loss 226.8640899658203, kl 15.298295974731445\n",
      "Epoch 64, Batch 0, rec loss 230.1021728515625, kl 14.850513458251953\n",
      "Epoch 64, Batch 100, rec loss 231.46768188476562, kl 15.251212120056152\n",
      "Epoch 65, Batch 0, rec loss 225.81549072265625, kl 14.770442962646484\n",
      "Epoch 65, Batch 100, rec loss 237.4126739501953, kl 15.7590970993042\n",
      "Epoch 66, Batch 0, rec loss 229.22296142578125, kl 15.107858657836914\n",
      "Epoch 66, Batch 100, rec loss 231.5027313232422, kl 15.246879577636719\n",
      "Epoch 67, Batch 0, rec loss 225.59368896484375, kl 15.261476516723633\n",
      "Epoch 67, Batch 100, rec loss 222.77606201171875, kl 14.937665939331055\n",
      "Epoch 68, Batch 0, rec loss 221.02328491210938, kl 15.285591125488281\n",
      "Epoch 68, Batch 100, rec loss 228.6055908203125, kl 15.46548843383789\n",
      "Epoch 69, Batch 0, rec loss 222.13421630859375, kl 15.013542175292969\n",
      "Epoch 69, Batch 100, rec loss 223.2274169921875, kl 15.299140930175781\n",
      "Epoch 70, Batch 0, rec loss 232.21444702148438, kl 14.881745338439941\n",
      "Epoch 70, Batch 100, rec loss 232.98451232910156, kl 15.400595664978027\n",
      "Epoch 71, Batch 0, rec loss 225.11631774902344, kl 14.715810775756836\n",
      "Epoch 71, Batch 100, rec loss 226.23475646972656, kl 15.30920696258545\n",
      "Epoch 72, Batch 0, rec loss 226.83413696289062, kl 15.309128761291504\n",
      "Epoch 72, Batch 100, rec loss 229.3366241455078, kl 14.982002258300781\n",
      "Epoch 73, Batch 0, rec loss 225.48910522460938, kl 15.357404708862305\n",
      "Epoch 73, Batch 100, rec loss 239.95291137695312, kl 15.099433898925781\n",
      "Epoch 74, Batch 0, rec loss 227.33282470703125, kl 15.662109375\n",
      "Epoch 74, Batch 100, rec loss 230.0397186279297, kl 15.301599502563477\n",
      "Epoch 75, Batch 0, rec loss 227.69854736328125, kl 15.449613571166992\n",
      "Epoch 75, Batch 100, rec loss 224.00209045410156, kl 15.787334442138672\n",
      "Epoch 76, Batch 0, rec loss 227.46690368652344, kl 15.196432113647461\n",
      "Epoch 76, Batch 100, rec loss 229.42234802246094, kl 15.367589950561523\n",
      "Epoch 77, Batch 0, rec loss 225.40115356445312, kl 15.478286743164062\n",
      "Epoch 77, Batch 100, rec loss 225.11447143554688, kl 14.818584442138672\n",
      "Epoch 78, Batch 0, rec loss 222.4205322265625, kl 15.103157043457031\n",
      "Epoch 78, Batch 100, rec loss 229.46298217773438, kl 14.878928184509277\n",
      "Epoch 79, Batch 0, rec loss 228.4767608642578, kl 15.068242073059082\n",
      "Epoch 79, Batch 100, rec loss 220.953857421875, kl 15.299982070922852\n",
      "Epoch 80, Batch 0, rec loss 220.8783416748047, kl 15.18214225769043\n",
      "Epoch 80, Batch 100, rec loss 226.75372314453125, kl 15.185245513916016\n",
      "Epoch 81, Batch 0, rec loss 226.55633544921875, kl 15.621899604797363\n",
      "Epoch 81, Batch 100, rec loss 223.086669921875, kl 14.97620677947998\n",
      "Epoch 82, Batch 0, rec loss 221.3675537109375, kl 15.40985107421875\n",
      "Epoch 82, Batch 100, rec loss 225.61512756347656, kl 14.903972625732422\n",
      "Epoch 83, Batch 0, rec loss 227.45257568359375, kl 15.1503324508667\n",
      "Epoch 83, Batch 100, rec loss 231.57376098632812, kl 15.25521183013916\n",
      "Epoch 84, Batch 0, rec loss 229.15103149414062, kl 15.01236343383789\n",
      "Epoch 84, Batch 100, rec loss 231.40516662597656, kl 15.459772109985352\n",
      "Epoch 85, Batch 0, rec loss 219.69593811035156, kl 15.195003509521484\n",
      "Epoch 85, Batch 100, rec loss 226.84561157226562, kl 15.088056564331055\n",
      "Epoch 86, Batch 0, rec loss 223.06814575195312, kl 15.139235496520996\n",
      "Epoch 86, Batch 100, rec loss 226.91539001464844, kl 14.775728225708008\n",
      "Epoch 87, Batch 0, rec loss 224.31460571289062, kl 15.160497665405273\n",
      "Epoch 87, Batch 100, rec loss 227.45657348632812, kl 15.173416137695312\n",
      "Epoch 88, Batch 0, rec loss 224.70761108398438, kl 15.098197937011719\n",
      "Epoch 88, Batch 100, rec loss 230.7292022705078, kl 15.028358459472656\n",
      "Epoch 89, Batch 0, rec loss 227.62205505371094, kl 15.089055061340332\n",
      "Epoch 89, Batch 100, rec loss 228.1893768310547, kl 15.195079803466797\n",
      "Epoch 90, Batch 0, rec loss 228.87188720703125, kl 15.309420585632324\n",
      "Epoch 90, Batch 100, rec loss 228.43692016601562, kl 15.42381477355957\n",
      "Epoch 91, Batch 0, rec loss 223.28997802734375, kl 15.068074226379395\n",
      "Epoch 91, Batch 100, rec loss 225.77439880371094, kl 15.535183906555176\n",
      "Epoch 92, Batch 0, rec loss 228.4051513671875, kl 14.493616104125977\n",
      "Epoch 92, Batch 100, rec loss 227.37322998046875, kl 15.011366844177246\n",
      "Epoch 93, Batch 0, rec loss 223.8580780029297, kl 14.954328536987305\n",
      "Epoch 93, Batch 100, rec loss 225.4228057861328, kl 15.417442321777344\n",
      "Epoch 94, Batch 0, rec loss 227.15875244140625, kl 14.576358795166016\n",
      "Epoch 94, Batch 100, rec loss 220.50140380859375, kl 15.507772445678711\n",
      "Epoch 95, Batch 0, rec loss 227.5120849609375, kl 15.114062309265137\n",
      "Epoch 95, Batch 100, rec loss 230.3238525390625, kl 14.882749557495117\n",
      "Epoch 96, Batch 0, rec loss 223.38925170898438, kl 14.915565490722656\n",
      "Epoch 96, Batch 100, rec loss 223.8690185546875, kl 15.099380493164062\n",
      "Epoch 97, Batch 0, rec loss 227.8948974609375, kl 15.12774658203125\n",
      "Epoch 97, Batch 100, rec loss 229.72479248046875, kl 15.01730728149414\n",
      "Epoch 98, Batch 0, rec loss 224.07496643066406, kl 14.854434967041016\n",
      "Epoch 98, Batch 100, rec loss 219.50962829589844, kl 15.065048217773438\n",
      "Epoch 99, Batch 0, rec loss 227.3610076904297, kl 15.258623123168945\n",
      "Epoch 99, Batch 100, rec loss 228.41741943359375, kl 14.42936897277832\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        rec_loss, kl = train_vae_step(vae_model, x)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {step}, rec loss {float(rec_loss)}, kl {float(kl)}\")\n",
    "    # ç”Ÿæˆ\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        z = tf.random.normal([100, h_dim])\n",
    "        logits = vae_model.decoder(z)\n",
    "        x = tf.sigmoid(logits)  # 0è‡³1çš„åƒç´ å€¼\n",
    "        img = tf.reshape(x, [-1, 28, 28]).numpy() * 255 # 0-255\n",
    "        img = img.astype(np.uint8)\n",
    "        save_image(img, f'vae_images/gen_image_{epoch}.png')\n",
    "\n",
    "        inp = next(iter(test_db))[:100]\n",
    "        inp = tf.reshape(inp, [-1, 784])\n",
    "        out, _, _ = vae_model(inp)\n",
    "        out = tf.sigmoid(out)  # 0è‡³1çš„åƒç´ å€¼\n",
    "        img = tf.reshape(out, [-1, 28, 28]).numpy() * 255 # 0-255\n",
    "        img = img.astype(np.uint8)\n",
    "        save_image(img, f'vae_images/test_image_{epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2conda2a75a45106264ceab7472c43279a5d24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
