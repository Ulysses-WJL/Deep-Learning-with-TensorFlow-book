{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的两种基本范式: 监督学习(Supervised Learning)和无监督学习(Unsupervised Learning).\n",
    "\n",
    "两者最主要的区别是在于模型在训练时是否需要**人工标注**的**标签信息**。\n",
    "\n",
    "自监督学习(Self-Supervised Learning): 算法把数据**$x$本身**作为监督信号来学习. 利用辅助任务（pretext）从大规模的无监督数据中挖掘自身的监督信息，通过这种构造的监督信息对网络进行训练，从而可以学习到对下游任务有价值的表征.\n",
    "\n",
    "\n",
    "[自监督学习](https://blog.csdn.net/sdu_hao/article/details/104515917) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器原理\n",
    "\n",
    "有监督学习中神经网络的功能可以看做是特种降维(Dimensionality Reduction)的过程: $高维输入特征x \\rightarrow 低维变量o$.\n",
    "\n",
    "自监督学习利用数据$x$本身作为监督信号来指导网络的训练，即希望神经网络能够学习到映射$f_{\\theta}: x \\rightarrow \\bar x$. 将网络分成两部分:\n",
    "- $g_{\\theta_1}:x \\rightarrow z$, Encoder网络: 输入$x$数据编码成低维隐变量(Latent Variable).\n",
    "- $h_{\\theta_2}:z \\rightarrow \\bar x$, Decoder网络: 编码过后的输入$z$解码为高维度的$\\bar x$\n",
    "把整个模型$f_{\\theta}$称为自动编码器(Auto-Encoder)\n",
    "\n",
    "![自编码器模型](自编码器模型.png)\n",
    "\n",
    "我们希望解码器的输出能够完美地或者近似重建(Reconstruct, 或恢复)出原来的输入, 即$\\bar x \\approx x$. 优化目标可以写成:\n",
    "$$\n",
    "Minimize L = dist(x, \\bar x) \\\\\n",
    "\\bar x = h_{\\theta_2}(g_{\\theta_1}(x))\n",
    "$$\n",
    "\n",
    "$dist$距离度量, 常用欧氏距离."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32) / 255. \n",
    "X_test = X_test.astype(np.float32)/ 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只需要图片原始数据 不需要标签y\n",
    "train_db = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "test_db = tf.data.Dataset.from_tensor_slices((X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 251.565 248.518125 \n",
       "L 251.565 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "L 26.925 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g clip-path=\"url(#p6f0420f6aa)\">\n",
       "    <image height=\"218\" id=\"image8c510762f6\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\n",
       "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\" y=\"-6.64\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mada214fdc9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mada214fdc9\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"mec82afbcc1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"11.082857\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"49.911429\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"88.74\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"127.568571\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"166.397143\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mec82afbcc1\" y=\"205.225714\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 26.925 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 244.365 224.64 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 224.64 \n",
       "L 244.365 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 7.2 \n",
       "L 244.365 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6f0420f6aa\">\n",
       "   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in train_db.take(1):\n",
    "    print(x.shape)\n",
    "    plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "train_db = train_db.shuffle(10000).batch(BATCH_SIZE)\n",
    "test_db = test_db.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        # 784 -> 20\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(hid_dim)\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(784)\n",
    "        ])\n",
    "    def call(self, inputs, training=None):\n",
    "        hidden = self.encoder(inputs)\n",
    "        x = self.decoder(hidden)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      multiple                  236436    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    multiple                  237200    \n",
      "=================================================================\n",
      "Total params: 473,636\n",
      "Trainable params: 473,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Computes sigmoid cross entropy given `logits`.\n",
      "\n",
      "Measures the probability error in discrete classification tasks in which each\n",
      "class is independent and not mutually exclusive.  For instance, one could\n",
      "perform multilabel classification where a picture can contain both an elephant\n",
      "and a dog at the same time.\n",
      "\n",
      "For brevity, let `x = logits`, `z = labels`.  The logistic loss is\n",
      "\n",
      "      z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
      "    = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n",
      "    = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n",
      "    = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n",
      "    = (1 - z) * x + log(1 + exp(-x))\n",
      "    = x - x * z + log(1 + exp(-x))\n",
      "\n",
      "For x < 0, to avoid overflow in exp(-x), we reformulate the above\n",
      "\n",
      "      x - x * z + log(1 + exp(-x))\n",
      "    = log(exp(x)) - x * z + log(1 + exp(-x))\n",
      "    = - x * z + log(1 + exp(x))\n",
      "\n",
      "Hence, to ensure stability and avoid overflow, the implementation uses this\n",
      "equivalent formulation\n",
      "\n",
      "    max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
      "\n",
      "`logits` and `labels` must have the same type and shape.\n",
      "\n",
      "Args:\n",
      "  labels: A `Tensor` of the same type and shape as `logits`.\n",
      "  logits: A `Tensor` of type `float32` or `float64`.\n",
      "  name: A name for the operation (optional).\n",
      "\n",
      "Returns:\n",
      "  A `Tensor` of the same shape as `logits` with the componentwise\n",
      "  logistic losses.\n",
      "\n",
      "Raises:\n",
      "  ValueError: If `logits` and `labels` do not have the same shape.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "tf.nn.sigmoid_cross_entropy_with_logits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 10), dtype=float32, numpy=\n",
       "array([[ 0.9972955 ,  0.11835951,  0.37546238,  0.81246483,  0.6811694 ,\n",
       "        -0.4075523 ,  0.20997974,  0.56174517,  2.3335836 ,  1.7735332 ],\n",
       "       [ 2.441628  ,  1.0275384 , -1.8056978 ,  2.8142114 ,  0.3980029 ,\n",
       "         0.4609581 ,  1.3966328 ,  3.3343709 ,  0.24670157,  1.0903974 ],\n",
       "       [ 1.028021  ,  1.2927948 ,  0.6225021 , -1.0816448 ,  1.9404129 ,\n",
       "         1.3503171 , -0.80758274,  0.66555625,  0.6770658 ,  0.09982179],\n",
       "       [-0.47754583,  0.81485367,  0.5431217 ,  0.9108361 ,  0.22581324,\n",
       "         0.01963142,  1.80678   ,  0.10277647,  0.7110898 ,  0.5084593 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.normal([4, 10])\n",
    "b = tf.random.normal([4, 10])\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(labels=a, logits=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_rec_logist = model(inp)\n",
    "        # 或者直接使用MSE损失\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=inp, logits=x_rec_logist)\n",
    "        loss =tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Creates an image memory from an object exporting the array interface\n",
      "(using the buffer protocol).\n",
      "\n",
      "If **obj** is not contiguous, then the tobytes method is called\n",
      "and :py:func:`~PIL.Image.frombuffer` is used.\n",
      "\n",
      "If you have an image in NumPy::\n",
      "\n",
      "  from PIL import Image\n",
      "  import numpy as np\n",
      "  im = Image.open('hopper.jpg')\n",
      "  a = np.asarray(im)\n",
      "\n",
      "Then this can be used to convert it to a Pillow image::\n",
      "\n",
      "  im = Image.fromarray(a)\n",
      "\n",
      ":param obj: Object with array interface\n",
      ":param mode: Mode to use (will be determined from type if None)\n",
      "  See: :ref:`concept-modes`.\n",
      ":returns: An image object.\n",
      "\n",
      ".. versionadded:: 1.1.6\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tf2/lib/python3.7/site-packages/PIL/Image.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "Image.fromarray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(images, name):\n",
    "    # 'L': 8-bit pixels, black and white\n",
    "    new_im  = Image.new('L', (280, 280))\n",
    "    index = 0\n",
    "    # 10行 10列  100张 = 50 + 50 \n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = images[index] \n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            # 将小图片写入对应位置  列方向排布\n",
    "            new_im.paste(im, (i, j))  # (x轴, y轴) 一列\n",
    "            index += 1\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        loss = train_step(x)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {step}, Loss {float(loss)}\")\n",
    "    \n",
    "    # 每个epoch 进行一次重建\n",
    "    x = next(iter(test_db))\n",
    "    logits = model(tf.reshape(x, [-1, 784]))\n",
    "    x_hat = tf.sigmoid(logits)  # 将输出转换为0至1的像素值，使用sigmoid 函数\n",
    "    x_hat = tf.reshape(x_hat, [-1, 28, 28])  # 恢复原来形状\n",
    "    \n",
    "    # 原始图片 + 重建图片 对比\n",
    "    x_concat = tf.concat([x[:50], x_hat[:50]], axis=0)\n",
    "    x_concat = x_concat.numpy() * 255  # 像素值恢复\n",
    "    x_concat = x_concat.astype(np.uint8)\n",
    "    \n",
    "    save_image(x_concat, f\"ae_images/rec_epoch_{epoch}.png\")\n",
    "print('Time taken for {} epochs {} sec\\n'.format(EPOCHS, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('ae_images/rec_epoch_99.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自编码器变种\n",
    "### Denising Auto-Encoder\n",
    "防止神经网络记忆住输入数据的底层特征, 给输入数据添加随机的噪声扰动:\n",
    "$$\n",
    "\\tilde x = x + \\epsilon, \\epsilon \\sim \\mathcal N(0, var)\n",
    "$$\n",
    "\n",
    "### Dropout Auto-Encoder\n",
    "在网络层之间插入Dropout 层实现网络连接的随即断开, 防止过拟合.\n",
    "\n",
    "### Adversarial Auto-Encoder\n",
    "对抗自编码器利用额外的判别器网络来判断降维的隐藏变量$z$是否采样自先验分布$P(z)$, 方便利用$P(z)$来重建输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Auto-Encoder\n",
    "\n",
    "视频教学: https://www.bilibili.com/video/BV15E411w7Pz\n",
    "\n",
    "基本的自编码器是一个判别模型, 而不是生成模型. \n",
    "\n",
    "变分自编码器(VAE)可以实现给定隐藏变量的分布$P(z)$, 通过学习条件概率分布$P(x|z)$, 对联合概率分布$P(x, z) = P(x|z)P(z)$进行采样, 生成不同的样本.\n",
    "\n",
    "![VAE](VAE.png)\n",
    "\n",
    "对比自编码器, VAE模型对隐藏变量$z$的分布有显示的约束, 希望其符合预设的先验分布$P(z)$. 因此，在损失函数的设计上，除了原有的重建误差项外，还添加了隐变量𝒛分布的约束项。\n",
    "\n",
    "最大化目标  \n",
    "$$L(\\phi, \\theta) = -D_{KL}(q_{\\phi}(z|x)||p(z)) + E_{z \\sim q}[log p_{\\theta}(x|z)]$$\n",
    "\n",
    "用编码器网络参数化$q_{\\phi}(z|x)$函数, 解码器网络参数化$p_{\\theta}(x|z)$.\n",
    "\n",
    "特别地, 当$q_{\\phi}(z|x)$和$p(z)$都为**正态分布**时, 第一项散度的计算可以简化为:\n",
    "$$\n",
    "D_{KL}(q_{\\phi}(z|x)||p(z)) = log\\frac {\\sigma_2}{\\sigma_1} + \\frac {\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{x\\sigma_2^2} - \\frac 1 2\n",
    "$$\n",
    "\n",
    "更特别地, 当$p(z) \\sim \\mathcal N(0, 1)$时, 即$\\mu_2=0, \\sigma_2=1$\n",
    "$$\n",
    "D_{KL}(q_{\\phi}(z|x)||p(z)) = -log\\sigma_1 + \\frac {\\sigma_1^2 + \\mu_1^2}{2} - \\frac 1 2\n",
    "$$\n",
    "\n",
    "便于计算, 第二项$E_{z \\sim q}[log p_{\\theta}(x|z)]$同样可以基于自编码器中的重建误差函数实现.\n",
    "\n",
    "VAE模型的优化目标转换为:\n",
    "$$\n",
    "min D_{KL}(q_{\\phi}(z|x)||p(z)) \\\\\n",
    "max E_{z \\sim q}[log p_{\\theta}(x|z)]\n",
    "$$\n",
    "\n",
    "详细推导过程: [KL散度推导](https://hsinjhao.github.io/2019/05/22/KL-DivergenceIntroduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重参数技巧\n",
    "\n",
    "\n",
    "![](VAE重参数.png)\n",
    "\n",
    "Reparameterization Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, h_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(128)\n",
    "        self.fc2 = layers.Dense(h_dim)\n",
    "        self.fc3 = layers.Dense(h_dim)\n",
    "\n",
    "        self.fc4 = layers.Dense(128)\n",
    "        self.fc5 = layers.Dense(784)\n",
    "   \n",
    "    def encoder(self, x):\n",
    "        # 编码器\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        # 均值\n",
    "        mu = self.fc2(h)\n",
    "        # 方差\n",
    "        log_var = self.fc3(h)\n",
    "\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        # 解码器\n",
    "        out = tf.nn.relu(self.fc4(z))\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "    def reparamentize(self, mu, log_var):\n",
    "        # 从标准正态分布采样\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        var = tf.exp(log_var * 0.5)\n",
    "        z = mu + var * eps\n",
    "        return z\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "\n",
    "        z = self.reparamentize(mu, log_var)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "\n",
    "        return out, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 10\n",
    "vae_model = VAE(h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_model.build(input_shape=(4, 784))  # tf.random.normal(log_var.shape) 需要确定的shape\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_vae_step(model, inp):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_rec_logist, mu, log_var = model(inp)\n",
    "        # 重建损失  [b, 784]\n",
    "        rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=inp, logits=x_rec_logist)\n",
    "        rec_loss = tf.reduce_sum(rec_loss) / inp.shape[0]\n",
    "        # 需要加上约束隐变量z  (b, h_dim)\n",
    "        # log_var = log(sigma ** 2) = 2log(sigma)\n",
    "        kl = 0.5 * (tf.exp(log_var) + mu ** 2 -1 - log_var)\n",
    "        kl = tf.reduce_sum(kl) / inp.shape[0]\n",
    "        loss = rec_loss + kl * 1.0\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return rec_loss, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, rec loss 545.8084716796875, kl 2.203584671020508\n",
      "Epoch 0, Batch 100, rec loss 287.5150146484375, kl 14.860334396362305\n",
      "Epoch 1, Batch 0, rec loss 280.6857604980469, kl 15.699186325073242\n",
      "Epoch 1, Batch 100, rec loss 257.4855041503906, kl 16.035247802734375\n",
      "Epoch 2, Batch 0, rec loss 254.42315673828125, kl 15.800569534301758\n",
      "Epoch 2, Batch 100, rec loss 247.7906036376953, kl 15.3215913772583\n",
      "Epoch 3, Batch 0, rec loss 243.2098388671875, kl 15.417115211486816\n",
      "Epoch 3, Batch 100, rec loss 251.05303955078125, kl 14.903411865234375\n",
      "Epoch 4, Batch 0, rec loss 245.0662841796875, kl 15.521085739135742\n",
      "Epoch 4, Batch 100, rec loss 250.90475463867188, kl 15.257770538330078\n",
      "Epoch 5, Batch 0, rec loss 244.802734375, kl 14.578432083129883\n",
      "Epoch 5, Batch 100, rec loss 238.14170837402344, kl 14.743280410766602\n",
      "Epoch 6, Batch 0, rec loss 242.73626708984375, kl 14.670919418334961\n",
      "Epoch 6, Batch 100, rec loss 241.05422973632812, kl 15.013476371765137\n",
      "Epoch 7, Batch 0, rec loss 235.83160400390625, kl 14.9539794921875\n",
      "Epoch 7, Batch 100, rec loss 242.90347290039062, kl 15.181007385253906\n",
      "Epoch 8, Batch 0, rec loss 232.73193359375, kl 14.33578872680664\n",
      "Epoch 8, Batch 100, rec loss 242.777099609375, kl 14.61133861541748\n",
      "Epoch 9, Batch 0, rec loss 241.61734008789062, kl 14.99118709564209\n",
      "Epoch 9, Batch 100, rec loss 240.18499755859375, kl 14.690179824829102\n",
      "Epoch 10, Batch 0, rec loss 238.6923828125, kl 15.339764595031738\n",
      "Epoch 10, Batch 100, rec loss 236.4783935546875, kl 14.33108901977539\n",
      "Epoch 11, Batch 0, rec loss 235.54266357421875, kl 14.578170776367188\n",
      "Epoch 11, Batch 100, rec loss 230.94107055664062, kl 15.086729049682617\n",
      "Epoch 12, Batch 0, rec loss 231.73512268066406, kl 14.854093551635742\n",
      "Epoch 12, Batch 100, rec loss 234.53924560546875, kl 14.802177429199219\n",
      "Epoch 13, Batch 0, rec loss 231.24566650390625, kl 14.666435241699219\n",
      "Epoch 13, Batch 100, rec loss 234.44589233398438, kl 15.101644515991211\n",
      "Epoch 14, Batch 0, rec loss 231.51658630371094, kl 15.067808151245117\n",
      "Epoch 14, Batch 100, rec loss 231.513427734375, kl 14.947068214416504\n",
      "Epoch 15, Batch 0, rec loss 229.2972412109375, kl 15.371150016784668\n",
      "Epoch 15, Batch 100, rec loss 236.37615966796875, kl 14.6906156539917\n",
      "Epoch 16, Batch 0, rec loss 230.3271026611328, kl 15.360008239746094\n",
      "Epoch 16, Batch 100, rec loss 231.8116455078125, kl 14.82005500793457\n",
      "Epoch 17, Batch 0, rec loss 230.79800415039062, kl 14.921131134033203\n",
      "Epoch 17, Batch 100, rec loss 232.12960815429688, kl 15.127669334411621\n",
      "Epoch 18, Batch 0, rec loss 236.00001525878906, kl 15.306396484375\n",
      "Epoch 18, Batch 100, rec loss 228.46058654785156, kl 14.46934986114502\n",
      "Epoch 19, Batch 0, rec loss 232.4032745361328, kl 15.2178955078125\n",
      "Epoch 19, Batch 100, rec loss 239.2562255859375, kl 15.517318725585938\n",
      "Epoch 20, Batch 0, rec loss 229.59474182128906, kl 14.57783317565918\n",
      "Epoch 20, Batch 100, rec loss 232.64109802246094, kl 15.095540046691895\n",
      "Epoch 21, Batch 0, rec loss 225.4127197265625, kl 15.090032577514648\n",
      "Epoch 21, Batch 100, rec loss 234.95449829101562, kl 15.308486938476562\n",
      "Epoch 22, Batch 0, rec loss 232.10528564453125, kl 15.392109870910645\n",
      "Epoch 22, Batch 100, rec loss 240.89561462402344, kl 15.227855682373047\n",
      "Epoch 23, Batch 0, rec loss 225.58819580078125, kl 15.145315170288086\n",
      "Epoch 23, Batch 100, rec loss 233.4103240966797, kl 14.857958793640137\n",
      "Epoch 24, Batch 0, rec loss 231.22694396972656, kl 15.627706527709961\n",
      "Epoch 24, Batch 100, rec loss 226.5390625, kl 14.941547393798828\n",
      "Epoch 25, Batch 0, rec loss 234.28713989257812, kl 14.48676872253418\n",
      "Epoch 25, Batch 100, rec loss 230.67922973632812, kl 15.388776779174805\n",
      "Epoch 26, Batch 0, rec loss 225.62765502929688, kl 15.388120651245117\n",
      "Epoch 26, Batch 100, rec loss 233.58157348632812, kl 14.937129974365234\n",
      "Epoch 27, Batch 0, rec loss 227.02151489257812, kl 15.013650894165039\n",
      "Epoch 27, Batch 100, rec loss 235.7631378173828, kl 15.17831039428711\n",
      "Epoch 28, Batch 0, rec loss 228.6434783935547, kl 15.353065490722656\n",
      "Epoch 28, Batch 100, rec loss 224.79225158691406, kl 15.554228782653809\n",
      "Epoch 29, Batch 0, rec loss 230.55039978027344, kl 15.382116317749023\n",
      "Epoch 29, Batch 100, rec loss 240.554931640625, kl 15.526281356811523\n",
      "Epoch 30, Batch 0, rec loss 231.55918884277344, kl 15.378150939941406\n",
      "Epoch 30, Batch 100, rec loss 229.79391479492188, kl 15.68019962310791\n",
      "Epoch 31, Batch 0, rec loss 225.93603515625, kl 15.16878890991211\n",
      "Epoch 31, Batch 100, rec loss 227.30764770507812, kl 15.440010070800781\n",
      "Epoch 32, Batch 0, rec loss 227.70468139648438, kl 15.71022891998291\n",
      "Epoch 32, Batch 100, rec loss 234.1063995361328, kl 15.463868141174316\n",
      "Epoch 33, Batch 0, rec loss 224.54766845703125, kl 15.512845993041992\n",
      "Epoch 33, Batch 100, rec loss 230.48992919921875, kl 14.921018600463867\n",
      "Epoch 34, Batch 0, rec loss 232.66770935058594, kl 15.559028625488281\n",
      "Epoch 34, Batch 100, rec loss 235.0677947998047, kl 15.429471969604492\n",
      "Epoch 35, Batch 0, rec loss 225.8807373046875, kl 15.279857635498047\n",
      "Epoch 35, Batch 100, rec loss 234.6699981689453, kl 15.51052474975586\n",
      "Epoch 36, Batch 0, rec loss 227.02108764648438, kl 15.150484085083008\n",
      "Epoch 36, Batch 100, rec loss 230.90701293945312, kl 15.560379981994629\n",
      "Epoch 37, Batch 0, rec loss 235.49993896484375, kl 16.08946990966797\n",
      "Epoch 37, Batch 100, rec loss 231.72378540039062, kl 15.53588581085205\n",
      "Epoch 38, Batch 0, rec loss 233.46070861816406, kl 14.986763000488281\n",
      "Epoch 38, Batch 100, rec loss 228.11248779296875, kl 15.761370658874512\n",
      "Epoch 39, Batch 0, rec loss 230.36117553710938, kl 15.425479888916016\n",
      "Epoch 39, Batch 100, rec loss 224.49002075195312, kl 15.381847381591797\n",
      "Epoch 40, Batch 0, rec loss 227.35240173339844, kl 15.176485061645508\n",
      "Epoch 40, Batch 100, rec loss 231.44277954101562, kl 15.644342422485352\n",
      "Epoch 41, Batch 0, rec loss 225.13632202148438, kl 15.661201477050781\n",
      "Epoch 41, Batch 100, rec loss 225.9834747314453, kl 15.176229476928711\n",
      "Epoch 42, Batch 0, rec loss 228.49966430664062, kl 15.213052749633789\n",
      "Epoch 42, Batch 100, rec loss 233.27346801757812, kl 15.421608924865723\n",
      "Epoch 43, Batch 0, rec loss 226.33018493652344, kl 16.139751434326172\n",
      "Epoch 43, Batch 100, rec loss 228.33541870117188, kl 15.277801513671875\n",
      "Epoch 44, Batch 0, rec loss 230.35147094726562, kl 15.02495288848877\n",
      "Epoch 44, Batch 100, rec loss 229.46104431152344, kl 15.068397521972656\n",
      "Epoch 45, Batch 0, rec loss 230.86053466796875, kl 15.897178649902344\n",
      "Epoch 45, Batch 100, rec loss 231.94839477539062, kl 15.406871795654297\n",
      "Epoch 46, Batch 0, rec loss 227.1590576171875, kl 14.83413314819336\n",
      "Epoch 46, Batch 100, rec loss 230.73941040039062, kl 14.991497039794922\n",
      "Epoch 47, Batch 0, rec loss 231.65621948242188, kl 15.233951568603516\n",
      "Epoch 47, Batch 100, rec loss 228.9786376953125, kl 15.258265495300293\n",
      "Epoch 48, Batch 0, rec loss 228.08834838867188, kl 15.475151062011719\n",
      "Epoch 48, Batch 100, rec loss 232.95761108398438, kl 15.464564323425293\n",
      "Epoch 49, Batch 0, rec loss 226.04647827148438, kl 15.238323211669922\n",
      "Epoch 49, Batch 100, rec loss 226.18092346191406, kl 15.193704605102539\n",
      "Epoch 50, Batch 0, rec loss 227.76873779296875, kl 15.182446479797363\n",
      "Epoch 50, Batch 100, rec loss 228.667724609375, kl 15.249063491821289\n",
      "Epoch 51, Batch 0, rec loss 222.00631713867188, kl 15.449186325073242\n",
      "Epoch 51, Batch 100, rec loss 232.13674926757812, kl 15.084578514099121\n",
      "Epoch 52, Batch 0, rec loss 229.72975158691406, kl 15.095390319824219\n",
      "Epoch 52, Batch 100, rec loss 230.85411071777344, kl 15.518477439880371\n",
      "Epoch 53, Batch 0, rec loss 230.87864685058594, kl 15.381686210632324\n",
      "Epoch 53, Batch 100, rec loss 229.6859588623047, kl 14.987796783447266\n",
      "Epoch 54, Batch 0, rec loss 225.45205688476562, kl 15.766867637634277\n",
      "Epoch 54, Batch 100, rec loss 229.8700714111328, kl 15.148429870605469\n",
      "Epoch 55, Batch 0, rec loss 224.09805297851562, kl 14.889766693115234\n",
      "Epoch 55, Batch 100, rec loss 230.92123413085938, kl 15.433218002319336\n",
      "Epoch 56, Batch 0, rec loss 225.5115509033203, kl 15.499868392944336\n",
      "Epoch 56, Batch 100, rec loss 232.858642578125, kl 15.02985954284668\n",
      "Epoch 57, Batch 0, rec loss 222.02391052246094, kl 15.568073272705078\n",
      "Epoch 57, Batch 100, rec loss 229.61000061035156, kl 15.084177017211914\n",
      "Epoch 58, Batch 0, rec loss 230.29354858398438, kl 15.632808685302734\n",
      "Epoch 58, Batch 100, rec loss 228.5897216796875, kl 15.183363914489746\n",
      "Epoch 59, Batch 0, rec loss 223.95278930664062, kl 15.802167892456055\n",
      "Epoch 59, Batch 100, rec loss 230.31866455078125, kl 15.612465858459473\n",
      "Epoch 60, Batch 0, rec loss 227.49346923828125, kl 14.800804138183594\n",
      "Epoch 60, Batch 100, rec loss 227.30319213867188, kl 15.253202438354492\n",
      "Epoch 61, Batch 0, rec loss 227.73440551757812, kl 14.465019226074219\n",
      "Epoch 61, Batch 100, rec loss 229.09925842285156, kl 15.293550491333008\n",
      "Epoch 62, Batch 0, rec loss 225.1915740966797, kl 14.348112106323242\n",
      "Epoch 62, Batch 100, rec loss 230.17474365234375, kl 15.0762357711792\n",
      "Epoch 63, Batch 0, rec loss 226.66964721679688, kl 15.180123329162598\n",
      "Epoch 63, Batch 100, rec loss 226.8640899658203, kl 15.298295974731445\n",
      "Epoch 64, Batch 0, rec loss 230.1021728515625, kl 14.850513458251953\n",
      "Epoch 64, Batch 100, rec loss 231.46768188476562, kl 15.251212120056152\n",
      "Epoch 65, Batch 0, rec loss 225.81549072265625, kl 14.770442962646484\n",
      "Epoch 65, Batch 100, rec loss 237.4126739501953, kl 15.7590970993042\n",
      "Epoch 66, Batch 0, rec loss 229.22296142578125, kl 15.107858657836914\n",
      "Epoch 66, Batch 100, rec loss 231.5027313232422, kl 15.246879577636719\n",
      "Epoch 67, Batch 0, rec loss 225.59368896484375, kl 15.261476516723633\n",
      "Epoch 67, Batch 100, rec loss 222.77606201171875, kl 14.937665939331055\n",
      "Epoch 68, Batch 0, rec loss 221.02328491210938, kl 15.285591125488281\n",
      "Epoch 68, Batch 100, rec loss 228.6055908203125, kl 15.46548843383789\n",
      "Epoch 69, Batch 0, rec loss 222.13421630859375, kl 15.013542175292969\n",
      "Epoch 69, Batch 100, rec loss 223.2274169921875, kl 15.299140930175781\n",
      "Epoch 70, Batch 0, rec loss 232.21444702148438, kl 14.881745338439941\n",
      "Epoch 70, Batch 100, rec loss 232.98451232910156, kl 15.400595664978027\n",
      "Epoch 71, Batch 0, rec loss 225.11631774902344, kl 14.715810775756836\n",
      "Epoch 71, Batch 100, rec loss 226.23475646972656, kl 15.30920696258545\n",
      "Epoch 72, Batch 0, rec loss 226.83413696289062, kl 15.309128761291504\n",
      "Epoch 72, Batch 100, rec loss 229.3366241455078, kl 14.982002258300781\n",
      "Epoch 73, Batch 0, rec loss 225.48910522460938, kl 15.357404708862305\n",
      "Epoch 73, Batch 100, rec loss 239.95291137695312, kl 15.099433898925781\n",
      "Epoch 74, Batch 0, rec loss 227.33282470703125, kl 15.662109375\n",
      "Epoch 74, Batch 100, rec loss 230.0397186279297, kl 15.301599502563477\n",
      "Epoch 75, Batch 0, rec loss 227.69854736328125, kl 15.449613571166992\n",
      "Epoch 75, Batch 100, rec loss 224.00209045410156, kl 15.787334442138672\n",
      "Epoch 76, Batch 0, rec loss 227.46690368652344, kl 15.196432113647461\n",
      "Epoch 76, Batch 100, rec loss 229.42234802246094, kl 15.367589950561523\n",
      "Epoch 77, Batch 0, rec loss 225.40115356445312, kl 15.478286743164062\n",
      "Epoch 77, Batch 100, rec loss 225.11447143554688, kl 14.818584442138672\n",
      "Epoch 78, Batch 0, rec loss 222.4205322265625, kl 15.103157043457031\n",
      "Epoch 78, Batch 100, rec loss 229.46298217773438, kl 14.878928184509277\n",
      "Epoch 79, Batch 0, rec loss 228.4767608642578, kl 15.068242073059082\n",
      "Epoch 79, Batch 100, rec loss 220.953857421875, kl 15.299982070922852\n",
      "Epoch 80, Batch 0, rec loss 220.8783416748047, kl 15.18214225769043\n",
      "Epoch 80, Batch 100, rec loss 226.75372314453125, kl 15.185245513916016\n",
      "Epoch 81, Batch 0, rec loss 226.55633544921875, kl 15.621899604797363\n",
      "Epoch 81, Batch 100, rec loss 223.086669921875, kl 14.97620677947998\n",
      "Epoch 82, Batch 0, rec loss 221.3675537109375, kl 15.40985107421875\n",
      "Epoch 82, Batch 100, rec loss 225.61512756347656, kl 14.903972625732422\n",
      "Epoch 83, Batch 0, rec loss 227.45257568359375, kl 15.1503324508667\n",
      "Epoch 83, Batch 100, rec loss 231.57376098632812, kl 15.25521183013916\n",
      "Epoch 84, Batch 0, rec loss 229.15103149414062, kl 15.01236343383789\n",
      "Epoch 84, Batch 100, rec loss 231.40516662597656, kl 15.459772109985352\n",
      "Epoch 85, Batch 0, rec loss 219.69593811035156, kl 15.195003509521484\n",
      "Epoch 85, Batch 100, rec loss 226.84561157226562, kl 15.088056564331055\n",
      "Epoch 86, Batch 0, rec loss 223.06814575195312, kl 15.139235496520996\n",
      "Epoch 86, Batch 100, rec loss 226.91539001464844, kl 14.775728225708008\n",
      "Epoch 87, Batch 0, rec loss 224.31460571289062, kl 15.160497665405273\n",
      "Epoch 87, Batch 100, rec loss 227.45657348632812, kl 15.173416137695312\n",
      "Epoch 88, Batch 0, rec loss 224.70761108398438, kl 15.098197937011719\n",
      "Epoch 88, Batch 100, rec loss 230.7292022705078, kl 15.028358459472656\n",
      "Epoch 89, Batch 0, rec loss 227.62205505371094, kl 15.089055061340332\n",
      "Epoch 89, Batch 100, rec loss 228.1893768310547, kl 15.195079803466797\n",
      "Epoch 90, Batch 0, rec loss 228.87188720703125, kl 15.309420585632324\n",
      "Epoch 90, Batch 100, rec loss 228.43692016601562, kl 15.42381477355957\n",
      "Epoch 91, Batch 0, rec loss 223.28997802734375, kl 15.068074226379395\n",
      "Epoch 91, Batch 100, rec loss 225.77439880371094, kl 15.535183906555176\n",
      "Epoch 92, Batch 0, rec loss 228.4051513671875, kl 14.493616104125977\n",
      "Epoch 92, Batch 100, rec loss 227.37322998046875, kl 15.011366844177246\n",
      "Epoch 93, Batch 0, rec loss 223.8580780029297, kl 14.954328536987305\n",
      "Epoch 93, Batch 100, rec loss 225.4228057861328, kl 15.417442321777344\n",
      "Epoch 94, Batch 0, rec loss 227.15875244140625, kl 14.576358795166016\n",
      "Epoch 94, Batch 100, rec loss 220.50140380859375, kl 15.507772445678711\n",
      "Epoch 95, Batch 0, rec loss 227.5120849609375, kl 15.114062309265137\n",
      "Epoch 95, Batch 100, rec loss 230.3238525390625, kl 14.882749557495117\n",
      "Epoch 96, Batch 0, rec loss 223.38925170898438, kl 14.915565490722656\n",
      "Epoch 96, Batch 100, rec loss 223.8690185546875, kl 15.099380493164062\n",
      "Epoch 97, Batch 0, rec loss 227.8948974609375, kl 15.12774658203125\n",
      "Epoch 97, Batch 100, rec loss 229.72479248046875, kl 15.01730728149414\n",
      "Epoch 98, Batch 0, rec loss 224.07496643066406, kl 14.854434967041016\n",
      "Epoch 98, Batch 100, rec loss 219.50962829589844, kl 15.065048217773438\n",
      "Epoch 99, Batch 0, rec loss 227.3610076904297, kl 15.258623123168945\n",
      "Epoch 99, Batch 100, rec loss 228.41741943359375, kl 14.42936897277832\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, x in enumerate(train_db):\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "        rec_loss, kl = train_vae_step(vae_model, x)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {step}, rec loss {float(rec_loss)}, kl {float(kl)}\")\n",
    "    # 生成\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        z = tf.random.normal([100, h_dim])\n",
    "        logits = vae_model.decoder(z)\n",
    "        x = tf.sigmoid(logits)  # 0至1的像素值\n",
    "        img = tf.reshape(x, [-1, 28, 28]).numpy() * 255 # 0-255\n",
    "        img = img.astype(np.uint8)\n",
    "        save_image(img, f'vae_images/gen_image_{epoch}.png')\n",
    "\n",
    "        inp = next(iter(test_db))[:100]\n",
    "        inp = tf.reshape(inp, [-1, 784])\n",
    "        out, _, _ = vae_model(inp)\n",
    "        out = tf.sigmoid(out)  # 0至1的像素值\n",
    "        img = tf.reshape(out, [-1, 28, 28]).numpy() * 255 # 0-255\n",
    "        img = img.astype(np.uint8)\n",
    "        save_image(img, f'vae_images/test_image_{epoch}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37664bittf2conda2a75a45106264ceab7472c43279a5d24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
