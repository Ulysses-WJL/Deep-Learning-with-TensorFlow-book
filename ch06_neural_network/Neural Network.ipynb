{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机\n",
    "\n",
    "\n",
    "感知机模型的结构如图所示，它接受长度为𝑛的一维向量$X = [x_1, x_2, … , x_n]$，每个输入节点通过权值为$w_i, i\\in[1, n]$的连接汇集为变量$z$，即：\n",
    "$$z = w_1x_1 + w_2x_2 + ⋯ + 𝑤_nx_n + b$$\n",
    "其中𝑏称为感知机的偏置(Bias)，一维向量$w = [w_1, w_2, … , w_n]$称为感知机的权值(Weight)，$𝑧$称为感知机的净活性值(Net Activation)。\n",
    "\n",
    "![](./感知机.png)\n",
    "\n",
    "向量形式:\n",
    "$$z = w^Tx + b$$\n",
    "感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数后得到活性值(Activation)$\\alpha$:\n",
    "$$\\alpha = \\sigma(z) = \\sigma(w^T + b)$$\n",
    "\n",
    "$\\sigma$激活函数可以是阶跃函数(Step function)或符号函数(Sign function)\n",
    "\n",
    "添加激活函数后，感知机可以用来完成二分类任务。阶跃函数和符号函数在𝑧 = 0处是不连续的，其他位置导数为 0，无法利用梯度下降算法进行参数优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层\n",
    "\n",
    "![](./全连接层.png)\n",
    "\n",
    "整个网络层关系:\n",
    "$$\n",
    "\\begin{bmatrix} o_1 & o_2\\end{bmatrix} = \n",
    "\\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} @\n",
    "\\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\\\ w_{31} & w_{32}\\end{bmatrix} +\n",
    "\\begin{bmatrix} b_1 & b_2\\end{bmatrix}\n",
    "$$\n",
    "即 $$O = X @ W + b$$\n",
    "输入矩阵X的shape定义为$[b, d_{in}]$，𝑏为样本数量，此处只有1个样本参与前向运算，$d_{in}$为输入节点数；权值矩阵 W 的 shape 定义为$[d_{in}, d_{out}]$，$𝑑_{out}$为输出节点数，偏置向量 b的 shape 定义为$[𝑑_{out}]$。\n",
    "\n",
    "由于每个输出节点与全部的输入节点相连接，这种网络层称为`全连接层(Fully-connected Layer)`，或者`稠密连接层(Dense Layer)`，𝑾矩阵叫做全连接层的权值矩阵，𝒃向量叫做全连接层的偏置向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['Noto Sans CJK JP']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用张量方式实现全层连接\n",
    "\n",
    "x = tf.random.normal([2, 784])\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "o1 = x @ w1 + b1\n",
    "o1 = tf.nn.relu(o1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow layers层方式实现\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "x = tf.random.normal([4, 28 * 28])\n",
    "# 创建全连接层，指定输出节点数和激活函数\n",
    "fc = layers.Dense(256, activation=tf.nn.relu)\n",
    "h1 = fc(x)  # 输入的节点数在fc(x)计算时自动获取\n",
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.kernel  # 获取 Dense 类的权值矩阵 w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.bias  # 获取 Dense 类的偏置向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.trainable_variables  # 返回待优化参数列表\n",
    "# fc.non_trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "通过层层堆叠全连接层，保证前一层的输出节点数与当前层的输入节点数匹配,即可堆叠出任意层数的网络-神经网络\n",
    "\n",
    "![](./神经网络.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用张量实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128]))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 64]))\n",
    "b3 = tf.Variable(tf.zeros([64]))\n",
    "w4 = tf.Variable(tf.random.truncated_normal([64, 10]))\n",
    "b4 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    h1 = x @ w1 + b1\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    h2 = h1 @ w2 + b2\n",
    "    h2 = tf.nn.relu(h2)\n",
    "    h3 = h2 @ w3 + b3\n",
    "    h3 = tf.nn.relu(h3)\n",
    "    out = h3 @ w4 + b4\n",
    "    # loss = ...\n",
    "    # 最后一层是否需要添加激活函数通常视具体的任务而定，这里加不加都可以\n",
    "# grads = tape.gradient(loss, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用层方式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "# 隐藏层\n",
    "fc1 = layers.Dense(256, activation=tf.nn.relu)\n",
    "fc2 = layers.Dense(128, activation=tf.nn.relu)\n",
    "fc3 = layers.Dense(64, activation=tf.nn.relu)\n",
    "# 输入层\n",
    "fc4 = layers.Dense(10, activation=None)\n",
    "\n",
    "h1 = fc1(x)\n",
    "h2 = fc2(h1)\n",
    "h3 = fc3(h2)\n",
    "out = fc4(h3)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Sequential封装\n",
    "model = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu), \n",
    "    layers.Dense(128, activation=tf.nn.relu), \n",
    "    layers.Dense(64, activation=tf.nn.relu), \n",
    "    layers.Dense(10, activation=tf.nn.relu), \n",
    "])\n",
    "out = model(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化目标\n",
    "我们把神经网络从输入到输出的计算过程叫做`前向传播(Forward Propagation)`或前向计算。神经网络的前向传播过程，也是数据张量(Tensor)从第一层流动(Flow)至输出层的过程，即从输入数据开始，途径每个隐藏层，直至得到输出并计算误差，这也是 TensorFlow框架名字由来。\n",
    "\n",
    "前向传播最后一步, 误差计算:\n",
    "\n",
    "$$ L = g(f_{\\theta}(x), y)$$\n",
    "\n",
    "优化目标:\n",
    "$$\\theta^*= \\mathop{argmin}_{\\theta} g(f_{\\theta}(x), y), x\\in D^{train}$$\n",
    "一般采用误差反向传播(Backward Propagation，简称 BP)算法来求解网络参数𝜃的梯度信息，并利用梯度下降(Gradient Descent，简称 GD)算法迭代更新参数:\n",
    "$$\\theta'= \\theta - \\eta \\cdot \\nabla_{\\theta}L$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各种激活函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid函数\n",
    "\n",
    "也叫logistic函数:\n",
    "$$ Sigmoid(x) = \\sigma(x)= \\frac {1} {1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-6., 6, 100)\n",
    "plt.plot(x, tf.nn.sigmoid(x))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 像 Dense 层一样将ReLU 函数作为一个网络层添加到网络中\n",
    "# layers.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU函数(REctified Linear Unit，修正线性单元)\n",
    "\n",
    "Sigmoid 函数在输入值较大或较小时容易出现梯度值接近于 0 的现象，称为梯度弥散现象。出现`梯度弥散`(梯度消失)现象时，网络参数长时间得不到更新，导致训练不收敛或停滞不动的现象发生，较深层次的网络模型中更容易出现梯度弥散现象.ReLU 对小于 0 的值全部抑制为 0；对于正数则直接输出，这种单边抑制特性来源于生物学.\n",
    "$$Relu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-6., 6, 100)\n",
    "plt.plot(x, tf.nn.relu(x))\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeakyReLU\n",
    "ReLU 函数在𝑥 < 0时导数值恒为 0，也可能会造成梯度弥散现象，为了克服这个问题，LeakyReLU 函数被提出:\n",
    "$$\n",
    "LeakyReLU = \\begin{cases} x \\quad x \\geq 0  \\\\ px \\quad x < 0\\end{cases} \\\\ \n",
    "g(z) = max(pz, z)\n",
    "$$\n",
    "其中𝑝为用户自行设置的某较小数值的超参数，如 0.02 等。当𝑝 = 0时，LeayReLU 函数退化为 ReLU 函数；当𝑝 ≠ 0时，𝑥< 0处能够获得较小的导数值𝑝，从而避免出现梯度弥散现象.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-6., 6, 100)\n",
    "plt.plot(x, tf.nn.leaky_relu(x, alpha=0.1))\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh函数能将数据压缩到(-1, 1)区间\n",
    "$$\n",
    "tanh(x) = \\frac {e^x - e^{-x}} {e^x + e^{-x}} = 2 \\cdot sigmoid(2x) - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-6., 6, 100)\n",
    "# tf.nn.tanh\n",
    "plt.plot(x, tf.tanh(x))\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['right'].set_color('none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftPlus函数\n",
    "$$\n",
    "f(x) =\\zeta(x) = ln(1+e^x)\n",
    "$$\n",
    "它是ReLU函数的平滑版本, 值域为$(0, +\\infty)$\n",
    "一些其他性质:\n",
    "$$\n",
    "log\\sigma(x) = -\\zeta(-x) \\\\\n",
    "\\frac {d} {dx}\\zeta(x) = \\sigma(x) \\\\\n",
    "\\zeta(x) = \\int_{-\\infty}^x \\sigma(y)dy  \\\\\n",
    "\\zeta(x) - \\zeta(-x) = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10., 10., 100)\n",
    "plt.plot(x, tf.nn.softplus(x))\n",
    "# ax = plt.gca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出层的设计\n",
    "最后一层需要根据具体的任务场景来决定是否使用激活函数, 以及使用什么类型的激活函数等:\n",
    "\n",
    "1. 普通实数空间$o_i \\in R^d$, 这一类问题比较普遍，像正弦函数曲线预测、年龄的预测、股票走势的预测等都属于整个或者部分连续的实数空间，输出层可以不加激活函数\n",
    "1. $o_i \\in [0, 1]$, 输出值特别地落在\\[0, 1\\]的区间，如图片生成，图片像素值一般用\\[0, 1\\]区间的值表示；或者二分类问题的概率，如硬币正反面的概率预测问题, 输出层可以只设一个节点, 表示事件发生的概率.可以使用Sigmoid函数\n",
    "1. $o_i \\in [0, 1]$, 且$\\sum_i o_i = 1$, 常见的如多分类问题，如 MNIST 手写数字图片识别，图片属于 10 个类别的概率之和应为 1。在输出层添加Softmax函数实现\n",
    "1. $o_i \\in [-1, 1]$, 可以简单的使用tf.tanh函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 函数\n",
    "z = tf.constant([2, 1, 0.1])\n",
    "tf.nn.softmax(z)\n",
    "# 添加 Softmax 层\n",
    "# layers.Softmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Softmax 函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交\n",
    "叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性，TensorFlow 中提供了一个统\n",
    "一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一\n",
    "般推荐使用这些接口函数，避免分开使用 Softmax 函数与交叉熵损失函数。函数式接口为\n",
    "`tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)`，其中 y_true 代表了\n",
    "One-hot 编码后的真实标签，y_pred 表示网络的预测值，当 from_logits 设置为 True 时，\n",
    "y_pred 表示须为未经过 Softmax 函数的变量 z；当 from_logits 设置为 False 时，y_pred 表示\n",
    "为经过 Softmax 函数的输出。为了数值计算稳定性，一般设置 from_logits 为 True，此时\n",
    "tf.keras.losses.categorical_crossentropy 将在内部进行 Softmax 函数计算，所以不需要在模型\n",
    "中显式调用 Softmax 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal([2, 10])  # 模拟输出层\n",
    "y_onehot = tf.one_hot(tf.constant([1, 3]), depth=10)\n",
    "# 未调用softmax函数时, 设置from_logists=True\n",
    "loss = tf.keras.losses.categorical_crossentropy(y_onehot, z, from_logits=True)\n",
    "loss = tf.reduce_mean(loss)  # 计算平均交叉熵损失\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteon = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "loss = criteon(y_onehot, z)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差计算\n",
    "\n",
    "常见的误差函数有均方差、交叉熵、KL 散度、Hinge Loss 函数等，其中均方差函数和交叉熵函数在深度学习中比较常见，均方差函数主要用于回归问题，交叉熵函数主要用于分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均方差(Mean Squared Error, MSE)\n",
    "\n",
    "$$\n",
    "MSE(y, o) = \\frac 1 {d_{out}} \\sum^{d_{out}}_{i=1}(y_i - o_i)^2\n",
    "$$\n",
    "\n",
    "均方差误差函数广泛应用在回归问题中，实际上，分类问题中也可以应用均方差误差函数。在 TensorFlow 中，可以通过函数方式或层方式实现 MSE 误差计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "out = tf.random.normal([2, 10])\n",
    "y_onehot = tf.one_hot(tf.constant([1, 3]), depth=10)\n",
    "loss = keras.losses.mse(y_onehot, out)  # 每个样本的MSE\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(loss)  # 计算 batch 均方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以layer的方式实现\n",
    "criterion = keras.losses.MeanSquaredError()\n",
    "loss = criterion(y_onehot, out)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵误差\n",
    "\n",
    "某个分布P(i)的熵(Entropy)定义为:\n",
    "$$H(P) = -\\sum_iP(i)log_2P(i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4类分类问题, 四种等可能情况时 熵\n",
    "p = tf.constant([0.25, 0.25, 0.25, 0.25])\n",
    "entropy = tf.reduce_sum(-tf.math.log(p) / tf.math.log([2., 2, 2, 2]) * p )\n",
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵(Cross Entropy):\n",
    "$$H(p||q) = -\\sum_i p(i)log_2q(i)$$\n",
    "通过变换，交叉熵可以分解为𝑝的熵𝐻(𝑝)和𝑝与𝑞的 KL 散度(Kullback-Leibler Divergence)的和\n",
    "$$H(p||q) = H(p) + D_{KL}(p||q)$$\n",
    "KL散度:\n",
    "$$D_{KL}(p||q) = \\sum_i p(i)log(\\frac {p(i)}{q(i)})$$\n",
    "是用于衡量 2 个分布之间距离的指标.当p=q时,$D_{KL}(p||q)$取得最小值, p和q之间的差距越大, $D_{KL}(p||q)$也就越大.\n",
    "\n",
    "需要注意的是，交叉熵和 KL 散度都不是对称的.  \n",
    "交叉熵可以很好地衡量 2 个分布之间的“距离”。特别地，当分类问题中y的编码分布𝑝采用One-hot编码𝒚时：𝐻(𝑝) = 0, 此时\n",
    "$$H(p||q) = H(p) + D_{KL}(p||q)= D_{KL}(p||q)$$\n",
    "推导分类问题中的交叉熵的计算表达式:\n",
    "$$ H(p||q) = D_{KL}(p||q) = \\sum_j y_j log(\\frac {y_j} {o_j}) \\\\\n",
    "= 1 \\cdot log\\frac {1}{o_i} + \\sum_{i \\neq j} 0 \\cdot log\\frac {0}{o_j} \\\\\n",
    "= -logo_i\n",
    "$$\n",
    "其中𝑖为 One-hot 编码中为 1 的索引号，也是当前输入的真实类别。可以看到，ℒ只与真实类别𝑖上的概率$𝑜_𝑖$有关，对应概率$𝑜_𝑖$越大，𝐻(𝑝||𝑞)越小。当对应类别上的概率为 1 时，交叉熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态.\n",
    "最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal([10])  # 模拟输出层\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = tf.nn.softmax(z)\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-tf.math.log(s1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = tf.one_hot(tf.constant([1]), depth=10)\n",
    "# 计算交叉熵\n",
    "# 未调用softmax函数时, 设置from_logists=True\n",
    "loss = tf.keras.losses.categorical_crossentropy(y_onehot, z, from_logits=True)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络类型\n",
    "\n",
    "- 卷积神经网络(Convolutional Neural Network, CNN): 应用于计算机视觉, 如图片分类;\n",
    "- 循环神经网络(Recurrent Neural Network, RNN): 序列信号处理, 如理解文本数据, NLP;\n",
    "- 注意力(机制)网络(Attention Mechanism): 自然语言处理;\n",
    "- 图卷积神经网络(Graph Convolution Network, GCN): 处理社交网络、通信网络、蛋白质分子结构等一系列的不规则空间拓扑结构的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 汽车耗油项目实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在线下载汽车效能数据集\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \n",
    "\"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 pandas 读取数据集，字段有效能（公里数每加仑），气缸数，排量，马力，重量, 加速度，型号年份，产地\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', \n",
    "    'Acceleration', 'Model Year', 'Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_daraset = pd.read_csv(dataset_path, names=column_names, \n",
    "    na_values='?', comment='\\t', sep=' ', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_daraset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看部分数据\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看 NaN\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接舍弃缺失的数据\n",
    "dataset = dataset.dropna()\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类别类型数据 # 处理类别型数据，其中origin列代表了类别1,2,3,分布代表产地：美国、欧洲、日本\n",
    "# 其弹出这一列\n",
    "dataset['Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = dataset.pop('Origin')\n",
    "dataset['USA'] = (origin == 1) * 1.0\n",
    "dataset['Europe'] = (origin == 2) * 1.0\n",
    "dataset['Japan'] = (origin == 3) * 1.0\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dataset.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集 训练:测试 = 8:2\n",
    "X_train = dataset.sample(frac=0.8, random_state=0)\n",
    "X_test = dataset.drop(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_train[[\"Cylinders\", \"Displacement\", \"Weight\", \"MPG\"]], \n",
    "diag_kind=\"kde\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPG 数据作为 labels \n",
    "y_train = X_train.pop('MPG')\n",
    "y_test = X_test.pop('MPG')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转成numpy\n",
    "X_train, X_test = X_train.values, X_test.values\n",
    "y_train, y_test = y_train.values, y_test.values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_train_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normal = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "X_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_normal.shape, y_train.shape)\n",
    "print(X_test_normal.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Dataset对象\n",
    "train_db = tf.data.Dataset.from_tensor_slices((X_train_normal, y_train))\n",
    "train_db = train_db.shuffle(100).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个3层的回归网络 \n",
    "# 输入𝑿的特征共有 9 种，因此第一层的输入节点数为 9。第一层、第二层的\n",
    "# 输出节点数设计为64和64，由于只有一种预测值，输出层输出节点设计为 1\n",
    "\n",
    "class Network(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        # self.fc1 = layers.Dense(64, activation='relu')\n",
    "        # self.fc2 = layers.Dense(64, activation='relu')\n",
    "        # self.fc3 = layers.Dense(1)\n",
    "    \n",
    "    # 在前向计算函数 call 中实现自定义网络类的计算逻辑即可\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        out = self.model(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()  # \n",
    "# 通过 build 函数完成内部张量的创建，其中 4 为任意设置的 batch 数量，9 为输入特征长度\n",
    "model.build(input_shape=(4, 9))\n",
    "model.summary() # 打印网络信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9 * 64 + 64 + 64 * 64 + 64 + 64 * 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(0.001)  # 创建优化器, 指定学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$mae = \\frac 1{d_{out}} \\sum_i |y_i - o_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae_losses = []\n",
    "train_losses = []\n",
    "test_mae_losses = []\n",
    "for epoch in range(200):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = model(x)\n",
    "            loss = tf.reduce_mean(losses.MSE(y, out))  # 最小化的目标MSE \n",
    "            mae_loss = tf.reduce_mean(losses.MAE(y, out))  # MAE用平均绝对误差\n",
    "        if step % 10 == 0:\n",
    "            print(epoch, step, float(loss))\n",
    "        \n",
    "        # 自动计算梯度\n",
    "        train_losses.append(loss)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # 记录每个epoch 的mae误差\n",
    "    train_mae_losses.append(float(mae_loss))\n",
    "    out = model(X_test_normal)\n",
    "    test_mae_losses.append(tf.reduce_mean(losses.MAE(out, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_mae_losses, label='Train')\n",
    "plt.plot(test_mae_losses, label='Test')\n",
    "# plt.ylim(1, 10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}