{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# å¾ªç¯ç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åºåˆ—\n",
    "å…·æœ‰å…ˆåé¡ºåºçš„æ•°æ®ä¸€èˆ¬å«ä½œåºåˆ—(Sequence). æˆ‘ä»¬æŠŠæ–‡å­—ç¼–ç ä¸ºæ•°å€¼çš„è¿‡ç¨‹å«ä½œ**Word Embedding**.\n",
    "\n",
    "one-hotç¼–ç çš„ä¼˜ç¼ºç‚¹:\n",
    "- ç®€å•ç›´è§‚ï¼Œç¼–ç è¿‡ç¨‹ä¸éœ€è¦å­¦ä¹ å’Œè®­ç»ƒ;\n",
    "- ä½†é«˜ç»´åº¦è€Œä¸”æå…¶ç¨€ç–çš„ï¼Œå¤§é‡çš„ä½ç½®ä¸º0ï¼Œè®¡ç®—æ•ˆç‡è¾ƒä½, å¿½ç•¥äº†å•è¯å…ˆå¤©å…·æœ‰çš„è¯­ä¹‰ç›¸å…³æ€§;\n",
    "\n",
    "ä½™å¼¦ç›¸å…³åº¦(Cosine similarity), è¡¡é‡è¯å‘é‡(word vector)ä¹‹é—´ç›¸å…³åº¦:\n",
    "$$similarity(a, b) \\triangleq \\frac {a \\cdot b}{|a|\\cdot|b|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddingå±‚\n",
    "å•è¯çš„è¡¨ç¤ºå±‚å«ä½œEmbeddingå±‚, è´Ÿè´£æŠŠå•è¯ç¼–ç ä¸ºæŸä¸ªè¯å‘é‡ğ’—\n",
    "\n",
    "$$v = f_{\\theta}(i|N_{vocab}, n)$$\n",
    "å•è¯æ•°é‡è®°ä¸º$N_{vocab}$, $vçš„é•¿åº¦ä¸ºn$, $i$è¡¨ç¤ºå•è¯ç¼–å·, å¦‚2 è¡¨ç¤ºâ€œIâ€ï¼Œ3 è¡¨ç¤ºâ€œmeâ€ç­‰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(10)  # ä»£è¡¨10ä¸ªä¸åŒå•è¯çš„ç¼–ç \n",
    "\n",
    "x = tf.random.shuffle(x)\n",
    "# 10ä¸ªå•è¯, æ¯ä¸ªå•è¯ç”¨é•¿åº¦4 çš„å‘é‡è¡¨ç¤º\n",
    "net = layers.Embedding(10, 4)\n",
    "out = net(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„è®­ç»ƒçš„è¯å‘é‡\n",
    "\n",
    "åº”ç”¨çš„æ¯”è¾ƒå¹¿æ³›çš„é¢„è®­ç»ƒæ¨¡å‹:Word2Vec å’ŒGloVeæ¨¡å‹.åˆ©ç”¨å·²é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°åˆå§‹åŒ–Embeddingå±‚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(path):\n",
    "    # å»ºç«‹æ˜ å°„å…³ç³»: å•è¯: è¯å‘é‡(é•¿åº¦50))\n",
    "    embedding_map = {}\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            coefs = np.asarray(l[1:], dtype='float32')\n",
    "            embedding_map[word] = coefs\n",
    "    return embedding_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map = load_embed('glove.6B.50d.txt')\n",
    "print('Found %s word vectors.' % len(embedding_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_map['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# åŠ è½½20newsgroupsæ•°æ®é›†\n",
    "news20 = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = news20.target_names  # ä¸€å…±20ç±»ä¸åŒçš„æ–°é—»\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = news20['target']  # æ¯æ¡æ–°é—»åˆ†å±çš„ç±»åˆ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news20['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news20['data'][0], category[news20['target'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000  # æœ€å¤šä¿ç•™ 20000-1 ä¸ªä¸åŒçš„å•è¯\n",
    "MAX_SEQUENCE_LENGTH = 1000  # æ¯ä¸ªåºåˆ—é•¿åº¦\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EMBEDDING_DIM = 50  # ç”¨50ç»´å‘é‡è¡¨ç¤ºä¸€ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)  #  Only the most common `num_words-1` words will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates internal vocabulary based on a list of texts\n",
    "tokenizer.fit_on_texts(news20['data'])\n",
    "sequences = tokenizer.texts_to_sequences(news20['data'])  # è¯­å¥ -> å•è¯åºåˆ—å·ç»„æˆçš„sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = tokenizer.texts_to_matrix(news20['data'])\n",
    "# matrix.shape  # (11314, 20000)  ç¨€ç–çŸ©é˜µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†sequences è½¬æˆæ–‡æœ¬list\n",
    "# tokenizer.sequences_to_texts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†å•è¯æ˜ å°„ä¸º index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "word_index_list = list(word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»1å¼€å§‹ç¼–ç  ç”¨0ä»£è¡¨å¡«å……\n",
    "word_index_list[:10]  # news20group å‡ºç°é¢‘ç‡æœ€é«˜çš„10ä¸ªå•è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_list[19998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads sequences to the same length.\n",
    "pad_sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯æ¡æ–°é—»éƒ½è¢«ç¼–ç æˆ ç­‰é•¿çš„ ç”¨æ•°å­—è¡¨ç¤ºçš„ åºåˆ—\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(data), np.min(data) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=VALIDATION_SPLIT, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°† å•è¯åºå·-> å•è¯å‘é‡(é•¿åº¦50)\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # æ ¹æ®glove.6B.50d å°†å•è¯è½¬ä¸ºè¯å‘é‡\n",
    "    embedding_vector = embedding_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new20groupä¸­æœ€å¸¸ç”¨çš„19999 è¯å‘é‡ + å¡«å…… + unknow\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    num_words, EMBEDDING_DIM,\n",
    "    weights = [embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "preds = layers.Dense(len(category), activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=128, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['loss'], label='loss')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, 15, 15), hist.history['accuracy'], label='accuracy')\n",
    "plt.plot(np.linspace(1, 15, 15), hist.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾ªç¯ç¥ç»ç½‘ç»œ\n",
    "\n",
    "\n",
    "$$h_t = \\sigma(W_{xh}x_t + W_{hh}h_{t-1} + b)$$\n",
    "åœ¨æ¯ä¸ªæ—¶é—´æˆ³$t$, ç½‘ç»œå±‚æ¥å—å½“å‰æ—¶é—´æˆ³çš„è¾“å…¥$x_t$å’Œä¸Šä¸€ä¸ªæ—¶é—´æˆ³çš„ç½‘ç»œçŠ¶æ€å‘é‡$h_{t-1}$,ç»è¿‡\n",
    "$$h_t = f_{\\theta}(h_{t-1}, x_t)$$\n",
    "å˜æ¢åå¾—åˆ°å½“å‰æ—¶é—´æˆ³çš„æ–°çŠ¶æ€å‘é‡$h_t$. åœ¨æ¯ä¸ªæ—¶é—´æˆ³ä¸Š, ç½‘ç»œå±‚å‡æœ‰è¾“å‡º$o_t = g_{\\phi}(h_t)$\n",
    "\n",
    "å¯¹äºè¿™ç§ç½‘ç»œç»“æ„ï¼Œæˆ‘ä»¬æŠŠå®ƒå«åšå¾ªç¯ç½‘ç»œç»“æ„(Recurrent Neural Networkï¼Œç®€ç§°RNN)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
