{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åå‘ä¼ æ’­ç®—æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['SimHei']# ['Noto Sans CJK JP']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
    "---\n",
    "### sigmodå‡½æ•°çš„å¯¼æ•°\n",
    "\n",
    "$$ Sigmoid(x) = \\sigma(x)= \\frac {1} {1+e^{-x}}$$\n",
    "å¯¼æ•°:\n",
    "$$\\frac d {dx} g(x) = \\frac {-1} {(1+e^{-x})^2} \\frac{d e^{-x}} {dx} = {\\frac{1}{1 + e^{-x}} (1-\\frac{1}{1 + e^{-x}})}=g(x)(1-g(x))$$\n",
    "\n",
    "æ³¨ï¼š\n",
    "\n",
    "å½“$z$ = 10æˆ–$z= -10$ ; $\\frac{d}{dz}g(z)\\approx0$\n",
    "\n",
    "å½“$z â€‹$= 0 , $\\frac{d}{dz}g(z)\\text{=g(z)(1-g(z))=}{1}/{4}â€‹$\n",
    "\n",
    "åœ¨ç¥ç»ç½‘ç»œä¸­$a= g(z)$; $g{{(z)}^{'}}=\\frac{d}{dz}g(z)=a(1-a)$\n",
    "ä¸€äº›å…¶ä»–æ€§è´¨:\n",
    "$$\n",
    "\\sigma(x) = \\frac {exp(x)}{exp(x) + exp(0)} \\\\\n",
    "1 - \\sigma(x) = \\sigma(-x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.plot(x, derivative(x), label='å¯¼æ•°')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "$$Relu(x) = max(0, x)$$\n",
    "\n",
    "å¯¼æ•°:\n",
    "$$\n",
    "g(z)^{'}=\n",
    "  \\begin{cases}\n",
    "  0&\t\\text{if z < 0}\\\\\n",
    "  1&\t\\text{if z > 0}\\\\\n",
    "undefined&\t\\text{if z = 0}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x>0, x, 0)\n",
    "\n",
    "\n",
    "def derivative(x):\n",
    "    d = np.array(x, copy=True)\n",
    "    d[x<0] = 0\n",
    "    d[x>=0] = 1\n",
    "    return d\n",
    "\n",
    "x = np.linspace(-6, 6, 100).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, relu(x), label='Relu')\n",
    "plt.plot(x, derivative(x), label='å¯¼æ•°')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "$$Leaky Relu(x) = max(px, x)$$\n",
    "å¯¼æ•°:\n",
    "$$\n",
    "g(z)^{'}=\n",
    "\\begin{cases}\n",
    "p& \t\\text{if z < 0}\\\\\n",
    "1&\t\\text{if z > 0}\\\\\n",
    "undefined&\t\\text{if z = 0}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, p=0.02):\n",
    "    return np.where(x>0, x, p*x)\n",
    "\n",
    "\n",
    "def derivative(x, p):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x<0] = p\n",
    "    return dx\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.figure()\n",
    "p = 0.1\n",
    "plt.plot(x, leaky_relu(x, p), label='Leaky Relu')\n",
    "plt.plot(x, derivative(x, p), label='å¯¼æ•°')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "$$\n",
    "tanh(x) = \\frac {e^x - e^{-x}} {e^x + e^{-x}} = 2 \\cdot sigmoid(2x) - 1\n",
    "$$\n",
    "\n",
    "å¯¼æ•°:\n",
    "$$\\frac{d}{{d}z}g(z) = \\frac {4e^z e^{-z}}{(e^z + e^{-z})^2}  =  1 - (tanh(z))^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return 2 * sigmoid(2*x) - 1\n",
    "\n",
    "def derivative(x):\n",
    "    return 1 - tanh(x) ** 2\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, tanh(x), label='Tanh')\n",
    "plt.plot(x, derivative(x), label='å¯¼æ•°')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŸå¤±å‡½æ•°çš„æ¢¯åº¦\n",
    "---\n",
    "### MSEå‡½æ•°æ¢¯åº¦\n",
    "å‡æ–¹å·®è¯¯å·®æŸå¤±å‡½æ•°:\n",
    "$$\n",
    "L = \\frac 1 2 \\sum_{k=1}^K (y_k - o_k)^2\n",
    "$$\n",
    "æ¢¯åº¦:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial {o_i}} = \\frac 1 2 \\sum_{k=1}^K \\frac {\\partial}{\\partial o_i}  (y_k - o_k)^2 \\\\ \n",
    "= \\frac 1 2 \\sum_{k=1}^K 2 \\cdot (y_k - o_k) \\cdot \\frac {\\partial (y_k - o_k)}{\\partial o_i} \\\\\n",
    "= \\sum_{k=1}^K (o_k - y_k)\\cdot \\frac {\\partial o_k} {\\partial o_i}\n",
    "$$\n",
    "\n",
    "å¯è§, $\\frac {\\partial o_k} {\\partial {o_i}}$ä»…å½“k=iæ—¶ä¸º1, å…¶å®ƒç‚¹éƒ½ä¸º0, ä¹Ÿå°±æ˜¯è¯´,åå¯¼æ•°$\\frac {\\partial L} {\\partial {o_i}}$åªä¸ç¬¬iå·èŠ‚ç‚¹ç›¸å…³, ä¸å…¶ä»–èŠ‚ç‚¹æ— å…³, æ‰€ä»¥å†™æˆ:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial {o_i}} = (o_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### äº¤å‰ç†µå‡½æ•°æ¢¯åº¦\n",
    "**Softmaxå‡½æ•°æ¢¯åº¦**\n",
    "$$\n",
    "p_i = \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "å®ƒçš„åŠŸèƒ½æ˜¯å°†ğ¾ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„å€¼è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå¹¶ä¿è¯æ¦‚ç‡ä¹‹å’Œä¸º1.\n",
    "- i = jæ—¶ã€‚Softmax å‡½æ•°çš„åå¯¼æ•°:\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\frac {\\partial \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}}{ \\partial z_j}\n",
    "= \\frac {e^{z_i} \\sum_{k=1}^K e^{z_k}  - e^{z_j}e^{z_i} } {(\\sum_{k=1}^K e^{z_k})^2}  \\\\ \n",
    "= \\frac {e^{z_i} (\\sum_{k=1}^K e^{z_k}  - e^{z_j})} {(\\sum_{k=1}^K e^{z_k})^2}\n",
    "= \\frac {e^{z_i}} {\\sum_{k=1}^K e^{z_k}} \\times \\frac {\\sum_{k=1}^K e^{z_k}  - e^{z_j}} {\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "å¯ä»¥çœ‹åˆ°, ä¸Šå¼ä¸º$p_i$å’Œ$1-p_j$ç›¸ä¹˜, å½“$i = j$æ—¶,\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = p_i(1-p_j), i=j\n",
    "$$\n",
    "- $i \\neq j$æ—¶,\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\frac {\\partial \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}}{ \\partial z_j} \n",
    "= \\frac { 0 - e^{z_j}e^{z_i} } {(\\sum_{k=1}^K e^{z_k})^2} \\\\\n",
    "= \\frac {-e^{z_j}} {\\sum_{k=1}^K e^{z_k}} \\times \\frac {e^{z_i}} {\\sum_{k=1}^K e^{z_k}}\n",
    "= - p_j \\cdot p_i\n",
    "$$\n",
    "\n",
    "ç»¼ä¸Š, Softmaxå‡½æ•°çš„æ¢¯åº¦è¡¨è¾¾å¼:\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\begin{cases}p_i(1-p_j), \\quad i=j \\\\\n",
    "-p_i\\cdot p_j, \\quad i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**äº¤å‰ç†µæ¢¯åº¦**\n",
    "\n",
    "[äº¤å‰ç†µæŸå¤±å‡½æ•°](https://blog.csdn.net/tsyccnh/article/details/79163834)\n",
    "\n",
    "ç¥ç»å…ƒçš„è¾“å‡ºå°±æ˜¯ $a = \\sigma(z)$ï¼Œå…¶ä¸­$z=\\sum w_{j}x_{j}+b$\n",
    "\n",
    "æŸå¤±å‡½æ•°:\n",
    "$$C=-\\frac{1}{n}\\sum[ylna+(1-y)ln(1-a)]$$\n",
    "\n",
    "$$\n",
    "L = -\\sum_k y_klog(p_k)\n",
    "$$\n",
    "è¿™é‡Œç›´æ¥æ¥æ¨å¯¼æœ€ç»ˆæŸå¤±å€¼Lå¯¹ç½‘ç»œè¾“å‡ºlogits å˜é‡$z_i$çš„åå¯¼æ•°ï¼Œå±•å¼€ä¸º\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = -\\sum_k y_k \\frac {\\partial log(p_k)}{\\partial z_i}\\\\\n",
    "=  -\\sum_k y_k \\frac {\\partial log(p_k)}{\\partial p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i} \\\\\n",
    "= -\\sum_k y_k \\frac 1 {p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i}\n",
    "$$\n",
    "ä¸ä¸Šé¢çš„Softmax å¤„ç†ç±»ä¼¼, å°†æ±‚å’Œç¬¦å·æ‹†åˆ†ä¸º ğ‘˜ = ğ‘– ä»¥åŠğ‘˜ â‰  ğ‘–çš„ä¸¤ç§æƒ…å†µ:\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = y_i \\frac 1 {p_i} \\cdot \\frac {\\partial p_i}{\\partial z_i} + \n",
    "-\\sum_{k\\neq i} y_k \\frac 1 {p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i} \\\\\n",
    "= -y_i(1-p_i) - \\sum_{k\\neq i} y_k \\frac 1 {p_k}(-p_k \\cdot p_i) \\\\\n",
    "= p_i(y_i + \\sum_{k\\neq i} y_k)- y_i\n",
    "$$\n",
    "ç‰¹åˆ«åœ°ï¼Œå¯¹äºåˆ†ç±»é—®é¢˜ä¸­æ ‡ç­¾ğ‘¦é€šè¿‡One-hot ç¼–ç çš„æ–¹å¼ï¼Œåˆ™æœ‰å¦‚ä¸‹å…³ç³»:\n",
    "$$\n",
    "\\sum_{k} y_k = 1 \\\\\n",
    "y_i + \\sum_{k\\neq i} y_k = 1\n",
    "$$\n",
    "æ‰€ä»¥äº¤å‰ç†µçš„åå¯¼æ•°å¯ä»¥è¿›ä¸€æ­¥åŒ–ç®€:\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = p_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…¨è¿æ¥å±‚æ¢¯åº¦\n",
    "ä»¥å…¨è¿æ¥å±‚ç½‘ç»œã€æ¿€æ´»å‡½æ•°é‡‡ç”¨Sigmoid å‡½æ•°ã€è¯¯å·®å‡½æ•°ä¸ºSoftmax+MSE æŸå¤±å‡½æ•°çš„ç¥ç»ç½‘ç»œä¸ºä¾‹ï¼Œæ¨å¯¼å…¶æ¢¯åº¦ä¼ æ’­è§„å¾‹\n",
    "### å•ç¥ç»å…ƒæ¢¯åº¦\n",
    "å¯¹äºé‡‡ç”¨Sigmoid æ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒæ¨¡å‹ï¼Œå®ƒçš„æ•°å­¦æ¨¡å‹å¯ä»¥å†™ä¸ºï¼š\n",
    "$$\n",
    "o^{(1)} = \\sigma(w^{(1)^T} x + b^{(1)})\n",
    "$$\n",
    "å…¶ä¸­å˜é‡çš„ä¸Šæ ‡è¡¨ç¤ºå±‚æ•°, $o^{(1)}$è¡¨ç¤ºç¬¬ä¸€å±‚çš„è¾“å‡º\n",
    "\n",
    "![](./å•ç¥ç»å…ƒæ¢¯åº¦.png)\n",
    "\n",
    "å¦‚æœæˆ‘ä»¬é‡‡ç”¨å‡æ–¹è¯¯å·®å‡½æ•°ï¼Œè€ƒè™‘åˆ°å•ä¸ªç¥ç»å…ƒåªæœ‰ä¸€ä¸ªè¾“å‡º$o_1^{(1)}$ï¼Œé‚£ä¹ˆæŸå¤±å¯ä»¥è¡¨è¾¾ä¸ºï¼š\n",
    "$$L = \\frac 1 2 (o^{(1)}_1 - t)^2 = \\frac 1 2 (o_1 - t)^2$$\n",
    "ä»¥æƒå€¼å‚æ•°$w_{j1}$ä¸ºä¾‹, åå¯¼æ•°:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{j1}} = (o_1 - t)\\frac {\\partial o_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t) \\frac {\\partial \\sigma(z_1)}{\\partial z_{1}} \\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t)\\sigma(z_1)(1-\\sigma(z_1))\\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t)o_1(1 - o_1)\\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "=  (o_1 - t)o_1(1 - o_1)x_{j}\n",
    "$$\n",
    "å¯è§, è¯¯å·®å¯¹æƒå€¼$w_{j1}$çš„åå¯¼åªä¸è¾“å‡ºå€¼$o_1$, çœŸå®å€¼tä»¥åŠå½“å‰æƒå€¼è¿æ¥çš„è¾“å…¥$x_j$æœ‰å…³\n",
    "\n",
    "    ä½¿ç”¨MSEæŸå¤±å‡½æ—¶, åå¯¼æ•°å—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°å½±å“ï¼Œsigmoidå‡½æ•°å¯¼æ•°åœ¨è¾“å‡ºæ¥è¿‘0å’Œ1æ—¶éå¸¸å°ï¼Œä¼šå¯¼è‡´ä¸€äº›å®ä¾‹åœ¨åˆšå¼€å§‹è®­ç»ƒæ—¶å­¦ä¹ å¾—éå¸¸æ…¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…¨è¿æ¥å±‚æ¢¯åº¦\n",
    "ä»¬æŠŠå•ä¸ªç¥ç»å…ƒæ¨¡å‹æ¨å¹¿åˆ°å•å±‚çš„å…¨è¿æ¥å±‚çš„ç½‘ç»œä¸Š\n",
    "\n",
    "![](./å…¨è¿æ¥å±‚æ¢¯åº¦.png)\n",
    "\n",
    "å…¨è¿æ¥å±‚çš„å‡æ–¹å·®è¯¯å·®:\n",
    "$$\n",
    "L = \\frac 1 2 \\sum_{i=1}^K(o_i^{(1)} - t_i)^2\n",
    "$$\n",
    "å¯¹æƒå€¼$w_{jk}$çš„åå¯¼æ•°åªä¸è¾“å‡ºèŠ‚ç‚¹$o_k^{(1)}$æœ‰å…³, å¯ä»¥å»é™¤æ±‚å’Œç¬¦å·, å³$i = k$:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}} = (o_k - t_k) \\frac {\\partial o_k}{ \\partial w_{jk}} \\\\\n",
    "= (o_k -t_k)o_k(1-o_k)\\frac {\\partial z_k}{\\partial w_jk} \\\\\n",
    "= (o_k -t_k)o_k(1-o_k)x_j\n",
    "$$\n",
    "ä»¤$\\delta_k = (o_k -t_k)o_k(1-o_k)$, åˆ™:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}} = \\delta_k x_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°Læ¥è‡ªå„ä¸ªè¾“å‡ºèŠ‚ç‚¹$o_k^{K}$, è€Œå…¶åˆä¸éšè—å±‚çš„è¾“å‡ºèŠ‚ç‚¹$o_j^{(J)}$ç›¸å…³è”.\n",
    "\n",
    "![](./æ¢¯åº¦ä¼ æ’­.png)\n",
    "\n",
    "æ ¹æ®é“¾å¼æ±‚å¯¼æ³•åˆ™\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \n",
    "\\frac {\\partial L}{\\partial o_{j}^{(J)}} \\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}} = \n",
    "\\frac {\\partial L}{\\partial o_{k}^{(K)}}  \\frac {\\partial o_{k}^{(K)}}{\\partial o_{j}^{(J)}} \\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}}\n",
    "$$\n",
    "$\\frac {\\partial L}{\\partial o_{k}^{(K)}}$å¯ä»¥ä»è¯¯å·®å‡½æ•°ä¸­ç›´æ¥æ¨åˆ°å‡º, $\\frac {\\partial o_{k}^{(K)}}{\\partial o_{j}^{(J)}}$å¯ä»¥ç”±å…¨è¿æ¥å±‚å…¬å¼æ¨å¯¼, $\\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}}\n",
    "$çš„å¯¼æ•°å³ä¸ºè¾“å…¥$x^{(I)}_i$\n",
    "\n",
    "é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œç›´æ¥å¯ä»¥å°†åå¯¼æ•°è¿›è¡Œåˆ†è§£ï¼Œå±‚å±‚è¿­ä»£å³å¯æ¨å¯¼å‡º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1.)\n",
    "w1 = tf.constant(2.)\n",
    "b1 = tf.constant(1.)\n",
    "w2 = tf.constant(2.)\n",
    "b2 = tf.constant(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºæ¢¯åº¦è®°å½•å™¨\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # é Variableç±»å‹ æ‰‹åŠ¨è®°å½•æ¢¯åº¦ä¿¡æ¯  \n",
    "    tape.watch([w1, b1, w2, b2])\n",
    "    y1 = x * w1 + b1\n",
    "    y2 = y1 * w2 + b2\n",
    "    \n",
    "# ç‹¬ç«‹æ±‚è§£å‡ºå„ä¸ªåå¯¼æ•°\n",
    "dy_2_dy1 = tape.gradient(y2, [y1])[0]\n",
    "dy_1_dw1 = tape.gradient(y1, [w1])[0]\n",
    "dy_2_dw1 = tape.gradient(y2, [w1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dy_2_dw1)\n",
    "print(dy_2_dy1 * dy_1_dw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åå‘ä¼ æ’­\n",
    "---\n",
    "è¾“å‡ºå±‚çš„åå¯¼å…¬å¼\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}}\n",
    "= (o_k -t_k)o_k(1-o_k)x_j = \\delta_k x_j\n",
    "$$\n",
    "\n",
    "![](./åå‘ä¼ æ’­.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¡ç®—å€’æ•°ç¬¬äºŒå±‚çš„åå¯¼æ•°$\\frac {L}{w_ij^{(J)}}$\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \\frac {\\partial}{ \\partial w_{ij}^{(J)}} \\frac 1 2 \\sum_k(o_k-t_k)^2 \\\\\n",
    "= \\sum_k(o_k-t_k) \\frac {\\partial o_k}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) \\frac {\\partial \\sigma(z_k)}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}}\\\\\n",
    "= \\sum_k(o_k-t_k) \\sigma(z_k) (1 - \\sigma(z_k)) \\frac {\\partial z_k}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\ \n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) \\frac {\\partial z_k}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\ \n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\frac {\\partial \\sigma(z_j)}{ \\partial z_{j}}  \n",
    "\\frac {\\partial z_j}{ \\partial w_{ij}}\\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\sigma(z_j)(1-\\sigma(z_j))\\frac {\\partial z_j}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} o_j(1-o_j)\\frac {\\partial z_j}{ \\partial w_{ij}} \\\\\n",
    "= o_j(1-o_j)\\frac {\\partial z_j}{ \\partial w_{ij}} \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\\\\n",
    "= o_j(1-o_j)o_i \\sum_k \\delta_k^{(k)}w_{jk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šä¹‰\n",
    "$$ \n",
    "\\delta^{(J)}_j \\triangleq o_j(1-o_j)\\sum_k \\delta_k^{(K)}w_{jk} \n",
    "$$\n",
    "åˆ™ä¸è¾“å‡ºå±‚åå¯¼æ•°ç±»ä¼¼çš„:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \\delta^{(J)}_j o_i^{(I)} \n",
    "$$\n",
    "$\\delta$å¯ä»¥ç®€å•ç†è§£ä¸ºå½“å‰è¿æ¥$w_{ij}$å¯¹è¯¯å·®å‡½æ•°çš„è´¡çŒ®å€¼\n",
    "\n",
    "ä¼ æ’­è§„å¾‹:\n",
    "è¾“å‡ºå±‚:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{jk}} = \\delta_k^{(K)}o_j \\\\\n",
    "\\delta_k^{(K)} = o_k(1-o_k)(o_k -t_k)\n",
    "$$\n",
    "å€’æ•°ç¬¬äºŒå±‚:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}} = \\delta_j^{(J)}o_i \\\\\n",
    "\\delta_j^{(J)} = o_j(1-o_j)\\sum_k \\delta_k^{(K)}w_{jk} \n",
    "$$\n",
    "å€’æ•°ç¬¬ä¸‰å±‚:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ni}} = \\delta_i^{(I)}o_n \\\\\n",
    "\\delta_i^{(I)} = o_i(1-o_i)\\sum_j \\delta_j^{(J)}w_{ij} \n",
    "$$\n",
    "$o_n$æ˜¯å€’æ•°ç¬¬ä¸‰å±‚çš„è¾“å…¥, å³å€’æ•°ç¬¬å››å±‚çš„è¾“å‡º.\n",
    "æŒ‰ç…§è¿™ä¸ªè§„å¾‹, åªéœ€è¦å¾ªç¯è¿­ä»£è®¡ç®—æ¯ä¸€æ¬¡æ¯ä¸ªèŠ‚ç‚¹çš„$\\delta_k^{(K)}, \\delta_j^{(J)} , \\delta_i^{(I)}$ç­‰å€¼å³å¯æ±‚å¾—å½“å‰å±‚çš„åå¯¼æ•°, ä»è€Œå¾—åˆ°æ¯å±‚æƒå€¼çŸ©é˜µWçš„æ¢¯åº¦,åœ¨é€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•è¿­ä»£ä¼˜åŒ–ç½‘ç»œå‚æ•°å³å¯.\n",
    "\n",
    "[è®¡ç®—å›¾(Computational Graph) 1](https://blog.csdn.net/xbinworld/article/details/56523063)\n",
    "\n",
    "[è®¡ç®—å›¾(Computational Graph) 2](https://samaelchen.github.io/deep_learning_step2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Himmelblau å‡½æ•°ä¼˜åŒ–å®æˆ˜\n",
    "Himmelblau å‡½æ•°æ˜¯ç”¨æ¥æµ‹è¯•ä¼˜åŒ–ç®—æ³•çš„å¸¸ç”¨æ ·ä¾‹å‡½æ•°ä¹‹ä¸€:\n",
    "$$\n",
    "f(x, y) = (x^2+y-11)^2 + (x + y^2 - 7)^2\n",
    "$$\n",
    "\n",
    "åˆ©ç”¨ TensorFlow è‡ªåŠ¨æ±‚å¯¼æ¥æ±‚å‡ºå‡½æ•°åœ¨ğ‘¥å’Œğ‘¦çš„åå¯¼æ•°ï¼Œå¹¶å¾ªç¯è¿­ä»£æ›´æ–°ğ‘¥å’Œğ‘¦å€¼ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x):\n",
    "    # x: 2ä¸ªå…ƒç´ çš„list [x,y]\n",
    "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 -7) ** 2\n",
    "\n",
    "x, y = np.mgrid[-6:6:0.01, -6:6:0.01]\n",
    "z = himmelblau([x, y])\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=plt.get_cmap('hot'))\n",
    "ax.view_init(60, -30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.contour(x, y, z, levels=100 ,cmap=plt.get_cmap('rainbow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([4., 0.])\n",
    "def calcu(x):\n",
    "    for step in range(200):\n",
    "        with tf.GradientTape() as tape:  # æ¢¯åº¦è·Ÿè¸ª\n",
    "            tape.watch([x])  # éVariable æ‰‹åŠ¨ åŠ å…¥æ¢¯åº¦è·Ÿè¸ªåˆ—è¡¨ \n",
    "            y = himmelblau(x)\n",
    "        # åå‘ä¼ æ’­\n",
    "        grads = tape.gradient(y, [x])[0]  # yå¯¹xçš„æ¢¯åº¦\n",
    "        # æ›´æ–°å‚æ•°\n",
    "        x -= 0.01 * grads\n",
    "        if step % 20 == 19:\n",
    "            print(f'step {step}: x={x.numpy()}, f(x)={y.numpy()}')\n",
    "calcu(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tf.constant([-3.,-3.])\n",
    "calcu(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = tf.constant([-3., 2.])\n",
    "calcu(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = tf.constant([3., 1.])\n",
    "calcu(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸åŒåˆå§‹å€¼å¾—åˆ°å¤šç§æå°å€¼æ•°å€¼è§£ã€‚å‚æ•°çš„åˆå§‹åŒ–çŠ¶æ€æ˜¯å¯èƒ½å½±å“æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æœç´¢è½¨è¿¹çš„ï¼Œç”šè‡³æœ‰å¯èƒ½æœç´¢å‡ºå®Œå…¨ä¸åŒçš„æ•°å€¼è§£."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åå‘ä¼ æ’­å®æˆ˜\n",
    "å®ç°ä¸€ä¸ª4 å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œæ¥å®ŒæˆäºŒåˆ†ç±»ä»»åŠ¡ã€‚ç½‘ç»œè¾“å…¥èŠ‚ç‚¹æ•°ä¸º2ï¼Œéšè—å±‚çš„èŠ‚ç‚¹æ•°è®¾è®¡ä¸ºï¼š25ã€50å’Œ25ï¼Œè¾“å‡ºå±‚ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œåˆ†åˆ«è¡¨ç¤ºå±äºç±»åˆ«1 çš„æ¦‚ç‡å’Œç±»åˆ«2çš„æ¦‚ç‡.\n",
    "\n",
    "åˆ©ç”¨å‰é¢ä»‹ç»çš„å¤šå±‚å…¨è¿æ¥ç½‘ç»œçš„æ¢¯åº¦æ¨å¯¼ç»“æœï¼Œç›´æ¥åˆ©ç”¨Python å¾ªç¯è®¡ç®—æ¯ä¸€å±‚çš„æ¢¯åº¦ï¼Œå¹¶æŒ‰ç€æ¢¯åº¦ä¸‹é™ç®—æ³•æ‰‹åŠ¨æ›´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "N_SAMPLES = 3000\n",
    "TEST_SIZE = 0.2\n",
    "X, y = make_moons(n_samples=N_SAMPLES, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "# 0.6 - 0.2 -0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], s=10, label='1')\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], s=10, label='0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[np.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10.0)\n",
    "np.array_split(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨åˆ›å»ºå…¨è¿æ¥å±‚\n",
    "class Layer:\n",
    "    # å‚æ•°: è¾“å…¥èŠ‚ç‚¹æ•° ,ç¥ç»å•å…ƒè¾“å‡ºèŠ‚ç‚¹æ•°, æ¿€æ´»å‡½æ•°, æƒå€¼, åç½® \n",
    "    def __init__(self, n_input, n_neurons, activation=None, weights=None, bias=None):\n",
    "        self.weights = weights if weights is not None else np.random.randn(\n",
    "            n_input, n_neurons) * np.sqrt(1 / n_neurons)\n",
    "        self.bias = bias if bias is not None else np.random.randn(n_neurons) * 0.1\n",
    "        self.activation = activation\n",
    "        self.activation_output = None  # è¿›è¿‡æ¿€æ´»å‡½æ•°åçš„è¾“å‡º\n",
    "        self.error = None  # è®¡ç®—delta çš„ä¸­é—´é‡\n",
    "        self.delta = None  # æ¯ä¸€å±‚çš„delta, ç”¨æ¥è®¡ç®—æ¢¯åº¦\n",
    "    \n",
    "    def activate(self, x):\n",
    "        # x: (b, d_in) w: (d_in, d_out) b: \n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        self.activation_out = self._apply_activation(r)\n",
    "        return self.activation_out\n",
    "    \n",
    "    def _apply_activation(self, r):\n",
    "        # æ¿€æ´»å‡½æ•°\n",
    "        if self.activation is None:\n",
    "            # æ’ç­‰æ¿€åŠ±\n",
    "            return r \n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(r, 0)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        else:\n",
    "            return r\n",
    "    def apply_activation_derivative(self, r):\n",
    "        # æ±‚æ¿€æ´»å‡½æ•°çš„å¯¼æ•°\n",
    "        if self.activation is None:\n",
    "            # å¸¸æ•°1\n",
    "            return np.ones_like(r)\n",
    "        elif self.activation == 'relu':\n",
    "            d = np.array(r, copy=True)\n",
    "            d[r < 0] = 0.\n",
    "            d[r >= 0] = 1.\n",
    "            return d \n",
    "        elif self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        else:\n",
    "            return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X:(m, n_features)\n",
    "\n",
    "ç¬¬Kå±‚(è¾“å‡ºå±‚): kä¸ªå•å…ƒ $W^{[K]} = (j, k), \\quad b^{[K]} = (k, ), \\quad A^{[K]} = g^{[K]}(Z^{[K]}) = g^{[K]}(A^{[J]}@W^{[K]} + b^{[K]}) =(m, k)$\n",
    "\n",
    "$dZ^{[K]} = dA^{[K]} * g'^{[K]} \\quad (m, k)$\n",
    "\n",
    "$dW^{[K]} =  A^{[J]}.T @ dZ^{[K]}  \\quad (j, k)$\n",
    "\n",
    "$db^{[K]} = np.mean(dZ^{[K]})  \\quad (k, )$\n",
    "\n",
    "ç¬¬Jå±‚(å€’æ•°ç¬¬äºŒå±‚): Jä¸ªå•å…ƒ $W^{[J]} = (i, j), \\quad b^{[J]} = (j, ), \\quad A^{[J]} = g^{[J]}(Z^{[J]}) = g^{[J]}(A^{[I]}@W^{[J]} + b^{[J]}) =(m, j)$ \n",
    "\n",
    "$dA^{[J]} = dZ^{[K]} @ W^{[K]}.T \\quad (m, j)$\n",
    "\n",
    "$dZ^{[J]} = dA^{[J]} * g'^{[J]} \\quad (m, j)$\n",
    "\n",
    "$dW^{[J]} =  A^{[I]}.T @ dZ^{[J]}  \\quad (i, j)$\n",
    "\n",
    "$db^{[J]} = np.mean(dZ^{[J]})  \\quad (j, )$\n",
    "\n",
    "\n",
    "ç¬¬Iå±‚(...): Iä¸ªå•å…ƒ $W^{[I]} = (n\\_features, i), \\quad b^{[J]} = (i, ) \\quad A^{[I]} = g^{[I]}(Z^{[I]}) = g^{[I]}(A^{[n]}@W^{[I]} + b^{[I]}) =(m, i)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¥ç»ç½‘ç»œ\n",
    "# å‘é‡åŒ– ä½¿ç”¨å°æ‰¹é‡æ¢¯åº¦ä¸‹é™\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # å­˜æ”¾ä¸Šé¢å®šä¹‰çš„å…¨è¿æ¥å±‚\n",
    "        self.layers = []\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # å‰å‘ä¼ æ’­, é€æ¸é€šè¿‡æ¯ä¸€å±‚\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X  # è¾“å‡ºå±‚  1ä¸ªèŠ‚ç‚¹ sigmoid \n",
    "    \n",
    "    def loss(self, y_true, y_preds):\n",
    "        # äº¤å‰ç†µè¯¯å·®\n",
    "        loss_ = - y_true* np.log(y_preds) - (1 - y_true)*np.log(1 - y_preds)  # (m, 1)\n",
    "        return np.mean(loss_, axis=0)\n",
    "    \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        # åå‘ä¼ æ’­ç®—æ³• è®¡ç®—æ¯ä¸€å±‚çš„delta \n",
    "        # å‰å‘è®¡ç®— å¾—åˆ°è¾“å‡ºå€¼\n",
    "        m = X.shape[0]  # batch size\n",
    "        out = self.feed_forward(X)  # (m, 1)\n",
    "        # loss_ = self.loss(y, out)\n",
    "        # print(loss)\n",
    "        for i in reversed(range(len(self.layers))):  # ä»æœ€åä¸€å±‚å¼€å§‹\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:  # è¾“å‡ºå±‚\n",
    "                # ä½¿ç”¨ äº¤å‰ç†µ è¯¯å·®\n",
    "                # layer.error = out - y  # è¯¯å·®çš„å¯¼æ•°\n",
    "                #layer.delta = layer.error * layer.apply_activation_derivative(out)\n",
    "                layer.error = -y / out + (1-y)/(1-out)  # dL/dA^K  # (m, 1)\n",
    "                # print('loss:', layer.error.shape, y.shape, out.shape)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(out) # dL/dZ^K  (m, 1) (m, 1)\n",
    "            else:  # éšè—å±‚\n",
    "                next_layer = self.layers[i + 1]\n",
    "#                 layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "#                 layer.delta = layer.error * layer.apply_activation_derivative(layer.activation_out)\n",
    "                layer.error = next_layer.delta @ next_layer.weights.T  # dL/dA^J (m, 1) (1, k)\n",
    "                # dL/dZ^J (m, k) (m,k)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.activation_out)\n",
    "        \n",
    "        # æ›´æ–°å‚æ•°\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            # ä¸Šä¸€å±‚çš„è¾“å‡º æœ¬å±‚çš„è¾“å…¥\n",
    "            o_i = np.atleast_2d(X if i == 0 else self.layers[i-1].activation_out)\n",
    "            # weights (I, J)\n",
    "            # o_i (m, I)\n",
    "            # delta (m, J)\n",
    "            layer.weights -= learning_rate *  o_i.T @ layer.delta  # (I, J)\n",
    "            layer.bias -=  learning_rate * np.mean(layer.delta)  # (J, )\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.01 ,max_epochs=300):\n",
    "        # y onehot å¤„ç†\n",
    "#         y_onehot = np.zeros((y.shape[0], 2))\n",
    "#         y_onehot[np.arange(y.shape[0]),  y] =1\n",
    "        y = y.reshape(-1, 1)\n",
    "        cross_entropy = []\n",
    "        batch_size = 32\n",
    "        split_size = X.shape[0] // batch_size\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            for x, y_true in zip(np.array_split(X, split_size), np.array_split(y, split_size)):\n",
    "            # for j in range(X.shape[0]):  # æ¯æ¬¡è®­ç»ƒä¸€ä¸ªæ ·æœ¬\n",
    "                self.backpropagation(x, y_true, learning_rate)\n",
    "            if epoch % 10 == 0:\n",
    "                # mse = np.mean(np.square(self.feed_forward(X) - y_onehot))\n",
    "                loss = self.loss(y, self.feed_forward(X))\n",
    "                cross_entropy.append(loss)\n",
    "                print(f'Epoch: {epoch}, cross_entropy: {loss}')\n",
    "        return cross_entropy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.feed_forward(X)\n",
    "        y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "        # out = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.add_layer(Layer(2, 25, 'tanh'))  # h1  \n",
    "model.add_layer(Layer(25, 50, 'tanh'))  # h2 \n",
    "model.add_layer(Layer(50, 25, 'tanh'))  # h3\n",
    "model.add_layer(Layer(25, 1, 'sigmoid'))  # output\n",
    "cross_entropy = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# y_pred\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1, 0, 1, 1])\n",
    "y_onehot = np.zeros((y.shape[0], 2))\n",
    "y_onehot[np.arange(y.shape[0]),  y] =1\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä½¿ç”¨tensorflow**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, losses, Model\n",
    "\n",
    "class Network(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Sequential([\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(50, activation='relu'),\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(2),\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # åœ¨å‰å‘è®¡ç®—å‡½æ•° call ä¸­å®ç°è‡ªå®šä¹‰ç½‘ç»œç±»çš„è®¡ç®—é€»è¾‘å³å¯\n",
    "        out = self.model(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "# é€šè¿‡ build å‡½æ•°å®Œæˆå†…éƒ¨å¼ é‡çš„åˆ›å»º (batch_size, n_input_features)\n",
    "network.build(input_shape=(32, 2))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.build?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * 25 + 25 + 25 * 50 + 50 + 50* 25 + 25 + 25* 2 +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = tf.one_hot(y_train, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train_onehot))\n",
    "train_db = train_db.shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape?\n",
    "```\n",
    "tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "persistent :é»˜è®¤æƒ…å†µä¸‹ä¸ºFalseï¼Œè¿™æ„å‘³ç€æœ€å¤šå¯ä»¥åœ¨æ­¤å¯¹è±¡ä¸Šå¯¹gradient()æ–¹æ³•è¿›è¡Œä¸€æ¬¡è°ƒç”¨ã€‚\n",
    "watch_accessed_variables: è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰ trainable å¯¹è±¡\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.01\n",
    "train_losses = []  # Relu - äº¤å‰ç†µæŸå¤±\n",
    "accurancy = []\n",
    "for epoch in range(100):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = network(x)\n",
    "            loss = losses.categorical_crossentropy(y, out, from_logits=True)\n",
    "        grads = tape.gradient(loss, network.trainable_variables)\n",
    "        \n",
    "        for p, g in zip(network.trainable_variables, grads):\n",
    "            p.assign_sub(learn_rate * g)\n",
    "    if epoch % 5 == 0: \n",
    "        out = network(X_test)\n",
    "        y_pred = np.argmax(out, axis=1)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(epoch, acc)\n",
    "        accurancy.append(acc)\n",
    "        # train_losses.append(loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(accurancy)) * 5, accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
