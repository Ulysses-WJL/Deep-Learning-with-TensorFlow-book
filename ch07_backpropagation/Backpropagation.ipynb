{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Default parameters for plots\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "matplotlib.rcParams['figure.titlesize'] = 20\n",
    "matplotlib.rcParams['figure.figsize'] = [9, 7]\n",
    "matplotlib.rcParams['font.family'] = ['SimHei']# ['Noto Sans CJK JP']\n",
    "matplotlib.rcParams['axes.unicode_minus']=False \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数的导数\n",
    "---\n",
    "### sigmod函数的导数\n",
    "\n",
    "$$ Sigmoid(x) = \\sigma(x)= \\frac {1} {1+e^{-x}}$$\n",
    "导数:\n",
    "$$\\frac d {dx} g(x) = \\frac {-1} {(1+e^{-x})^2} \\frac{d e^{-x}} {dx} = {\\frac{1}{1 + e^{-x}} (1-\\frac{1}{1 + e^{-x}})}=g(x)(1-g(x))$$\n",
    "\n",
    "注：\n",
    "\n",
    "当$z$ = 10或$z= -10$ ; $\\frac{d}{dz}g(z)\\approx0$\n",
    "\n",
    "当$z ​$= 0 , $\\frac{d}{dz}g(z)\\text{=g(z)(1-g(z))=}{1}/{4}​$\n",
    "\n",
    "在神经网络中$a= g(z)$; $g{{(z)}^{'}}=\\frac{d}{dz}g(z)=a(1-a)$\n",
    "一些其他性质:\n",
    "$$\n",
    "\\sigma(x) = \\frac {exp(x)}{exp(x) + exp(0)} \\\\\n",
    "1 - \\sigma(x) = \\sigma(-x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.plot(x, derivative(x), label='导数')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "$$Relu(x) = max(0, x)$$\n",
    "\n",
    "导数:\n",
    "$$\n",
    "g(z)^{'}=\n",
    "  \\begin{cases}\n",
    "  0&\t\\text{if z < 0}\\\\\n",
    "  1&\t\\text{if z > 0}\\\\\n",
    "undefined&\t\\text{if z = 0}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x>0, x, 0)\n",
    "\n",
    "\n",
    "def derivative(x):\n",
    "    d = np.array(x, copy=True)\n",
    "    d[x<0] = 0\n",
    "    d[x>=0] = 1\n",
    "    return d\n",
    "\n",
    "x = np.linspace(-6, 6, 100).reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, relu(x), label='Relu')\n",
    "plt.plot(x, derivative(x), label='导数')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "$$Leaky Relu(x) = max(px, x)$$\n",
    "导数:\n",
    "$$\n",
    "g(z)^{'}=\n",
    "\\begin{cases}\n",
    "p& \t\\text{if z < 0}\\\\\n",
    "1&\t\\text{if z > 0}\\\\\n",
    "undefined&\t\\text{if z = 0}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, p=0.02):\n",
    "    return np.where(x>0, x, p*x)\n",
    "\n",
    "\n",
    "def derivative(x, p):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x<0] = p\n",
    "    return dx\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.figure()\n",
    "p = 0.1\n",
    "plt.plot(x, leaky_relu(x, p), label='Leaky Relu')\n",
    "plt.plot(x, derivative(x, p), label='导数')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh\n",
    "$$\n",
    "tanh(x) = \\frac {e^x - e^{-x}} {e^x + e^{-x}} = 2 \\cdot sigmoid(2x) - 1\n",
    "$$\n",
    "\n",
    "导数:\n",
    "$$\\frac{d}{{d}z}g(z) = \\frac {4e^z e^{-z}}{(e^z + e^{-z})^2}  =  1 - (tanh(z))^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return 2 * sigmoid(2*x) - 1\n",
    "\n",
    "def derivative(x):\n",
    "    return 1 - tanh(x) ** 2\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, tanh(x), label='Tanh')\n",
    "plt.plot(x, derivative(x), label='导数')\n",
    "ax = plt.gca()\n",
    "ax.spines['left'].set_position(('data', 0))\n",
    "ax.spines['bottom'].set_position(('data', 0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数的梯度\n",
    "---\n",
    "### MSE函数梯度\n",
    "均方差误差损失函数:\n",
    "$$\n",
    "L = \\frac 1 2 \\sum_{k=1}^K (y_k - o_k)^2\n",
    "$$\n",
    "梯度:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial {o_i}} = \\frac 1 2 \\sum_{k=1}^K \\frac {\\partial}{\\partial o_i}  (y_k - o_k)^2 \\\\ \n",
    "= \\frac 1 2 \\sum_{k=1}^K 2 \\cdot (y_k - o_k) \\cdot \\frac {\\partial (y_k - o_k)}{\\partial o_i} \\\\\n",
    "= \\sum_{k=1}^K (o_k - y_k)\\cdot \\frac {\\partial o_k} {\\partial o_i}\n",
    "$$\n",
    "\n",
    "可见, $\\frac {\\partial o_k} {\\partial {o_i}}$仅当k=i时为1, 其它点都为0, 也就是说,偏导数$\\frac {\\partial L} {\\partial {o_i}}$只与第i号节点相关, 与其他节点无关, 所以写成:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial {o_i}} = (o_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵函数梯度\n",
    "**Softmax函数梯度**\n",
    "$$\n",
    "p_i = \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "它的功能是将𝐾个输出节点的值转换为概率，并保证概率之和为1.\n",
    "- i = j时。Softmax 函数的偏导数:\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\frac {\\partial \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}}{ \\partial z_j}\n",
    "= \\frac {e^{z_i} \\sum_{k=1}^K e^{z_k}  - e^{z_j}e^{z_i} } {(\\sum_{k=1}^K e^{z_k})^2}  \\\\ \n",
    "= \\frac {e^{z_i} (\\sum_{k=1}^K e^{z_k}  - e^{z_j})} {(\\sum_{k=1}^K e^{z_k})^2}\n",
    "= \\frac {e^{z_i}} {\\sum_{k=1}^K e^{z_k}} \\times \\frac {\\sum_{k=1}^K e^{z_k}  - e^{z_j}} {\\sum_{k=1}^K e^{z_k}}\n",
    "$$\n",
    "可以看到, 上式为$p_i$和$1-p_j$相乘, 当$i = j$时,\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = p_i(1-p_j), i=j\n",
    "$$\n",
    "- $i \\neq j$时,\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\frac {\\partial \\frac {e^{z_i}}{\\sum_{k=1}^K e^{z_k}}}{ \\partial z_j} \n",
    "= \\frac { 0 - e^{z_j}e^{z_i} } {(\\sum_{k=1}^K e^{z_k})^2} \\\\\n",
    "= \\frac {-e^{z_j}} {\\sum_{k=1}^K e^{z_k}} \\times \\frac {e^{z_i}} {\\sum_{k=1}^K e^{z_k}}\n",
    "= - p_j \\cdot p_i\n",
    "$$\n",
    "\n",
    "综上, Softmax函数的梯度表达式:\n",
    "$$\n",
    "\\frac {\\partial p_i}{\\partial z_j} = \\begin{cases}p_i(1-p_j), \\quad i=j \\\\\n",
    "-p_i\\cdot p_j, \\quad i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**交叉熵梯度**\n",
    "\n",
    "[交叉熵损失函数](https://blog.csdn.net/tsyccnh/article/details/79163834)\n",
    "\n",
    "神经元的输出就是 $a = \\sigma(z)$，其中$z=\\sum w_{j}x_{j}+b$\n",
    "\n",
    "损失函数:\n",
    "$$C=-\\frac{1}{n}\\sum[ylna+(1-y)ln(1-a)]$$\n",
    "\n",
    "$$\n",
    "L = -\\sum_k y_klog(p_k)\n",
    "$$\n",
    "这里直接来推导最终损失值L对网络输出logits 变量$z_i$的偏导数，展开为\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = -\\sum_k y_k \\frac {\\partial log(p_k)}{\\partial z_i}\\\\\n",
    "=  -\\sum_k y_k \\frac {\\partial log(p_k)}{\\partial p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i} \\\\\n",
    "= -\\sum_k y_k \\frac 1 {p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i}\n",
    "$$\n",
    "与上面的Softmax 处理类似, 将求和符号拆分为 𝑘 = 𝑖 以及𝑘 ≠ 𝑖的两种情况:\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = y_i \\frac 1 {p_i} \\cdot \\frac {\\partial p_i}{\\partial z_i} + \n",
    "-\\sum_{k\\neq i} y_k \\frac 1 {p_k}\\cdot \\frac {\\partial p_k}{\\partial z_i} \\\\\n",
    "= -y_i(1-p_i) - \\sum_{k\\neq i} y_k \\frac 1 {p_k}(-p_k \\cdot p_i) \\\\\n",
    "= p_i(y_i + \\sum_{k\\neq i} y_k)- y_i\n",
    "$$\n",
    "特别地，对于分类问题中标签𝑦通过One-hot 编码的方式，则有如下关系:\n",
    "$$\n",
    "\\sum_{k} y_k = 1 \\\\\n",
    "y_i + \\sum_{k\\neq i} y_k = 1\n",
    "$$\n",
    "所以交叉熵的偏导数可以进一步化简:\n",
    "$$\n",
    "\\frac {\\partial L}{z_i} = p_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层梯度\n",
    "以全连接层网络、激活函数采用Sigmoid 函数、误差函数为Softmax+MSE 损失函数的神经网络为例，推导其梯度传播规律\n",
    "### 单神经元梯度\n",
    "对于采用Sigmoid 激活函数的神经元模型，它的数学模型可以写为：\n",
    "$$\n",
    "o^{(1)} = \\sigma(w^{(1)^T} x + b^{(1)})\n",
    "$$\n",
    "其中变量的上标表示层数, $o^{(1)}$表示第一层的输出\n",
    "\n",
    "![](./单神经元梯度.png)\n",
    "\n",
    "如果我们采用均方误差函数，考虑到单个神经元只有一个输出$o_1^{(1)}$，那么损失可以表达为：\n",
    "$$L = \\frac 1 2 (o^{(1)}_1 - t)^2 = \\frac 1 2 (o_1 - t)^2$$\n",
    "以权值参数$w_{j1}$为例, 偏导数:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{j1}} = (o_1 - t)\\frac {\\partial o_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t) \\frac {\\partial \\sigma(z_1)}{\\partial z_{1}} \\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t)\\sigma(z_1)(1-\\sigma(z_1))\\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "= (o_1 - t)o_1(1 - o_1)\\frac {\\partial z^{(1)}_1}{\\partial w_{j1}} \\\\\n",
    "=  (o_1 - t)o_1(1 - o_1)x_{j}\n",
    "$$\n",
    "可见, 误差对权值$w_{j1}$的偏导只与输出值$o_1$, 真实值t以及当前权值连接的输入$x_j$有关\n",
    "\n",
    "    使用MSE损失函时, 偏导数受激活函数的导数影响，sigmoid函数导数在输出接近0和1时非常小，会导致一些实例在刚开始训练时学习得非常慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层梯度\n",
    "们把单个神经元模型推广到单层的全连接层的网络上\n",
    "\n",
    "![](./全连接层梯度.png)\n",
    "\n",
    "全连接层的均方差误差:\n",
    "$$\n",
    "L = \\frac 1 2 \\sum_{i=1}^K(o_i^{(1)} - t_i)^2\n",
    "$$\n",
    "对权值$w_{jk}$的偏导数只与输出节点$o_k^{(1)}$有关, 可以去除求和符号, 即$i = k$:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}} = (o_k - t_k) \\frac {\\partial o_k}{ \\partial w_{jk}} \\\\\n",
    "= (o_k -t_k)o_k(1-o_k)\\frac {\\partial z_k}{\\partial w_jk} \\\\\n",
    "= (o_k -t_k)o_k(1-o_k)x_j\n",
    "$$\n",
    "令$\\delta_k = (o_k -t_k)o_k(1-o_k)$, 则:\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}} = \\delta_k x_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的损失函数L来自各个输出节点$o_k^{K}$, 而其又与隐藏层的输出节点$o_j^{(J)}$相关联.\n",
    "\n",
    "![](./梯度传播.png)\n",
    "\n",
    "根据链式求导法则\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \n",
    "\\frac {\\partial L}{\\partial o_{j}^{(J)}} \\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}} = \n",
    "\\frac {\\partial L}{\\partial o_{k}^{(K)}}  \\frac {\\partial o_{k}^{(K)}}{\\partial o_{j}^{(J)}} \\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}}\n",
    "$$\n",
    "$\\frac {\\partial L}{\\partial o_{k}^{(K)}}$可以从误差函数中直接推到出, $\\frac {\\partial o_{k}^{(K)}}{\\partial o_{j}^{(J)}}$可以由全连接层公式推导, $\\frac {\\partial o_{j}^{(J)}}{\\partial w_{ij}^{(J)}}\n",
    "$的导数即为输入$x^{(I)}_i$\n",
    "\n",
    "通过链式法则，直接可以将偏导数进行分解，层层迭代即可推导出."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1.)\n",
    "w1 = tf.constant(2.)\n",
    "b1 = tf.constant(1.)\n",
    "w2 = tf.constant(2.)\n",
    "b2 = tf.constant(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建梯度记录器\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # 非 Variable类型 手动记录梯度信息  \n",
    "    tape.watch([w1, b1, w2, b2])\n",
    "    y1 = x * w1 + b1\n",
    "    y2 = y1 * w2 + b2\n",
    "    \n",
    "# 独立求解出各个偏导数\n",
    "dy_2_dy1 = tape.gradient(y2, [y1])[0]\n",
    "dy_1_dw1 = tape.gradient(y1, [w1])[0]\n",
    "dy_2_dw1 = tape.gradient(y2, [w1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dy_2_dw1)\n",
    "print(dy_2_dy1 * dy_1_dw1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "---\n",
    "输出层的偏导公式\n",
    "$$\n",
    "\\frac {\\partial L} {\\partial w_{jk}}\n",
    "= (o_k -t_k)o_k(1-o_k)x_j = \\delta_k x_j\n",
    "$$\n",
    "\n",
    "![](./反向传播.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算倒数第二层的偏导数$\\frac {L}{w_ij^{(J)}}$\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \\frac {\\partial}{ \\partial w_{ij}^{(J)}} \\frac 1 2 \\sum_k(o_k-t_k)^2 \\\\\n",
    "= \\sum_k(o_k-t_k) \\frac {\\partial o_k}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) \\frac {\\partial \\sigma(z_k)}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}}\\\\\n",
    "= \\sum_k(o_k-t_k) \\sigma(z_k) (1 - \\sigma(z_k)) \\frac {\\partial z_k}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\ \n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) \\frac {\\partial z_k}{ \\partial o_j} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\ \n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\frac {\\partial o_j}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\frac {\\partial \\sigma(z_j)}{ \\partial z_{j}}  \n",
    "\\frac {\\partial z_j}{ \\partial w_{ij}}\\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\sigma(z_j)(1-\\sigma(z_j))\\frac {\\partial z_j}{ \\partial w_{ij}} \\\\\n",
    "= \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} o_j(1-o_j)\\frac {\\partial z_j}{ \\partial w_{ij}} \\\\\n",
    "= o_j(1-o_j)\\frac {\\partial z_j}{ \\partial w_{ij}} \\sum_k(o_k-t_k) o_k (1 - o_k) w_{jk} \\\\\n",
    "= o_j(1-o_j)o_i \\sum_k \\delta_k^{(k)}w_{jk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义\n",
    "$$ \n",
    "\\delta^{(J)}_j \\triangleq o_j(1-o_j)\\sum_k \\delta_k^{(K)}w_{jk} \n",
    "$$\n",
    "则与输出层偏导数类似的:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}^{(J)}} = \\delta^{(J)}_j o_i^{(I)} \n",
    "$$\n",
    "$\\delta$可以简单理解为当前连接$w_{ij}$对误差函数的贡献值\n",
    "\n",
    "传播规律:\n",
    "输出层:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{jk}} = \\delta_k^{(K)}o_j \\\\\n",
    "\\delta_k^{(K)} = o_k(1-o_k)(o_k -t_k)\n",
    "$$\n",
    "倒数第二层:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ij}} = \\delta_j^{(J)}o_i \\\\\n",
    "\\delta_j^{(J)} = o_j(1-o_j)\\sum_k \\delta_k^{(K)}w_{jk} \n",
    "$$\n",
    "倒数第三层:\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial w_{ni}} = \\delta_i^{(I)}o_n \\\\\n",
    "\\delta_i^{(I)} = o_i(1-o_i)\\sum_j \\delta_j^{(J)}w_{ij} \n",
    "$$\n",
    "$o_n$是倒数第三层的输入, 即倒数第四层的输出.\n",
    "按照这个规律, 只需要循环迭代计算每一次每个节点的$\\delta_k^{(K)}, \\delta_j^{(J)} , \\delta_i^{(I)}$等值即可求得当前层的偏导数, 从而得到每层权值矩阵W的梯度,在通过梯度下降算法迭代优化网络参数即可.\n",
    "\n",
    "[计算图(Computational Graph) 1](https://blog.csdn.net/xbinworld/article/details/56523063)\n",
    "\n",
    "[计算图(Computational Graph) 2](https://samaelchen.github.io/deep_learning_step2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Himmelblau 函数优化实战\n",
    "Himmelblau 函数是用来测试优化算法的常用样例函数之一:\n",
    "$$\n",
    "f(x, y) = (x^2+y-11)^2 + (x + y^2 - 7)^2\n",
    "$$\n",
    "\n",
    "利用 TensorFlow 自动求导来求出函数在𝑥和𝑦的偏导数，并循环迭代更新𝑥和𝑦值，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x):\n",
    "    # x: 2个元素的list [x,y]\n",
    "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 -7) ** 2\n",
    "\n",
    "x, y = np.mgrid[-6:6:0.01, -6:6:0.01]\n",
    "z = himmelblau([x, y])\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap=plt.get_cmap('hot'))\n",
    "ax.view_init(60, -30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.contour(x, y, z, levels=100 ,cmap=plt.get_cmap('rainbow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([4., 0.])\n",
    "def calcu(x):\n",
    "    for step in range(200):\n",
    "        with tf.GradientTape() as tape:  # 梯度跟踪\n",
    "            tape.watch([x])  # 非Variable 手动 加入梯度跟踪列表 \n",
    "            y = himmelblau(x)\n",
    "        # 反向传播\n",
    "        grads = tape.gradient(y, [x])[0]  # y对x的梯度\n",
    "        # 更新参数\n",
    "        x -= 0.01 * grads\n",
    "        if step % 20 == 19:\n",
    "            print(f'step {step}: x={x.numpy()}, f(x)={y.numpy()}')\n",
    "calcu(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tf.constant([-3.,-3.])\n",
    "calcu(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = tf.constant([-3., 2.])\n",
    "calcu(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = tf.constant([3., 1.])\n",
    "calcu(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同初始值得到多种极小值数值解。参数的初始化状态是可能影响梯度下降算法的搜索轨迹的，甚至有可能搜索出完全不同的数值解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播实战\n",
    "实现一个4 层的全连接网络，来完成二分类任务。网络输入节点数为2，隐藏层的节点数设计为：25、50和25，输出层两个节点，分别表示属于类别1 的概率和类别2的概率.\n",
    "\n",
    "利用前面介绍的多层全连接网络的梯度推导结果，直接利用Python 循环计算每一层的梯度，并按着梯度下降算法手动更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "N_SAMPLES = 3000\n",
    "TEST_SIZE = 0.2\n",
    "X, y = make_moons(n_samples=N_SAMPLES, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "# 0.6 - 0.2 -0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], s=10, label='1')\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], s=10, label='0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[np.newaxis,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10.0)\n",
    "np.array_split(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动创建全连接层\n",
    "class Layer:\n",
    "    # 参数: 输入节点数 ,神经单元输出节点数, 激活函数, 权值, 偏置 \n",
    "    def __init__(self, n_input, n_neurons, activation=None, weights=None, bias=None):\n",
    "        self.weights = weights if weights is not None else np.random.randn(\n",
    "            n_input, n_neurons) * np.sqrt(1 / n_neurons)\n",
    "        self.bias = bias if bias is not None else np.random.randn(n_neurons) * 0.1\n",
    "        self.activation = activation\n",
    "        self.activation_output = None  # 进过激活函数后的输出\n",
    "        self.error = None  # 计算delta 的中间量\n",
    "        self.delta = None  # 每一层的delta, 用来计算梯度\n",
    "    \n",
    "    def activate(self, x):\n",
    "        # x: (b, d_in) w: (d_in, d_out) b: \n",
    "        r = np.dot(x, self.weights) + self.bias\n",
    "        self.activation_out = self._apply_activation(r)\n",
    "        return self.activation_out\n",
    "    \n",
    "    def _apply_activation(self, r):\n",
    "        # 激活函数\n",
    "        if self.activation is None:\n",
    "            # 恒等激励\n",
    "            return r \n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(r, 0)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-r))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(r)\n",
    "        else:\n",
    "            return r\n",
    "    def apply_activation_derivative(self, r):\n",
    "        # 求激活函数的导数\n",
    "        if self.activation is None:\n",
    "            # 常数1\n",
    "            return np.ones_like(r)\n",
    "        elif self.activation == 'relu':\n",
    "            d = np.array(r, copy=True)\n",
    "            d[r < 0] = 0.\n",
    "            d[r >= 0] = 1.\n",
    "            return d \n",
    "        elif self.activation == 'sigmoid':\n",
    "            return r * (1 - r)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - r ** 2\n",
    "        else:\n",
    "            return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X:(m, n_features)\n",
    "\n",
    "第K层(输出层): k个单元 $W^{[K]} = (j, k), \\quad b^{[K]} = (k, ), \\quad A^{[K]} = g^{[K]}(Z^{[K]}) = g^{[K]}(A^{[J]}@W^{[K]} + b^{[K]}) =(m, k)$\n",
    "\n",
    "$dZ^{[K]} = dA^{[K]} * g'^{[K]} \\quad (m, k)$\n",
    "\n",
    "$dW^{[K]} =  A^{[J]}.T @ dZ^{[K]}  \\quad (j, k)$\n",
    "\n",
    "$db^{[K]} = np.mean(dZ^{[K]})  \\quad (k, )$\n",
    "\n",
    "第J层(倒数第二层): J个单元 $W^{[J]} = (i, j), \\quad b^{[J]} = (j, ), \\quad A^{[J]} = g^{[J]}(Z^{[J]}) = g^{[J]}(A^{[I]}@W^{[J]} + b^{[J]}) =(m, j)$ \n",
    "\n",
    "$dA^{[J]} = dZ^{[K]} @ W^{[K]}.T \\quad (m, j)$\n",
    "\n",
    "$dZ^{[J]} = dA^{[J]} * g'^{[J]} \\quad (m, j)$\n",
    "\n",
    "$dW^{[J]} =  A^{[I]}.T @ dZ^{[J]}  \\quad (i, j)$\n",
    "\n",
    "$db^{[J]} = np.mean(dZ^{[J]})  \\quad (j, )$\n",
    "\n",
    "\n",
    "第I层(...): I个单元 $W^{[I]} = (n\\_features, i), \\quad b^{[J]} = (i, ) \\quad A^{[I]} = g^{[I]}(Z^{[I]}) = g^{[I]}(A^{[n]}@W^{[I]} + b^{[I]}) =(m, i)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建神经网络\n",
    "# 向量化 使用小批量梯度下降\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # 存放上面定义的全连接层\n",
    "        self.layers = []\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # 前向传播, 逐渐通过每一层\n",
    "        for layer in self.layers:\n",
    "            X = layer.activate(X)\n",
    "        return X  # 输出层  1个节点 sigmoid \n",
    "    \n",
    "    def loss(self, y_true, y_preds):\n",
    "        # 交叉熵误差\n",
    "        loss_ = - y_true* np.log(y_preds) - (1 - y_true)*np.log(1 - y_preds)  # (m, 1)\n",
    "        return np.mean(loss_, axis=0)\n",
    "    \n",
    "    def backpropagation(self, X, y, learning_rate):\n",
    "        # 反向传播算法 计算每一层的delta \n",
    "        # 前向计算 得到输出值\n",
    "        m = X.shape[0]  # batch size\n",
    "        out = self.feed_forward(X)  # (m, 1)\n",
    "        # loss_ = self.loss(y, out)\n",
    "        # print(loss)\n",
    "        for i in reversed(range(len(self.layers))):  # 从最后一层开始\n",
    "            layer = self.layers[i]\n",
    "            if layer == self.layers[-1]:  # 输出层\n",
    "                # 使用 交叉熵 误差\n",
    "                # layer.error = out - y  # 误差的导数\n",
    "                #layer.delta = layer.error * layer.apply_activation_derivative(out)\n",
    "                layer.error = -y / out + (1-y)/(1-out)  # dL/dA^K  # (m, 1)\n",
    "                # print('loss:', layer.error.shape, y.shape, out.shape)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(out) # dL/dZ^K  (m, 1) (m, 1)\n",
    "            else:  # 隐藏层\n",
    "                next_layer = self.layers[i + 1]\n",
    "#                 layer.error = np.dot(next_layer.weights, next_layer.delta)\n",
    "#                 layer.delta = layer.error * layer.apply_activation_derivative(layer.activation_out)\n",
    "                layer.error = next_layer.delta @ next_layer.weights.T  # dL/dA^J (m, 1) (1, k)\n",
    "                # dL/dZ^J (m, k) (m,k)\n",
    "                layer.delta = layer.error * layer.apply_activation_derivative(layer.activation_out)\n",
    "        \n",
    "        # 更新参数\n",
    "        for i in range(len(self.layers)):\n",
    "            layer = self.layers[i]\n",
    "            # 上一层的输出 本层的输入\n",
    "            o_i = np.atleast_2d(X if i == 0 else self.layers[i-1].activation_out)\n",
    "            # weights (I, J)\n",
    "            # o_i (m, I)\n",
    "            # delta (m, J)\n",
    "            layer.weights -= learning_rate *  o_i.T @ layer.delta  # (I, J)\n",
    "            layer.bias -=  learning_rate * np.mean(layer.delta)  # (J, )\n",
    "    \n",
    "    def fit(self, X, y, learning_rate=0.01 ,max_epochs=300):\n",
    "        # y onehot 处理\n",
    "#         y_onehot = np.zeros((y.shape[0], 2))\n",
    "#         y_onehot[np.arange(y.shape[0]),  y] =1\n",
    "        y = y.reshape(-1, 1)\n",
    "        cross_entropy = []\n",
    "        batch_size = 32\n",
    "        split_size = X.shape[0] // batch_size\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            for x, y_true in zip(np.array_split(X, split_size), np.array_split(y, split_size)):\n",
    "            # for j in range(X.shape[0]):  # 每次训练一个样本\n",
    "                self.backpropagation(x, y_true, learning_rate)\n",
    "            if epoch % 10 == 0:\n",
    "                # mse = np.mean(np.square(self.feed_forward(X) - y_onehot))\n",
    "                loss = self.loss(y, self.feed_forward(X))\n",
    "                cross_entropy.append(loss)\n",
    "                print(f'Epoch: {epoch}, cross_entropy: {loss}')\n",
    "        return cross_entropy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.feed_forward(X)\n",
    "        y_pred = np.where(y_pred >0.5, 1, 0)\n",
    "        # out = np.argmax(y_pred, axis=1)\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.add_layer(Layer(2, 25, 'tanh'))  # h1  \n",
    "model.add_layer(Layer(25, 50, 'tanh'))  # h2 \n",
    "model.add_layer(Layer(50, 25, 'tanh'))  # h3\n",
    "model.add_layer(Layer(25, 1, 'sigmoid'))  # output\n",
    "cross_entropy = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# y_pred\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp  = confusion_matrix(y_test, y_pred).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1, 0, 1, 1])\n",
    "y_onehot = np.zeros((y.shape[0], 2))\n",
    "y_onehot[np.arange(y.shape[0]),  y] =1\n",
    "y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用tensorflow**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, losses, Model\n",
    "\n",
    "class Network(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Sequential([\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(50, activation='relu'),\n",
    "            layers.Dense(25, activation='relu'),\n",
    "            layers.Dense(2),\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # 在前向计算函数 call 中实现自定义网络类的计算逻辑即可\n",
    "        out = self.model(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "# 通过 build 函数完成内部张量的创建 (batch_size, n_input_features)\n",
    "network.build(input_shape=(32, 2))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.build?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * 25 + 25 + 25 * 50 + 50 + 50* 25 + 25 + 25* 2 +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = tf.one_hot(y_train, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((X_train, y_train_onehot))\n",
    "train_db = train_db.shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.GradientTape?\n",
    "```\n",
    "tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "persistent :默认情况下为False，这意味着最多可以在此对象上对gradient()方法进行一次调用。\n",
    "watch_accessed_variables: 自动追踪所有 trainable 对象\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.01\n",
    "train_losses = []  # Relu - 交叉熵损失\n",
    "accurancy = []\n",
    "for epoch in range(100):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            out = network(x)\n",
    "            loss = losses.categorical_crossentropy(y, out, from_logits=True)\n",
    "        grads = tape.gradient(loss, network.trainable_variables)\n",
    "        \n",
    "        for p, g in zip(network.trainable_variables, grads):\n",
    "            p.assign_sub(learn_rate * g)\n",
    "    if epoch % 5 == 0: \n",
    "        out = network(X_test)\n",
    "        y_pred = np.argmax(out, axis=1)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(epoch, acc)\n",
    "        accurancy.append(acc)\n",
    "        # train_losses.append(loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(accurancy)) * 5, accurancy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
